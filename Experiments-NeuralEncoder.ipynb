{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import baseline\n",
    "from encoder import *\n",
    "from baseline import *\n",
    "from decoder import *\n",
    "from alphabet import *\n",
    "from train import *\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76738 training examples, 9590 validation examples, 9616 test exampless\n"
     ]
    }
   ],
   "source": [
    "LANGUAGE = 'Python'\n",
    "\n",
    "def filter_ascii(strings):\n",
    "    'Returns only the strings that can be encoded in ASCII.'\n",
    "    l = []\n",
    "    for s in strings:\n",
    "        try:\n",
    "            s.encode('ascii')\n",
    "            if len(s) <= 80:\n",
    "                l.append(s)\n",
    "        except UnicodeEncodeError:\n",
    "            pass\n",
    "        \n",
    "    return l\n",
    "\n",
    "with open('dataset/medium.json') as f:\n",
    "    multilang_dataset = json.load(f)\n",
    "    dataset = multilang_dataset[LANGUAGE]\n",
    "    \n",
    "    dataset['train'] = filter_ascii(dataset['train'])\n",
    "    dataset['dev'] = filter_ascii(dataset['dev'])\n",
    "    dataset['test'] = filter_ascii(dataset['test'])\n",
    "    \n",
    "    tiny_dataset = {\n",
    "        'train': dataset['train'][:50],\n",
    "        'dev': dataset['train'][:50],\n",
    "        'test': dataset['train'][:50],\n",
    "    }\n",
    "    \n",
    "    print('{} training examples, {} validation examples, {} test exampless'.format(\n",
    "        len(dataset['train']), \n",
    "        len(dataset['dev']),\n",
    "        len(dataset['test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dumb_dataset():\n",
    "    'Returns a dataset where all examples are the same string, which consists of 10 times the same letter.'\n",
    "\n",
    "    SIZE = 200\n",
    "    l = []\n",
    "\n",
    "    for i in range(SIZE):\n",
    "        l.append(random.choice('abcdefghijklmnopqrstuvwxyz') * random.choice([10]))\n",
    "        \n",
    "    return {'train': l, 'dev': l, 'test': l}\n",
    "\n",
    "dumb_dataset = generate_dumb_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(0) if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "alphabet = AsciiOneHotEncoding(device)\n",
    "encoder = baseline.UniformEncoder(0.9)\n",
    "decoder = AutoCompleteDecoderModel(alphabet, hidden_size=64)\n",
    "nencoder = NeuralEncoder(alphabet, epsilon=0.2, hidden_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240.10151319598083\n"
     ]
    }
   ],
   "source": [
    "def expected_initial_loss(input_string, epsilon, alphabet, lam):\n",
    "    s, a = len(input_string), alphabet.size()\n",
    "    return s/2.0 + lam*(s*math.log(a)-epsilon)\n",
    "\n",
    "print(expected_initial_loss('ddddd',0.5,alphabet,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial lambda: tensor(50., device='cuda:0', requires_grad=True)\n",
      "Epoch 0 iteration 0: loss = 1542.217, lambda: 50.308, % kept: 0.484, rec_loss: 4.852, avg likelihood: -6.930tp = 502.72 lines/s, ETA 01h24m52s\n",
      "Epoch 25 iteration 0: loss = 2179.743, lambda: 80.643, % kept: 0.506, rec_loss: 4.269, avg likelihood: -6.932tp = 513.35 lines/s, ETA 01h22m54s\n",
      "Epoch 50 iteration 0: loss = 2168.913, lambda: 103.394, % kept: 0.495, rec_loss: 3.311, avg likelihood: -6.931tp = 514.84 lines/s, ETA 01h22m27s\n",
      "Epoch 75 iteration 0: loss = 2565.568, lambda: 124.440, % kept: 0.494, rec_loss: 3.254, avg likelihood: -6.931tp = 513.57 lines/s, ETA 01h22m27s\n",
      "Epoch 100 iteration 0: loss = 2964.909, lambda: 145.159, % kept: 0.489, rec_loss: 3.224, avg likelihood: -6.931tp = 511.96 lines/s, ETA 01h22m30s\n",
      "Epoch 125 iteration 0: loss = 3452.394, lambda: 165.792, % kept: 0.478, rec_loss: 3.288, avg likelihood: -6.930tp = 511.98 lines/s, ETA 01h22m17s\n",
      "Epoch 150 iteration 0: loss = 3871.032, lambda: 186.369, % kept: 0.475, rec_loss: 3.279, avg likelihood: -6.930tp = 512.90 lines/s, ETA 01h21m56s\n",
      "Epoch 175 iteration 0: loss = 4256.221, lambda: 206.890, % kept: 0.498, rec_loss: 3.247, avg likelihood: -6.931tp = 514.05 lines/s, ETA 01h21m32s\n",
      "Epoch 200 iteration 0: loss = 4631.944, lambda: 227.400, % kept: 0.492, rec_loss: 3.215, avg likelihood: -6.931tp = 515.65 lines/s, ETA 01h21m05s\n",
      "Epoch 225 iteration 0: loss = 5118.706, lambda: 247.879, % kept: 0.494, rec_loss: 3.259, avg likelihood: -6.931tp = 516.92 lines/s, ETA 01h20m40s\n",
      "Epoch 250 iteration 0: loss = 5502.523, lambda: 268.306, % kept: 0.497, rec_loss: 3.237, avg likelihood: -6.931tp = 517.47 lines/s, ETA 01h20m23s\n",
      "Epoch 275 iteration 0: loss = 5886.245, lambda: 288.713, % kept: 0.475, rec_loss: 3.219, avg likelihood: -6.930tp = 517.87 lines/s, ETA 01h20m07s\n",
      "Epoch 300 iteration 0: loss = 6208.445, lambda: 309.112, % kept: 0.462, rec_loss: 3.172, avg likelihood: -6.928tp = 517.95 lines/s, ETA 01h19m54s\n",
      "Epoch 325 iteration 0: loss = 6626.321, lambda: 329.261, % kept: 0.528, rec_loss: 3.175, avg likelihood: -6.934tp = 517.78 lines/s, ETA 01h19m43s\n",
      "Epoch 350 iteration 0: loss = 6570.332, lambda: 348.696, % kept: 0.513, rec_loss: 2.974, avg likelihood: -6.933tp = 517.89 lines/s, ETA 01h19m29s\n",
      "Epoch 375 iteration 0: loss = 6697.274, lambda: 367.167, % kept: 0.497, rec_loss: 2.880, avg likelihood: -6.931tp = 511.96 lines/s, ETA 01h20m12s\n",
      "Epoch 400 iteration 0: loss = 6787.581, lambda: 384.962, % kept: 0.506, rec_loss: 2.783, avg likelihood: -6.932tp = 511.48 lines/s, ETA 01h20m04s\n",
      "Epoch 425 iteration 0: loss = 6997.338, lambda: 402.391, % kept: 0.509, rec_loss: 2.745, avg likelihood: -6.933tp = 511.58 lines/s, ETA 01h19m51s\n",
      "Epoch 450 iteration 0: loss = 7162.962, lambda: 419.591, % kept: 0.502, rec_loss: 2.695, avg likelihood: -6.932tp = 511.86 lines/s, ETA 01h19m36s\n",
      "Epoch 475 iteration 0: loss = 7342.145, lambda: 436.596, % kept: 0.508, rec_loss: 2.655, avg likelihood: -6.932tp = 512.18 lines/s, ETA 01h19m20s\n",
      "Epoch 500 iteration 0: loss = 7542.003, lambda: 453.439, % kept: 0.502, rec_loss: 2.626, avg likelihood: -6.932tp = 512.88 lines/s, ETA 01h19m01s\n",
      "Epoch 525 iteration 0: loss = 7913.648, lambda: 470.055, % kept: 0.541, rec_loss: 2.656, avg likelihood: -6.936tp = 513.50 lines/s, ETA 01h18m43s\n",
      "Epoch 550 iteration 0: loss = 7996.494, lambda: 486.493, % kept: 0.484, rec_loss: 2.596, avg likelihood: -6.930tp = 513.91 lines/s, ETA 01h18m27s\n",
      "Epoch 575 iteration 0: loss = 8189.303, lambda: 502.817, % kept: 0.505, rec_loss: 2.572, avg likelihood: -6.932tp = 514.44 lines/s, ETA 01h18m10s\n",
      "Epoch 600 iteration 0: loss = 8354.983, lambda: 518.981, % kept: 0.517, rec_loss: 2.541, avg likelihood: -6.935tp = 514.94 lines/s, ETA 01h17m53s\n",
      "Epoch 625 iteration 0: loss = 8627.014, lambda: 535.075, % kept: 0.491, rec_loss: 2.547, avg likelihood: -6.929tp = 515.31 lines/s, ETA 01h17m37s\n",
      "Epoch 650 iteration 0: loss = 8742.713, lambda: 551.087, % kept: 0.514, rec_loss: 2.504, avg likelihood: -6.934tp = 515.40 lines/s, ETA 01h17m24s\n",
      "Epoch 675 iteration 0: loss = 8836.333, lambda: 567.009, % kept: 0.484, rec_loss: 2.462, avg likelihood: -6.930tp = 515.68 lines/s, ETA 01h17m09s\n",
      "Epoch 700 iteration 0: loss = 9279.735, lambda: 582.942, % kept: 0.519, rec_loss: 2.512, avg likelihood: -6.937tp = 515.81 lines/s, ETA 01h16m55s\n",
      "Epoch 725 iteration 0: loss = 9223.256, lambda: 598.833, % kept: 0.541, rec_loss: 2.430, avg likelihood: -6.939tp = 515.85 lines/s, ETA 01h16m42s\n",
      "Epoch 750 iteration 0: loss = 9703.576, lambda: 614.631, % kept: 0.519, rec_loss: 2.492, avg likelihood: -6.937tp = 515.98 lines/s, ETA 01h16m29s\n",
      "Epoch 775 iteration 0: loss = 9818.691, lambda: 630.379, % kept: 0.453, rec_loss: 2.464, avg likelihood: -6.919tp = 516.26 lines/s, ETA 01h16m14s\n",
      "Epoch 800 iteration 0: loss = 10158.291, lambda: 646.075, % kept: 0.475, rec_loss: 2.486, avg likelihood: -6.924tp = 516.11 lines/s, ETA 01h16m03s\n",
      "Epoch 825 iteration 0: loss = 10253.109, lambda: 661.815, % kept: 0.516, rec_loss: 2.444, avg likelihood: -6.937tp = 516.15 lines/s, ETA 01h15m50s\n",
      "Epoch 850 iteration 0: loss = 10672.406, lambda: 677.541, % kept: 0.534, rec_loss: 2.484, avg likelihood: -6.943tp = 516.29 lines/s, ETA 01h15m36s\n",
      "Epoch 875 iteration 0: loss = 10691.834, lambda: 693.176, % kept: 0.464, rec_loss: 2.440, avg likelihood: -6.921tp = 515.53 lines/s, ETA 01h15m31s\n",
      "Epoch 900 iteration 0: loss = 11139.013, lambda: 708.794, % kept: 0.458, rec_loss: 2.488, avg likelihood: -6.914tp = 512.68 lines/s, ETA 01h15m43s\n",
      "Epoch 925 iteration 0: loss = 11623.375, lambda: 724.451, % kept: 0.480, rec_loss: 2.536, avg likelihood: -6.927tp = 510.03 lines/s, ETA 01h15m54s\n",
      "Epoch 950 iteration 0: loss = 11460.264, lambda: 740.112, % kept: 0.472, rec_loss: 2.449, avg likelihood: -6.922tp = 508.58 lines/s, ETA 01h15m55s\n",
      "Epoch 975 iteration 0: loss = 11925.395, lambda: 755.703, % kept: 0.552, rec_loss: 2.475, avg likelihood: -6.979tp = 509.01 lines/s, ETA 01h15m38s\n",
      "Epoch 1000 iteration 0: loss = 11830.963, lambda: 771.290, % kept: 0.469, rec_loss: 2.426, avg likelihood: -6.917tp = 507.60 lines/s, ETA 01h15m38s\n",
      "Epoch 1025 iteration 0: loss = 12234.019, lambda: 786.858, % kept: 0.464, rec_loss: 2.464, avg likelihood: -6.907tp = 507.80 lines/s, ETA 01h15m24s\n",
      "Epoch 1050 iteration 0: loss = 12795.078, lambda: 802.370, % kept: 0.492, rec_loss: 2.519, avg likelihood: -6.928tp = 508.02 lines/s, ETA 01h15m09s\n",
      "Epoch 1075 iteration 0: loss = 12999.188, lambda: 817.894, % kept: 0.431, rec_loss: 2.541, avg likelihood: -6.851tp = 508.24 lines/s, ETA 01h14m55s\n",
      "Epoch 1100 iteration 0: loss = 12735.785, lambda: 833.361, % kept: 0.455, rec_loss: 2.432, avg likelihood: -6.880tp = 508.47 lines/s, ETA 01h14m40s\n",
      "Epoch 1125 iteration 0: loss = 13184.428, lambda: 848.671, % kept: 0.467, rec_loss: 2.472, avg likelihood: -6.884tp = 508.61 lines/s, ETA 01h14m26s\n",
      "Epoch 1150 iteration 0: loss = 13106.703, lambda: 863.963, % kept: 0.434, rec_loss: 2.439, avg likelihood: -6.812tp = 508.69 lines/s, ETA 01h14m13s\n",
      "Epoch 1175 iteration 0: loss = 13248.924, lambda: 879.281, % kept: 0.439, rec_loss: 2.422, avg likelihood: -6.815tp = 508.72 lines/s, ETA 01h14m00s\n",
      "Epoch 1200 iteration 0: loss = 13519.165, lambda: 894.121, % kept: 0.398, rec_loss: 2.471, avg likelihood: -6.681tp = 508.75 lines/s, ETA 01h13m47s\n",
      "Epoch 1225 iteration 0: loss = 12341.044, lambda: 908.628, % kept: 0.331, rec_loss: 2.462, avg likelihood: -6.076tp = 508.86 lines/s, ETA 01h13m34s\n",
      "Epoch 1250 iteration 0: loss = 10421.609, lambda: 921.686, % kept: 0.272, rec_loss: 2.377, avg likelihood: -5.278tp = 509.03 lines/s, ETA 01h13m20s\n",
      "Epoch 1275 iteration 0: loss = 9541.323, lambda: 933.040, % kept: 0.214, rec_loss: 2.566, avg likelihood: -4.396tp = 509.12 lines/s, ETA 01h13m07s\n",
      "Epoch 1300 iteration 0: loss = 8658.004, lambda: 943.140, % kept: 0.164, rec_loss: 2.554, avg likelihood: -4.053tp = 509.24 lines/s, ETA 01h12m53s\n",
      "Epoch 1325 iteration 0: loss = 7415.656, lambda: 951.206, % kept: 0.136, rec_loss: 2.750, avg likelihood: -3.238tp = 509.57 lines/s, ETA 01h12m38s\n",
      "Epoch 1350 iteration 0: loss = 5099.259, lambda: 957.860, % kept: 0.091, rec_loss: 2.733, avg likelihood: -2.208tp = 509.90 lines/s, ETA 01h12m22s\n",
      "Epoch 1375 iteration 0: loss = 4952.650, lambda: 963.286, % kept: 0.083, rec_loss: 2.645, avg likelihood: -2.303tp = 510.25 lines/s, ETA 01h12m07s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1400 iteration 0: loss = 3889.176, lambda: 967.623, % kept: 0.059, rec_loss: 2.842, avg likelihood: -1.690tp = 510.59 lines/s, ETA 01h11m51s\n",
      "Epoch 1425 iteration 0: loss = 2881.319, lambda: 971.116, % kept: 0.041, rec_loss: 2.827, avg likelihood: -1.268tp = 509.24 lines/s, ETA 01h11m50s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-950f1f8d5b55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain_loss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_avg_kept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_reconstruction_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdumb_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/BingBong/cs224n-project/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, dataset, parameters, device)\u001b[0m\n\u001b[1;32m    111\u001b[0m                                          for i in range(batch_size)]\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 \u001b[0mper_prediction_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_batch_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#B\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mkept_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/BingBong/cs224n-project/decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, compressed, expected)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphabet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_batch_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mE_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphabet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/BingBong/cs224n-project/alphabet.py\u001b[0m in \u001b[0;36mencode_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    121\u001b[0m         return torch.stack(\n\u001b[1;32m    122\u001b[0m                 [torch.cat([self.encode(s), self.PADDING.repeat(max_length - len(s), 1)])\n\u001b[0;32m--> 123\u001b[0;31m                  for s in batch])\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode_batch_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/BingBong/cs224n-project/alphabet.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    121\u001b[0m         return torch.stack(\n\u001b[1;32m    122\u001b[0m                 [torch.cat([self.encode(s), self.PADDING.repeat(max_length - len(s), 1)])\n\u001b[0;32m--> 123\u001b[0;31m                  for s in batch])\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode_batch_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'learning_rate': 1e-4,\n",
    "    'verbose': True,\n",
    "    'batch_size': 64,\n",
    "    'init_scale': 0.01,\n",
    "    'epochs': 10000,\n",
    "    'initial_lambda': 50,\n",
    "    'epsilon': 0.1,\n",
    "}\n",
    "\n",
    "train_loss_history, train_avg_kept, train_reconstruction_loss = train(nencoder, decoder, dumb_dataset, parameters, device)\n",
    "plt.plot(train_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., grad_fn=<SumBackward0>)\n",
      "tensor([1., 1.], grad_fn=<BernoulliBackward0>)\n",
      "None\n",
      "tensor(-0.3185, grad_fn=<SqueezeBackward1>)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ps = torch.tensor([0.3, 0.8], requires_grad=True)\n",
    "bs = torch.bernoulli(ps)\n",
    "s = bs.sum()\n",
    "print(s)\n",
    "s.backward()\n",
    "print(bs)\n",
    "print(bs.grad)\n",
    "\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "b = Categorical(ps)\n",
    "mask = b.sample()\n",
    "# next_state, reward = env.step(action)\n",
    "# loss = -m.log_prob(action) * reward\n",
    "loss = b.log_prob(mask)\n",
    "loss.backward()\n",
    "print(loss)\n",
    "print(mask.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs._grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String: 'ffffffffff'\n",
      "Encoded: 'ffffffffff'\n",
      "Decoded: 'd}{}\\x13d}}|d}\\x13\\x13d})dddA}}\\x13})}7d}}}|}}\\x13d}\\x13d}A|\\x13}}Z(AA|\\x13}}{}}}K}}AP}d}}\\x13}d}}}}\\x13d}j}}\\x13Z\\x13A}j}}}\\x13dd})}\\x13}Z\\x13}|d\\x13\\x13}}}t})}\\x13d}}dZ}}|}}d}Z}}dZAA}\\x13})}dd}}dt}\\x04d}dA}}}}A})d}}d}d}A}}\\x04}A}A\\x13d|A}}}}\\x13A}\\x13d}}}dA}}})d}d}{)\\x13|A'\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "SPLIT = 'train'\n",
    "\n",
    "import copy\n",
    "\n",
    "s = random.choice(dumb_dataset[SPLIT])\n",
    "compressed = encoder.encode(s)\n",
    "decompressed = decoder([compressed])\n",
    "\n",
    "print('String:', repr(s))\n",
    "print('Encoded:', repr(compressed))\n",
    "print('Decoded:', repr(decompressed[0]))\n",
    "print(len(decompressed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    }
   ],
   "source": [
    "def top1accuracy(dataset):\n",
    "    return len(list(filter(lambda s: s == decoder([encoder.encode(s)])[0],\n",
    "                         dataset)))/len(dataset)\n",
    "print(top1accuracy(dumb_dataset[SPLIT]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
