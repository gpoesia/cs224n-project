\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{lee2019learning}
\citation{Little2009}
\citation{Reiss}
\citation{lee2019learning}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Approach}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Encoder}{2}{subsection.3.1}\protected@file@percent }
\citation{lee2019learning}
\citation{lee2019learning}
\citation{copy_mech}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Decoder}{3}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Attention-based copy mechanism}{3}{subsubsection.3.2.1}\protected@file@percent }
\citation{lee2019learning}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}\emph  {Hard-copy} mechanism}{4}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{4}{section.4}\protected@file@percent }
\newlabel{sec:experiments}{{4}{4}{Experiments}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Data}{4}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Experimental Details}{4}{subsection.4.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Top-1 Accuracies / Top-5 Accuracies for Encoder/Decoder Pairs.}}{5}{table.1}\protected@file@percent }
\newlabel{tab:top1accnoisy}{{1}{5}{Top-1 Accuracies / Top-5 Accuracies for Encoder/Decoder Pairs}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Accuracy and Robustness of Auto-Complete Schemes}{5}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Impact of compactness on accuracy}{5}{subsection.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Top-1 accuracy of \emph  {Uniform} decoders trained with different compression rates.}}{6}{figure.1}\protected@file@percent }
\newlabel{fig:unif}{{1}{6}{Top-1 accuracy of \emph {Uniform} decoders trained with different compression rates}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Impact of different programming languages on auto-complete schemes}{6}{subsection.4.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Top-1 Accuracies for \emph  {Uniform} trained with constantly dropping 5 characters for Python, Java, and Haskell. Long lines contain more than 65 characters and short lines contain less than 25 characters.}}{7}{table.2}\protected@file@percent }
\newlabel{tab:top1acclangs}{{2}{7}{Top-1 Accuracies for \emph {Uniform} trained with constantly dropping 5 characters for Python, Java, and Haskell. Long lines contain more than 65 characters and short lines contain less than 25 characters}{table.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Analysis}{7}{section.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Top-2 Translations for the input \texttt  {for i in rng(10):}. }}{8}{table.3}\protected@file@percent }
\newlabel{tab:translations}{{3}{8}{Top-2 Translations for the input \texttt {for i in rng(10):}}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Top-2 Translations for \texttt  {r i i r10):}. R = Rules-Based encoder, U = Uniform Encoder at .8 compression, F = Frequency Encoder dropping 5-grams to compression of .8. NF = Noisy Frequency Encoder dropping 5-grams to compression of .8. All models are trained with lines of Python code. }}{8}{table.4}\protected@file@percent }
\newlabel{tab:translations_harder}{{4}{8}{Top-2 Translations for \texttt {r i i r10):}. R = Rules-Based encoder, U = Uniform Encoder at .8 compression, F = Frequency Encoder dropping 5-grams to compression of .8. NF = Noisy Frequency Encoder dropping 5-grams to compression of .8. All models are trained with lines of Python code}{table.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion and Future Work}{8}{section.6}\protected@file@percent }
\citation{lee2019learning}
\bibstyle{unsrt}
\bibdata{references}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Uniform and Noisy Frequency decoders, at their best and worst performances}}{9}{table.5}\protected@file@percent }
\newlabel{tab:best_and_worst}{{5}{9}{Uniform and Noisy Frequency decoders, at their best and worst performances}{table.5}{}}
