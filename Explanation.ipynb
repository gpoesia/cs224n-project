{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inside-Out Code Auto-Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "We would like to build an auto-complete engine that treats user-input as clues toward the full correct line of code, where these clues can be arbitrary parts of the target line. This is in contrast to the usual setting for auto-completion, where user input is given as the left-most part of the target line, and the system only guesses the right hand side of the line.\n",
    "\n",
    "We seek to train a standard seq-to-seq model to \"decode\" the correct line of a programming language given a compressed representation of that line. We use a bidirectional LSTM with attention, and use character embeddings to represent the input. \n",
    "\n",
    "First we import our decoder model and prepare our dataset of Python, Haskell, and Java lines from Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602762 training examples, 80964 validation examples, 80789 test exampless\n",
      "616686 training examples, 82731 validation examples, 82543 test exampless\n",
      "565796 training examples, 77886 validation examples, 77884 test exampless\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import baseline\n",
    "import numpy as np\n",
    "from baseline import *\n",
    "from decoder import *\n",
    "from alphabet import *\n",
    "from train import *\n",
    "import pandas\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import collections\n",
    "\n",
    "def filter_ascii(strings):\n",
    "    'Returns only the strings that can be encoded in ASCII.'\n",
    "    l = []\n",
    "    for s in strings:\n",
    "        try:\n",
    "            s.encode('ascii')\n",
    "            if 10 <= len(s) <= 80:\n",
    "                l.append(s)\n",
    "        except UnicodeEncodeError:\n",
    "            pass\n",
    "\n",
    "    return list(set(l))\n",
    "\n",
    "def language_2_dataset(language='Python'):\n",
    "    with open('dataset/large.json') as f:\n",
    "        multilang_dataset = json.load(f)\n",
    "        dataset = multilang_dataset[language]\n",
    "\n",
    "        dataset['train'] = filter_ascii(dataset['train'])\n",
    "        dataset['dev'] = filter_ascii(dataset['dev'])\n",
    "        dataset['test'] = filter_ascii(dataset['test'])\n",
    "        print('{} training examples, {} validation examples, {} test exampless'.format(\n",
    "            len(dataset['train']), \n",
    "            len(dataset['dev']),\n",
    "            len(dataset['test'])))\n",
    "        return dataset\n",
    "    \n",
    "full_dataset = {\n",
    "    \"python\": language_2_dataset(\"Python\"),\n",
    "    \"haskell\": language_2_dataset(\"Haskell\"),\n",
    "    \"java\": language_2_dataset(\"Java\"),\n",
    "}\n",
    "python_dat = full_dataset[\"python\"]\n",
    "java_dat = full_dataset[\"java\"]\n",
    "haskell_dat = full_dataset[\"haskell\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a choice of how to train our decoder -- since we have lines from Github, we can synthetically generate training data based off of guesses of how users might abbreviate their lines of code.\n",
    "\n",
    "A very basic way of compressing lines of code is to keep a table of keywords and common likely abbreviations of those keywords. Then we generate our training data by replacing keywords in our Github lines according to the table.\n",
    "\n",
    "Examples of rules that we use for Python are 'string' -> 'str', 'tuple' -> 'tup', 'reversed' -> 'rev', 'True' -> 'T', and 'False'-> 'F'. \n",
    "Below we have machinery to load models, and then we load a decoder model trained using rule-based encodings in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(models, is_baseline=True, load_best=False):\n",
    "    def load_model(model):\n",
    "        device = torch.device(0) if torch.cuda.is_available() else torch.device('cpu')\n",
    "        encoder, alpha, dataset, lang_name = model\n",
    "\n",
    "        if load_best:\n",
    "            # Load the best model saved during training.\n",
    "            filename = \"trained_models/{}_best_\".format(encoder.name())\n",
    "            loss_history_filename = \"trained_models/best_model_{}\".format(encoder.name())\n",
    "        else:\n",
    "            # Load the final model.\n",
    "            filename = \"trained_models/{}_{}_{}\".format(encoder.name(), alpha, lang_name)\n",
    "            loss_history_filename = \"trained_models/{}_{}_{}.json\".format(encoder.name(), alpha, lang_name)\n",
    "\n",
    "        alphabet = AsciiEmbeddedEncoding(device)\n",
    "        decoder = AutoCompleteDecoderModel(alphabet, hidden_size=512, copy=None)    \n",
    "        decoder.load_state_dict(torch.load(filename + \"decoder.model\"))\n",
    "        alphabet.load_state_dict(torch.load(filename + \"alphabet.model\"))\n",
    "\n",
    "        decoder.to(device)\n",
    "        alphabet.to(device)\n",
    "\n",
    "        if not is_baseline:\n",
    "            encoder.load_state_dict(torch.load(filename + \"encoder.model\"))\n",
    "\n",
    "        with open(loss_history_filename) as f:\n",
    "            j = json.load(f)\n",
    "            loss_history = j[\"losses\"]\n",
    "\n",
    "        print('Model', model[0].name(), model[3], 'loaded!')\n",
    "        return (encoder, decoder, alphabet, lang_name, loss_history)\n",
    "    return [load_model(m) for m in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model RulesBasedEncoderPython(whitespace=False) python loaded!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = [ (RulesBasedEncoder(), .0001, python_dat, 'python')]\n",
    "loaded_models = load_models(models)\n",
    "\n",
    "for (encoder, decoder, alphabet, lang, loss_history) in loaded_models:\n",
    "    plt.title('{} {} loss history'.format(encoder.name(), lang))\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(loss_history)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the trained decoder on some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<baseline.RulesBasedEncoder object at 0x7fec03b38358> \n",
      " for i in range(10): \n",
      "\n",
      "<baseline.RulesBasedEncoder object at 0x7fec03b38358> \n",
      " for i in rg(10): \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_models_on_string(string, models,beam=False):\n",
    "    for model in models:\n",
    "        (encoder, decoder, alphabet, language, _) = model\n",
    "        print(encoder, \"\\n\", decoder([string], alphabet)[0],\"\\n\") \n",
    "        if beam:\n",
    "            print(beam_search_predictions(decoder, string, alphabet, beam_size=2),\"\\n\")\n",
    "\n",
    "run_models_on_string(\"for i in rng(10):\", loaded_models)\n",
    "run_models_on_string(\"for i in rg(10):\", loaded_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works for the abbreviation 'rng', because it was explicitly in the translation list.\n",
    "But if the abbreviation is not on the list, the decoder struggles: clearly this is not going to scale.\n",
    "We want our model to be flexible with respect to the various encodings that a programmer might use.\n",
    "\n",
    "One way to ensure flexibility is to separately decide whether to drop each character with some fixed probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model UniformEncoder(0.80) python loaded!\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    (UniformEncoder(removal_probability=.8),.0005, python_dat, 'python')\n",
    "]\n",
    "\n",
    "loaded_models = load_models(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<baseline.UniformEncoder object at 0x7fec03b38eb8> \n",
      " for i in range(10): \n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_models_on_string(\"for i in rng(10):\", loaded_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<baseline.UniformEncoder object at 0x7fec03b38eb8> \n",
      " for i in range(10): \n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_models_on_string(\"for i in rg(10):\", loaded_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The uniform distribution encoding works to train a decoder which can solve both our original example and the perturbed one.\n",
    " \n",
    "But it seems like we are missing something in our model of human line compression. Certain characters are more likely to be dropped in accordance with how redundant they are semantically. One way of approximating this is to iteratively drop the most-common n-grams in the corpus. To keep the learning reasonable, we don't drop the first and last charaters of the n-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model UniformEncoder(0.80) python loaded!\n",
      "Model FrequencyEncoder(5-gram, target_size:0.8) python loaded!\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "   (UniformEncoder(removal_probability=.8),.0005, python_dat, 'python'),\n",
    "   (FrequencyEncoder(dataset=python_dat['train'], compression_rate=.8, n_gram=5), .0005, python_dat, 'python')    \n",
    "]\n",
    "\n",
    "loaded_models = load_models(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<baseline.UniformEncoder object at 0x7fec03b382e8> \n",
      " per._open(\"-v\", \"--version\", help=\"user a specific zc.bad out version\") \n",
      "\n",
      "<baseline.FrequencyEncoder object at 0x7fec03b38208> \n",
      " parser.add_option(\"-v\", \"--version\", help=\"use a specific zc.buildout version\") \n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_models_on_string('per._opn(\"-v\", \"--vern\", help=\"use a specific zc.bdout vern\")', loaded_models, beam=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency-based decoder gets this example correct. The common 5-grams that were identified in the original string were 'parse', '.add_', 'ption', 'rsion', 'build', and 'rsion' again. The uniform case is working off of more local information and cannot reconstruct 'zc.buildout' from 'zc.bdout'.\n",
    "\n",
    "But if we look back at our range example, we find that the frequency-based encoding badly misbehaves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<baseline.UniformEncoder object at 0x7fec03b382e8> \n",
      " for i in range(10): \n",
      "\n",
      "<baseline.FrequencyEncoder object at 0x7fec03b38208> \n",
      " for i in rng(10000): \n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_models_on_string(\"for i in rng(10):\", loaded_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency-based decoder sees 10, and guesses that it really comes from the sequence 10000.\n",
    "Even worse, in the beam search the number is further expanded because it thinks that 00 comes from 00000. \n",
    "\n",
    "This could be due to the decoder learning that it needs to expand the input string, and will prefer longer explanations as a result.\n",
    "Frequency-based can sometimes build sophisticated translations using fairly non-local information; however, it tends to violate Occam's Razor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should we evaluate these models? Accuracies of models using their own encoding schemes are somewhat incomparable -- we also need to know which of the encoding schemes is closest to what programmers would actually type when using these autocompletion facilities.\n",
    "\n",
    "Maybe we can get more information by comparing how the decoders work using each others' encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model RulesBasedEncoderPython(whitespace=False) python loaded!\n",
      "Model UniformEncoder(0.80) python loaded!\n",
      "Model FrequencyEncoder(5-gram, target_size:0.8) python loaded!\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "   (RulesBasedEncoder(), .0001, python_dat, 'python'),\n",
    "   (UniformEncoder(removal_probability=.8),.0005, python_dat, 'python'),\n",
    "   (FrequencyEncoder(dataset=python_dat['train'], compression_rate=.8, n_gram=5), .0005, python_dat, 'python')    \n",
    "]\n",
    "loaded_models = load_models(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top1_accuracy(encoder, decoder, alphabet, dataset):\n",
    "    return len(list(filter(lambda x:x[0]==x[1],\n",
    "                          zip(dataset, decoder(encoder.encode_batch(dataset),alphabet))))) / len(dataset)\n",
    "\n",
    "def models_to_name(m1,m2):\n",
    "    return str((m1[0].name()+m1[3], m2[0].name()+m2[3]))\n",
    "\n",
    "def models_to_accuracy(m1,m2,dataset,accuracy_metric):\n",
    "    (encoder, _, _, _, _), (_, decoder, alphabet, _, _) = m1, m2\n",
    "    return accuracy_metric(encoder, decoder, alphabet, dataset)\n",
    "\n",
    "def accuracy_table(models, dataset, accuracy_metric=top1_accuracy):\n",
    "    d = {}\n",
    "    for m1 in models:\n",
    "        for m2 in models:\n",
    "            d[models_to_name(m1,m2)] = \\\n",
    "                models_to_accuracy(m1,m2,dataset,accuracy_metric) \n",
    "    return d\n",
    "    \n",
    "def visual_accuracy_table(models, dataset, accuracy_metric=top1_accuracy):\n",
    "    return pandas.DataFrame(\n",
    "            np.array([[models_to_accuracy(m1,m2,dataset,accuracy_metric) for m2 in models]\n",
    "                      for m1 in models]), \n",
    "            columns = [m[0].name()+m[3] for m in models],\n",
    "            index = [m[0].name()+m[3] for m in models])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RulesBasedEncoderPython(whitespace=False)python</th>\n",
       "      <th>UniformEncoder(0.80)python</th>\n",
       "      <th>FrequencyEncoder(5-gram, target_size:0.8)python</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RulesBasedEncoderPython(whitespace=False)python</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UniformEncoder(0.80)python</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FrequencyEncoder(5-gram, target_size:0.8)python</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 RulesBasedEncoderPython(whitespace=False)python  \\\n",
       "RulesBasedEncoderPython(whitespace=False)python                                             0.97   \n",
       "UniformEncoder(0.80)python                                                                  0.00   \n",
       "FrequencyEncoder(5-gram, target_size:0.8)python                                             0.00   \n",
       "\n",
       "                                                 UniformEncoder(0.80)python  \\\n",
       "RulesBasedEncoderPython(whitespace=False)python                        0.59   \n",
       "UniformEncoder(0.80)python                                             0.21   \n",
       "FrequencyEncoder(5-gram, target_size:0.8)python                        0.09   \n",
       "\n",
       "                                                 FrequencyEncoder(5-gram, target_size:0.8)python  \n",
       "RulesBasedEncoderPython(whitespace=False)python                                             0.19  \n",
       "UniformEncoder(0.80)python                                                                  0.01  \n",
       "FrequencyEncoder(5-gram, target_size:0.8)python                                             0.69  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#row labels correspond to the encoder, and column labels correspond to the decoder\n",
    "visual_accuracy_table(loaded_models,full_dataset['python']['test'][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule-based decodes its own encodings nearly perfectly, but cannot decode uniform or frequency -- it is overfitting to its own very simple encodings. We can see the simplicity of the rule-based encodings in the high uniform and frequency based decoding scores.\n",
    "\n",
    "The uniform decoder performs meagerly on its on encodings, but also performs non-negligibly on rule and frequency based encodings.\n",
    "\n",
    "Contrast this with the frequency decoder. It does well on itself and partially figures out rule-based, but it completely fails on uniform encodings. The frequency decoder is inflexible -- a small perterbation causes it to leave the regime where it can give confident predictions. This is because the frequency encoder is forced to only consider fixed deletions of n-grams in a fixed order, given by relative n-gram frequencies in the dataset.\n",
    "\n",
    "We can find a middle ground between the under and over fitting of uniform and frequency based encoders respectively by introducing noise into the frequency encoding:\n",
    "Each time an n-gram would be deleted instead flip a fair coin. If heads then continue normally, otherwise delete a random character. This allows the frequency encoder to generate greater variation during training, while not drowning out the benefits of preferentially deleting redundant characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model RulesBasedEncoderPython(whitespace=False) python loaded!\n",
      "Model UniformEncoder(0.80) python loaded!\n",
      "Model FrequencyEncoder(5-gram, target_size:0.8) python loaded!\n",
      "Model FuzzyFrequencyEncoder(5-gram, target_size:0.8) python loaded!\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "   (RulesBasedEncoder(), .0001, python_dat, 'python'),\n",
    "   (UniformEncoder(removal_probability=.8),.0005, python_dat, 'python'),\n",
    "   (FrequencyEncoder(dataset=python_dat['train'], compression_rate=.8, n_gram=5), .0005, python_dat, 'python'),    \n",
    "   (FuzzyFrequencyEncoder(dataset=python_dat['train'], compression_rate=.8, n_gram=5), .0005, python_dat, 'python')\n",
    "]\n",
    "loaded_models = load_models(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RulesBasedEncoderPython(whitespace=False)python</th>\n",
       "      <th>UniformEncoder(0.80)python</th>\n",
       "      <th>FrequencyEncoder(5-gram, target_size:0.8)python</th>\n",
       "      <th>FuzzyFrequencyEncoder(5-gram, target_size:0.8)python</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RulesBasedEncoderPython(whitespace=False)python</th>\n",
       "      <td>0.940</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UniformEncoder(0.80)python</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FrequencyEncoder(5-gram, target_size:0.8)python</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FuzzyFrequencyEncoder(5-gram, target_size:0.8)python</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    RulesBasedEncoderPython(whitespace=False)python  \\\n",
       "RulesBasedEncoderPython(whitespace=False)python                                               0.940   \n",
       "UniformEncoder(0.80)python                                                                    0.010   \n",
       "FrequencyEncoder(5-gram, target_size:0.8)python                                               0.005   \n",
       "FuzzyFrequencyEncoder(5-gram, target_size:0.8)p...                                            0.000   \n",
       "\n",
       "                                                    UniformEncoder(0.80)python  \\\n",
       "RulesBasedEncoderPython(whitespace=False)python                          0.640   \n",
       "UniformEncoder(0.80)python                                               0.160   \n",
       "FrequencyEncoder(5-gram, target_size:0.8)python                          0.080   \n",
       "FuzzyFrequencyEncoder(5-gram, target_size:0.8)p...                       0.015   \n",
       "\n",
       "                                                    FrequencyEncoder(5-gram, target_size:0.8)python  \\\n",
       "RulesBasedEncoderPython(whitespace=False)python                                               0.185   \n",
       "UniformEncoder(0.80)python                                                                    0.005   \n",
       "FrequencyEncoder(5-gram, target_size:0.8)python                                               0.645   \n",
       "FuzzyFrequencyEncoder(5-gram, target_size:0.8)p...                                            0.000   \n",
       "\n",
       "                                                    FuzzyFrequencyEncoder(5-gram, target_size:0.8)python  \n",
       "RulesBasedEncoderPython(whitespace=False)python                                                 0.160     \n",
       "UniformEncoder(0.80)python                                                                      0.070     \n",
       "FrequencyEncoder(5-gram, target_size:0.8)python                                                 0.035     \n",
       "FuzzyFrequencyEncoder(5-gram, target_size:0.8)p...                                              0.230     "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual_accuracy_table(loaded_models,full_dataset['python']['test'][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a decoder that performs well enough on rules and uniform, but also performs better than uniform on its own encodings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These metrics are helpful, but to really get a feel for the efficacies of these learned decoders we will need to see examples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<baseline.RulesBasedEncoder object at 0x7feb6a6b9b38> \n",
      " for i in range(10): \n",
      "\n",
      "['for i in range(10):', 'for i in range(10):]'] \n",
      "\n",
      "<baseline.UniformEncoder object at 0x7feb6a6b9b00> \n",
      " for i in range(10): \n",
      "\n",
      "['for i in range(10):', 'for i in range(100):'] \n",
      "\n",
      "<baseline.FrequencyEncoder object at 0x7feb6a6b9780> \n",
      " for i in rng(10): \n",
      "\n",
      "['for i in rng(10000):', 'for i in rng(10):'] \n",
      "\n",
      "<baseline.FuzzyFrequencyEncoder object at 0x7feb6a6b9748> \n",
      " for i in range(10): \n",
      "\n",
      "['for i in range(10):', 'for i in range(100):'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_models_on_string(\"for i in rng(10):\", loaded_models, beam=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<baseline.RulesBasedEncoder object at 0x7feb6a6b9b38> \n",
      " r i i r10): \n",
      "\n",
      "['r i i r10):', 'r i i  0):'] \n",
      "\n",
      "<baseline.UniformEncoder object at 0x7feb6a6b9b00> \n",
      " for i in r100000000000000000): \n",
      "\n",
      "['for i in r10):', 'for i in r1000000):'] \n",
      "\n",
      "<baseline.FrequencyEncoder object at 0x7feb6a6b9780> \n",
      " rul, i == r10): \n",
      "\n",
      "['rum, i in r10):', 'run, i in r10):'] \n",
      "\n",
      "<baseline.FuzzyFrequencyEncoder object at 0x7feb6a6b9748> \n",
      " for i in range(10): \n",
      "\n",
      "['for i in range(10):', 'for i in range(1,0):'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_models_on_string(\"r i i r10):\", loaded_models, beam=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoders that do the right sort of thing in practice are uniform and fuzzy frequency, even though fuzzy frequency didn't come out amazingly well in the above accuracies. In the last example, the n-gram training helps FuzzyFrequencyEncoder recover the full word 'range' from just 'r'. \n",
    "The extra ability of uniform and fuzzy frequency will come out clearer when evaluating on whether the correct answer is in the top five predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RulesBasedEncoderPython(whitespace=False)python</th>\n",
       "      <th>UniformEncoder(0.80)python</th>\n",
       "      <th>FrequencyEncoder(5-gram, target_size:0.8)python</th>\n",
       "      <th>FuzzyFrequencyEncoder(5-gram, target_size:0.8)python</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RulesBasedEncoderPython(whitespace=False)python</th>\n",
       "      <td>0.980</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UniformEncoder(0.80)python</th>\n",
       "      <td>0.015</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FrequencyEncoder(5-gram, target_size:0.8)python</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FuzzyFrequencyEncoder(5-gram, target_size:0.8)python</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    RulesBasedEncoderPython(whitespace=False)python  \\\n",
       "RulesBasedEncoderPython(whitespace=False)python                                               0.980   \n",
       "UniformEncoder(0.80)python                                                                    0.015   \n",
       "FrequencyEncoder(5-gram, target_size:0.8)python                                               0.005   \n",
       "FuzzyFrequencyEncoder(5-gram, target_size:0.8)p...                                            0.000   \n",
       "\n",
       "                                                    UniformEncoder(0.80)python  \\\n",
       "RulesBasedEncoderPython(whitespace=False)python                          0.805   \n",
       "UniformEncoder(0.80)python                                               0.340   \n",
       "FrequencyEncoder(5-gram, target_size:0.8)python                          0.270   \n",
       "FuzzyFrequencyEncoder(5-gram, target_size:0.8)p...                       0.060   \n",
       "\n",
       "                                                    FrequencyEncoder(5-gram, target_size:0.8)python  \\\n",
       "RulesBasedEncoderPython(whitespace=False)python                                               0.385   \n",
       "UniformEncoder(0.80)python                                                                    0.010   \n",
       "FrequencyEncoder(5-gram, target_size:0.8)python                                               0.765   \n",
       "FuzzyFrequencyEncoder(5-gram, target_size:0.8)p...                                            0.000   \n",
       "\n",
       "                                                    FuzzyFrequencyEncoder(5-gram, target_size:0.8)python  \n",
       "RulesBasedEncoderPython(whitespace=False)python                                                 0.450     \n",
       "UniformEncoder(0.80)python                                                                      0.250     \n",
       "FrequencyEncoder(5-gram, target_size:0.8)python                                                 0.115     \n",
       "FuzzyFrequencyEncoder(5-gram, target_size:0.8)p...                                              0.410     "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top5_accuracy(encoder, decoder, alphabet, dataset):\n",
    "    count = 0\n",
    "    for string in dataset:\n",
    "        compressed = encoder.encode(string)\n",
    "        best_five = beam_search_predictions(decoder, compressed, alphabet, beam_size=5)\n",
    "        if string in best_five: count += 1\n",
    "    return float(count) / len(dataset)\n",
    "\n",
    "visual_accuracy_table(loaded_models,full_dataset['python']['test'][:200],top5_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, fuzzy frequency has a wider support in the top 5 accuracy model, and edges out uniform in self-accuracy. But if anything this chart further affirms the accuracy of the plain old uniform encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Put in a section here if the scheduled uniform or .6 uniform does well --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finish off by showing the uniform and fuzzy frequency decoders at their best and worst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model UniformEncoder(0.80) python loaded!\n",
      "Model FuzzyFrequencyEncoder(5-gram, target_size:0.8) python loaded!\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "   (UniformEncoder(removal_probability=.8),.0005, python_dat, 'python'),\n",
    "   (FuzzyFrequencyEncoder(dataset=python_dat['train'], compression_rate=.8, n_gram=5), .0005, python_dat, 'python')\n",
    "]\n",
    "loaded_models = load_models(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "def reset_seeds():\n",
    "    # Set seed before computing accuracy to make the encoders deterministic.\n",
    "    np.random.seed(224)\n",
    "    random.seed(224)\n",
    "    torch.manual_seed(224)\n",
    "    \n",
    "def batched_decode_all(model, dataset):\n",
    "    reset_seeds()\n",
    "    (encoder, decoder, alphabet, _, _) = model\n",
    "    encoded_strings = [encoder.encode(s) for s in dataset]\n",
    "    decoded = []    \n",
    "    for i in range((len(dataset) + BATCH_SIZE - 1) // BATCH_SIZE):\n",
    "        decoded.extend(decoder(encoded_strings[i*BATCH_SIZE:(i+1)*BATCH_SIZE], alphabet))\n",
    "    return list(zip(dataset, encoded_strings, decoded))\n",
    "\n",
    "def best_example(model, dataset, k=1):\n",
    "    correct = list(filter(lambda x: x[0]==x[2],\n",
    "                          batched_decode_all(model, dataset)))\n",
    "    correct_with_fxn = [(c[1],c[2],len(c[0])-len(c[1])) for c in correct]\n",
    "    return sorted(correct_with_fxn, key=lambda x: -x[2])[:k]\n",
    "\n",
    "def worst_example(model, dataset, k=1):\n",
    "    incorrect = list(filter(lambda x: x[0]!=x[2],\n",
    "                          batched_decode_all(model, dataset)))\n",
    "    incorrect_with_fxn = [(c[0],c[1],c[2],len(c[0])-len(c[1])) for c in incorrect]\n",
    "    return sorted(incorrect_with_fxn, key=lambda x: x[3])[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('BASEPTH = o.ath.dirnamo.patabpath__fil_))', 'BASE_PATH = os.path.dirname(os.path.abspath(__file__))', 13) \n",
      "\n",
      "('tod_ = smm.tod', 'tod_ = smm.tod', 'todo_ = smm.todo', 0)\n"
     ]
    }
   ],
   "source": [
    "unif8 = loaded_models[0]\n",
    "# prints (encoded, decoded, difference in length between original and compressed)\n",
    "print(best_example(unif8, full_dataset['python']['test'][:1000])[0],\"\\n\")\n",
    "# prints (original, encoded, decoded, different in length between original and compressed)\n",
    "print(worst_example(unif8, full_dataset['python']['test'][:1000])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"fotcreator = os.os.pth.dirnme(___), '..', fontcreator.py')\", \"fontcreator = os.path.join(os.path.dirname(__file__), '..', 'fontcreator.py')\", 19) \n",
      "\n",
      "('coord = []', 'ood = []', 'ood_list = []', 2)\n"
     ]
    }
   ],
   "source": [
    "ffreq8 = loaded_models[1]\n",
    "# prints (encoded, decoded, difference in length between original and compressed)\n",
    "print(best_example(ffreq8, full_dataset['python']['test'][:1000])[0],\"\\n\")\n",
    "# prints (original, encoded, decoded, different in length between original and compressed)\n",
    "print(worst_example(ffreq8, full_dataset['python']['test'][:1000])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final note -- if I was using this project for autocompletion in the wild, I would use either fuzzy frequency 0.8 or uniform 0.7. I did not bring up uniform 0.7 during the explanation because it performs slightly worse than 0.8 on our metrics.\n",
    "\n",
    "However, when looking at the output, it is able to get common-sense answers more of the time, and it is clear that it has to learn more of a language model in order to autocomplete with so many characters missing. Top 1 and top 5 accuracies are still missing an important piece of the evaluation for this project. We also considered a way to measure distance from desired string and use that as a metric, but we do not think this will give more clarity into the quality of encoder/decoder pairs.\n",
    "\n",
    "We might be able to quantify the performance of fuzzy frequency 0.8 and uniform 0.7 by using a language model on the translations, but this seems out of scope for our current effort. \n",
    "\n",
    "We also tried uniform 0.6 and a uniform curriculum where we decrease the probability of keeping characters during training. These led to some reasonable but not outstanding translations. I think it would be difficult to do much better than uniform 0.7 and fuzzy frequency 0.8 for translation through further ad-hoc methods. I would not be surprised if successfully learning the encoder and decoder simultaneously would lead to significantly better performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
