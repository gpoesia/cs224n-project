{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import baseline\n",
    "from encoder import *\n",
    "from baseline import *\n",
    "from decoder import *\n",
    "from alphabet import *\n",
    "from train import *\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76738 training examples, 9590 validation examples, 9616 test exampless\n"
     ]
    }
   ],
   "source": [
    "LANGUAGE = 'Python'\n",
    "\n",
    "def filter_ascii(strings):\n",
    "    'Returns only the strings that can be encoded in ASCII.'\n",
    "    l = []\n",
    "    for s in strings:\n",
    "        try:\n",
    "            s.encode('ascii')\n",
    "            if len(s) <= 80:\n",
    "                l.append(s)\n",
    "        except UnicodeEncodeError:\n",
    "            pass\n",
    "        \n",
    "    return l\n",
    "\n",
    "with open('dataset/medium.json') as f:\n",
    "    multilang_dataset = json.load(f)\n",
    "    dataset = multilang_dataset[LANGUAGE]\n",
    "    \n",
    "    dataset['train'] = filter_ascii(dataset['train'])\n",
    "    dataset['dev'] = filter_ascii(dataset['dev'])\n",
    "    dataset['test'] = filter_ascii(dataset['test'])\n",
    "    \n",
    "    tiny_dataset = {\n",
    "        'train': dataset['train'][:50],\n",
    "        'dev': dataset['train'][:50],\n",
    "        'test': dataset['train'][:50],\n",
    "    }\n",
    "    \n",
    "    print('{} training examples, {} validation examples, {} test exampless'.format(\n",
    "        len(dataset['train']), \n",
    "        len(dataset['dev']),\n",
    "        len(dataset['test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dumb_dataset():\n",
    "    'Returns a dataset where all examples are the same string, which consists of 10 times the same letter.'\n",
    "\n",
    "    SIZE = 200\n",
    "    l = []\n",
    "\n",
    "    for i in range(SIZE):\n",
    "        l.append(random.choice('abcdefghijklmnopqrstuvwxyz') * random.choice([5, 10]))\n",
    "        \n",
    "    return {'train': l, 'dev': l, 'test': l}\n",
    "\n",
    "dumb_dataset = generate_dumb_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "hello darkness\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(0) if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "alphabet = AsciiOneHotEncoding(device)\n",
    "encoder = baseline.UniformEncoder(0.9)\n",
    "decoder = AutoCompleteDecoderModel(alphabet, hidden_size=64)\n",
    "nencoder = NeuralEncoder(alphabet, epsilon=0.5, hidden_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nencoder([\"hello\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240.10151319598083\n"
     ]
    }
   ],
   "source": [
    "def expected_initial_loss(input_string, epsilon, alphabet, lam):\n",
    "    s, a = len(input_string), alphabet.size()\n",
    "    return s/2.0 + lam*(s*math.log(a)-epsilon)\n",
    "\n",
    "print(expected_initial_loss('ddddd',0.5,alphabet,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lam:  tensor(10., requires_grad=True)\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddd']\n",
      "per_prediction_loss:  tensor([29.1122], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-4.0009e-10, -6.5455e-10,  1.1578e-10,  ..., -9.2728e-11,\n",
      "         -5.0189e-11, -9.0278e-11],\n",
      "        [-2.5475e-09, -4.0080e-09,  6.6233e-10,  ..., -5.8527e-10,\n",
      "         -3.6778e-10, -5.4992e-10],\n",
      "        [ 3.7202e-10,  7.2577e-10, -1.6257e-10,  ...,  8.9990e-11,\n",
      "          1.1289e-11,  1.0220e-10],\n",
      "        ...,\n",
      "        [-3.5533e-11, -2.7469e-10,  1.1180e-10,  ..., -1.5154e-11,\n",
      "          6.1033e-11, -4.1753e-11],\n",
      "        [-3.0619e-09, -4.8137e-09,  7.9436e-10,  ..., -7.0333e-10,\n",
      "         -4.4314e-10, -6.6040e-10],\n",
      "        [-3.5018e-10, -6.6098e-10,  1.4263e-10,  ..., -8.3966e-11,\n",
      "         -1.7277e-11, -9.2734e-11]])\n",
      "Epoch 0 iteration 0: loss = -291.122, tp = 29.20 lines/s, ETA 00h00m20s\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ttttt']\n",
      "per_prediction_loss:  tensor([53.3732], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-2.1936e-10,  3.9865e-09, -9.6195e-10,  ..., -1.0778e-09,\n",
      "         -1.2576e-09,  1.8057e-09],\n",
      "        [-2.9554e-10,  4.6887e-09, -1.1167e-09,  ..., -1.2686e-09,\n",
      "         -1.4750e-09,  2.1154e-09],\n",
      "        [ 7.1022e-10, -1.0325e-08,  2.4357e-09,  ...,  2.7931e-09,\n",
      "          3.2383e-09, -4.6400e-09],\n",
      "        ...,\n",
      "        [ 4.1952e-10, -5.7018e-09,  1.3343e-09,  ...,  1.5429e-09,\n",
      "          1.7848e-09, -2.5554e-09],\n",
      "        [ 1.0996e-09, -1.4733e-08,  3.4417e-09,  ...,  3.9873e-09,\n",
      "          4.6105e-09, -6.6004e-09],\n",
      "        [-1.0568e-09,  1.4439e-08, -3.3811e-09,  ..., -3.9063e-09,\n",
      "         -4.5196e-09,  6.4715e-09]])\n",
      "batch:  ['qqqqqqqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['qqqqqqqq']\n",
      "per_prediction_loss:  tensor([53.3728], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-2.6950e-09,  1.2375e-08,  5.3309e-09,  ..., -5.9855e-10,\n",
      "         -1.6736e-09,  2.2829e-09],\n",
      "        [ 6.8475e-10, -3.1315e-09, -1.3432e-09,  ...,  1.7906e-10,\n",
      "          4.2871e-10, -5.7670e-10],\n",
      "        [ 8.2768e-09, -3.7633e-08, -1.6043e-08,  ...,  2.6958e-09,\n",
      "          5.3618e-09, -7.0703e-09],\n",
      "        ...,\n",
      "        [ 3.4155e-09, -1.5437e-08, -6.5389e-09,  ...,  1.3270e-09,\n",
      "          2.2721e-09, -2.9385e-09],\n",
      "        [ 1.0635e-08, -4.8556e-08, -2.0792e-08,  ...,  2.9955e-09,\n",
      "          6.7638e-09, -9.0457e-09],\n",
      "        [-7.8241e-09,  3.5545e-08,  1.5139e-08,  ..., -2.6144e-09,\n",
      "         -5.0841e-09,  6.6860e-09]])\n",
      "Epoch 0 iteration 2: loss = -538.728, tp = 26.30 lines/s, ETA 00h00m22s\n",
      "batch:  ['nnnnnnnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnn']\n",
      "per_prediction_loss:  tensor([53.3695], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 4.1046e-09, -4.5549e-08, -1.3031e-08,  ...,  2.9825e-09,\n",
      "          8.1332e-09, -2.3059e-08],\n",
      "        [-2.9362e-10, -8.7492e-10, -3.7854e-10,  ..., -7.2857e-10,\n",
      "         -2.8700e-10,  2.4210e-10],\n",
      "        [-1.9767e-08,  2.0532e-07,  5.8300e-08,  ..., -1.6129e-08,\n",
      "         -3.8175e-08,  1.0628e-07],\n",
      "        ...,\n",
      "        [-1.3257e-08,  1.3254e-07,  3.7465e-08,  ..., -1.1465e-08,\n",
      "         -2.5238e-08,  6.9529e-08],\n",
      "        [-1.6155e-08,  1.5959e-07,  4.5046e-08,  ..., -1.4213e-08,\n",
      "         -3.0619e-08,  8.4075e-08],\n",
      "        [ 1.4617e-08, -1.5411e-07, -4.3835e-08,  ...,  1.1642e-08,\n",
      "          2.8392e-08, -7.9367e-08]])\n",
      "batch:  ['bbbbbbbbbb']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['bbbb']\n",
      "per_prediction_loss:  tensor([53.3928], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 5.9734e-09,  3.1427e-08,  2.8232e-09,  ...,  7.7852e-09,\n",
      "         -7.3723e-10,  1.4461e-08],\n",
      "        [-2.5299e-08, -8.5584e-08, -4.9229e-09,  ..., -2.1175e-08,\n",
      "          4.3589e-09, -3.5336e-08],\n",
      "        [-1.9131e-07, -7.4244e-07, -5.1329e-08,  ..., -1.8374e-07,\n",
      "          3.0492e-08, -3.1906e-07],\n",
      "        ...,\n",
      "        [-1.4700e-07, -5.5749e-07, -3.7515e-08,  ..., -1.3796e-07,\n",
      "          2.3766e-08, -2.3809e-07],\n",
      "        [-2.2463e-07, -8.4510e-07, -5.6323e-08,  ..., -2.0913e-07,\n",
      "          3.6493e-08, -3.6011e-07],\n",
      "        [ 1.3524e-07,  5.2551e-07,  3.6382e-08,  ...,  1.3006e-07,\n",
      "         -2.1537e-08,  2.2593e-07]])\n",
      "Epoch 0 iteration 4: loss = -532.928, tp = 26.22 lines/s, ETA 00h00m22s\n",
      "batch:  ['pppppppppp']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['pppp']\n",
      "per_prediction_loss:  tensor([53.4133], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 3.6543e-08,  1.4184e-07,  5.6895e-08,  ...,  4.5250e-08,\n",
      "          3.0234e-09,  6.0678e-08],\n",
      "        [-7.8166e-08, -2.7872e-07, -1.2185e-07,  ..., -8.7131e-08,\n",
      "         -4.8412e-09, -1.2195e-07],\n",
      "        [-4.2131e-07, -1.5669e-06, -6.5636e-07,  ..., -4.9503e-07,\n",
      "         -3.0349e-08, -6.7799e-07],\n",
      "        ...,\n",
      "        [-2.5836e-07, -9.4839e-07, -4.0256e-07,  ..., -2.9876e-07,\n",
      "         -1.7792e-08, -4.1191e-07],\n",
      "        [-4.8973e-07, -1.7986e-06, -7.6309e-07,  ..., -5.6663e-07,\n",
      "         -3.3783e-08, -7.8102e-07],\n",
      "        [ 2.6879e-07,  1.0017e-06,  4.1873e-07,  ...,  3.1660e-07,\n",
      "          1.9499e-08,  4.3317e-07]])\n",
      "batch:  ['jjjjjjjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjjjj']\n",
      "per_prediction_loss:  tensor([53.4407], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-3.4471e-08, -6.0139e-08, -2.6573e-08,  ..., -1.6713e-08,\n",
      "         -8.6807e-09, -1.2845e-08],\n",
      "        [-2.0092e-06, -4.2923e-06, -1.6964e-06,  ..., -1.1603e-06,\n",
      "         -4.1651e-07, -1.4266e-06],\n",
      "        [-5.3122e-06, -1.1426e-05, -4.4998e-06,  ..., -3.0860e-06,\n",
      "         -1.0923e-06, -3.8387e-06],\n",
      "        ...,\n",
      "        [-4.1480e-06, -8.8936e-06, -3.5083e-06,  ..., -2.4031e-06,\n",
      "         -8.5625e-07, -2.9728e-06],\n",
      "        [-7.0909e-06, -1.5209e-05, -5.9984e-06,  ..., -4.1092e-06,\n",
      "         -1.4631e-06, -5.0866e-06],\n",
      "        [ 2.8506e-06,  6.1379e-06,  2.4159e-06,  ...,  1.6575e-06,\n",
      "          5.8538e-07,  2.0656e-06]])\n",
      "Epoch 0 iteration 6: loss = -535.407, tp = 25.19 lines/s, ETA 00h00m23s\n",
      "batch:  ['ooooo']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['o']\n",
      "per_prediction_loss:  tensor([29.2795], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 1.3684e-06,  5.5171e-06,  2.1537e-06,  ...,  7.2153e-07,\n",
      "          1.0425e-06,  1.4774e-06],\n",
      "        [-2.6090e-06, -1.0771e-05, -4.2260e-06,  ..., -1.2448e-06,\n",
      "         -2.0384e-06, -2.6982e-06],\n",
      "        [-7.1311e-06, -2.8964e-05, -1.1325e-05,  ..., -3.6492e-06,\n",
      "         -5.4759e-06, -7.5988e-06],\n",
      "        ...,\n",
      "        [-5.4792e-06, -2.2444e-05, -8.7918e-06,  ..., -2.7053e-06,\n",
      "         -4.2456e-06, -5.7492e-06],\n",
      "        [-8.9182e-06, -3.6476e-05, -1.4284e-05,  ..., -4.4319e-06,\n",
      "         -6.8991e-06, -9.3836e-06],\n",
      "        [ 4.4696e-06,  1.8141e-05,  7.0921e-06,  ...,  2.2940e-06,\n",
      "          3.4295e-06,  4.7690e-06]])\n",
      "batch:  ['xxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xxxx']\n",
      "per_prediction_loss:  tensor([29.6355], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 4.0595e-05,  9.8422e-05,  5.0504e-05,  ...,  3.7325e-05,\n",
      "          2.1493e-05,  5.4492e-05],\n",
      "        [-4.8569e-05, -1.1834e-04, -6.0291e-05,  ..., -4.4666e-05,\n",
      "         -2.5849e-05, -6.5243e-05],\n",
      "        [-1.3821e-04, -3.3546e-04, -1.7187e-04,  ..., -1.2709e-04,\n",
      "         -7.3260e-05, -1.8556e-04],\n",
      "        ...,\n",
      "        [-1.0679e-04, -2.5971e-04, -1.3267e-04,  ..., -9.8200e-05,\n",
      "         -5.6724e-05, -1.4341e-04],\n",
      "        [-1.5855e-04, -3.8552e-04, -1.9700e-04,  ..., -1.4580e-04,\n",
      "         -8.4202e-05, -2.1292e-04],\n",
      "        [ 9.7251e-05,  2.3603e-04,  1.2093e-04,  ...,  8.9421e-05,\n",
      "          5.1548e-05,  1.3056e-04]])\n",
      "Epoch 0 iteration 8: loss = -297.355, tp = 26.93 lines/s, ETA 00h00m21s\n",
      "batch:  ['fffff']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['fff']\n",
      "per_prediction_loss:  tensor([30.2161], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 0.0002,  0.0004,  0.0002,  ...,  0.0002,  0.0001,  0.0002],\n",
      "        [-0.0002, -0.0005, -0.0003,  ..., -0.0002, -0.0002, -0.0003],\n",
      "        [-0.0005, -0.0012, -0.0006,  ..., -0.0004, -0.0004, -0.0007],\n",
      "        ...,\n",
      "        [-0.0004, -0.0010, -0.0005,  ..., -0.0004, -0.0003, -0.0006],\n",
      "        [-0.0006, -0.0013, -0.0007,  ..., -0.0005, -0.0004, -0.0008],\n",
      "        [ 0.0004,  0.0009,  0.0004,  ...,  0.0003,  0.0003,  0.0005]])\n",
      "batch:  ['qqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['qqq']\n",
      "per_prediction_loss:  tensor([35.5866], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 0.0022,  0.0044,  0.0026,  ...,  0.0019,  0.0016,  0.0028],\n",
      "        [-0.0020, -0.0037, -0.0023,  ..., -0.0016, -0.0014, -0.0024],\n",
      "        [-0.0046, -0.0090, -0.0053,  ..., -0.0039, -0.0032, -0.0057],\n",
      "        ...,\n",
      "        [-0.0034, -0.0065, -0.0039,  ..., -0.0028, -0.0024, -0.0041],\n",
      "        [-0.0044, -0.0085, -0.0050,  ..., -0.0037, -0.0031, -0.0054],\n",
      "        [ 0.0037,  0.0072,  0.0042,  ...,  0.0031,  0.0026,  0.0046]])\n",
      "Epoch 0 iteration 10: loss = -354.866, tp = 27.99 lines/s, ETA 00h00m21s\n",
      "batch:  ['qqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['qq']\n",
      "per_prediction_loss:  tensor([39.7165], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[ 0.0025,  0.0044,  0.0029,  ...,  0.0019,  0.0016,  0.0027],\n",
      "        [-0.0020, -0.0036, -0.0023,  ..., -0.0015, -0.0013, -0.0021],\n",
      "        [-0.0044, -0.0077, -0.0050,  ..., -0.0033, -0.0029, -0.0047],\n",
      "        ...,\n",
      "        [-0.0031, -0.0055, -0.0036,  ..., -0.0024, -0.0021, -0.0033],\n",
      "        [-0.0040, -0.0070, -0.0045,  ..., -0.0030, -0.0026, -0.0042],\n",
      "        [ 0.0036,  0.0064,  0.0042,  ...,  0.0027,  0.0024,  0.0039]])\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['m']\n",
      "per_prediction_loss:  tensor([32.7852], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 4.6837e-05,  8.7030e-05,  5.1530e-05,  ...,  3.0197e-05,\n",
      "          2.0806e-05,  3.9858e-05],\n",
      "        [-6.6976e-05, -1.2455e-04, -7.3796e-05,  ..., -4.4168e-05,\n",
      "         -3.0416e-05, -5.8277e-05],\n",
      "        [-4.8903e-05, -9.0803e-05, -5.3728e-05,  ..., -3.0851e-05,\n",
      "         -2.1267e-05, -4.0738e-05],\n",
      "        ...,\n",
      "        [-6.5648e-05, -1.2203e-04, -7.2278e-05,  ..., -4.2801e-05,\n",
      "         -2.9482e-05, -5.6485e-05],\n",
      "        [-6.1879e-05, -1.1498e-04, -6.8081e-05,  ..., -3.9915e-05,\n",
      "         -2.7501e-05, -5.2686e-05],\n",
      "        [ 5.3950e-05,  1.0022e-04,  5.9329e-05,  ...,  3.4538e-05,\n",
      "          2.3801e-05,  4.5594e-05]])\n",
      "Epoch 0 iteration 12: loss = -324.852, tp = 28.59 lines/s, ETA 00h00m20s\n",
      "batch:  ['bbbbb']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['bb']\n",
      "per_prediction_loss:  tensor([45.0065], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 0.0009,  0.0017,  0.0011,  ...,  0.0006,  0.0005,  0.0008],\n",
      "        [-0.0006, -0.0010, -0.0007,  ..., -0.0004, -0.0003, -0.0005],\n",
      "        [-0.0013, -0.0023, -0.0015,  ..., -0.0008, -0.0007, -0.0011],\n",
      "        ...,\n",
      "        [-0.0009, -0.0016, -0.0010,  ..., -0.0006, -0.0005, -0.0007],\n",
      "        [-0.0011, -0.0020, -0.0013,  ..., -0.0007, -0.0006, -0.0009],\n",
      "        [ 0.0011,  0.0021,  0.0013,  ...,  0.0007,  0.0006,  0.0010]])\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dddd']\n",
      "per_prediction_loss:  tensor([38.1961], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 2.3578e-08,  1.8862e-08,  1.7191e-08,  ...,  9.6248e-09,\n",
      "          6.9152e-09,  1.8112e-08],\n",
      "        [-2.1444e-08, -1.6172e-08, -1.5122e-08,  ..., -9.4944e-09,\n",
      "         -6.8242e-09, -1.7731e-08],\n",
      "        [-2.7694e-08, -2.3007e-08, -2.0638e-08,  ..., -1.0661e-08,\n",
      "         -7.6577e-09, -2.0181e-08],\n",
      "        ...,\n",
      "        [-2.6137e-08, -2.1212e-08, -1.9215e-08,  ..., -1.0440e-08,\n",
      "         -7.5003e-09, -1.9689e-08],\n",
      "        [-3.1114e-08, -2.6754e-08, -2.3660e-08,  ..., -1.1295e-08,\n",
      "         -8.1104e-09, -2.1514e-08],\n",
      "        [ 2.7125e-08,  2.2403e-08,  2.0145e-08,  ...,  1.0541e-08,\n",
      "          7.5718e-09,  1.9934e-08]])\n",
      "Epoch 0 iteration 14: loss = -380.961, tp = 29.34 lines/s, ETA 00h00m19s\n",
      "batch:  ['zzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['']\n",
      "per_prediction_loss:  tensor([33.7344], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-1.3058e-06, -2.1291e-06, -1.4325e-06,  ..., -1.3635e-06,\n",
      "         -1.0123e-06, -1.6731e-06],\n",
      "        [ 1.0043e-06,  1.6373e-06,  1.1017e-06,  ...,  1.0466e-06,\n",
      "          7.7698e-07,  1.2844e-06],\n",
      "        [ 1.7341e-06,  2.8277e-06,  1.9024e-06,  ...,  1.8131e-06,\n",
      "          1.3461e-06,  2.2247e-06],\n",
      "        ...,\n",
      "        [ 1.3279e-06,  2.1652e-06,  1.4568e-06,  ...,  1.3864e-06,\n",
      "          1.0293e-06,  1.7012e-06],\n",
      "        [ 1.6179e-06,  2.6381e-06,  1.7749e-06,  ...,  1.6909e-06,\n",
      "          1.2554e-06,  2.0748e-06],\n",
      "        [-1.5693e-06, -2.5589e-06, -1.7216e-06,  ..., -1.6400e-06,\n",
      "         -1.2176e-06, -2.0123e-06]])\n",
      "batch:  ['ffffffffff']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['fffffff']\n",
      "per_prediction_loss:  tensor([84.7915], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-1.8567e-07, -1.4359e-07, -1.8663e-07,  ..., -2.4012e-07,\n",
      "         -2.2024e-07, -3.0281e-07],\n",
      "        [ 1.8879e-07,  1.4602e-07,  1.8976e-07,  ...,  2.4409e-07,\n",
      "          2.2388e-07,  3.0782e-07],\n",
      "        [ 1.9124e-07,  1.4797e-07,  1.9223e-07,  ...,  2.4702e-07,\n",
      "          2.2656e-07,  3.1150e-07],\n",
      "        ...,\n",
      "        [ 1.9142e-07,  1.4811e-07,  1.9241e-07,  ...,  2.4725e-07,\n",
      "          2.2678e-07,  3.1180e-07],\n",
      "        [ 1.9270e-07,  1.4911e-07,  1.9370e-07,  ...,  2.4885e-07,\n",
      "          2.2824e-07,  3.1381e-07],\n",
      "        [-1.9084e-07, -1.4765e-07, -1.9182e-07,  ..., -2.4653e-07,\n",
      "         -2.2612e-07, -3.1090e-07]])\n",
      "Epoch 0 iteration 16: loss = -851.916, tp = 29.56 lines/s, ETA 00h00m19s\n",
      "batch:  ['kkkkkkkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kkkkkk']\n",
      "per_prediction_loss:  tensor([59.2526], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 2.4600e-09,  2.4512e-09,  2.4738e-09,  ...,  2.3509e-10,\n",
      "          2.2045e-10,  2.9694e-10],\n",
      "        [-2.4834e-09, -2.4744e-09, -2.4974e-09,  ..., -2.3783e-10,\n",
      "         -2.2298e-10, -3.0035e-10],\n",
      "        [-2.5018e-09, -2.4927e-09, -2.5160e-09,  ..., -2.3962e-10,\n",
      "         -2.2466e-10, -3.0261e-10],\n",
      "        ...,\n",
      "        [-2.5030e-09, -2.4939e-09, -2.5172e-09,  ..., -2.3985e-10,\n",
      "         -2.2487e-10, -3.0289e-10],\n",
      "        [-2.5113e-09, -2.5020e-09, -2.5255e-09,  ..., -2.4093e-10,\n",
      "         -2.2587e-10, -3.0422e-10],\n",
      "        [ 2.4981e-09,  2.4889e-09,  2.5122e-09,  ...,  2.3943e-10,\n",
      "          2.2448e-10,  3.0235e-10]])\n",
      "batch:  ['aaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaaaa']\n",
      "per_prediction_loss:  tensor([36.6146], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-3.5822e-12, -3.9240e-12, -3.5826e-12,  ...,  4.4918e-13,\n",
      "          3.5811e-13,  4.9277e-13],\n",
      "        [ 3.6213e-12,  3.9669e-12,  3.6217e-12,  ..., -4.5409e-13,\n",
      "         -3.6202e-13, -4.9815e-13],\n",
      "        [ 3.6423e-12,  3.9898e-12,  3.6427e-12,  ..., -4.5672e-13,\n",
      "         -3.6412e-13, -5.0103e-13],\n",
      "        ...,\n",
      "        [ 3.6427e-12,  3.9903e-12,  3.6431e-12,  ..., -4.5677e-13,\n",
      "         -3.6416e-13, -5.0109e-13],\n",
      "        [ 3.6572e-12,  4.0061e-12,  3.6576e-12,  ..., -4.5859e-13,\n",
      "         -3.6561e-13, -5.0309e-13],\n",
      "        [-3.6379e-12, -3.9850e-12, -3.6383e-12,  ...,  4.5617e-13,\n",
      "          3.6368e-13,  5.0044e-13]])\n",
      "Epoch 0 iteration 18: loss = -366.146, tp = 29.48 lines/s, ETA 00h00m19s\n",
      "batch:  ['aaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aa']\n",
      "per_prediction_loss:  tensor([39.6935], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-3.5250e-19, -4.6615e-19, -3.7777e-19,  ...,  1.7180e-19,\n",
      "          1.3864e-19,  1.8857e-19],\n",
      "        [ 3.5516e-19,  4.6966e-19,  3.8062e-19,  ..., -1.7310e-19,\n",
      "         -1.3968e-19, -1.8999e-19],\n",
      "        [ 3.5673e-19,  4.7174e-19,  3.8230e-19,  ..., -1.7387e-19,\n",
      "         -1.4030e-19, -1.9083e-19],\n",
      "        ...,\n",
      "        [ 3.5687e-19,  4.7193e-19,  3.8245e-19,  ..., -1.7393e-19,\n",
      "         -1.4036e-19, -1.9090e-19],\n",
      "        [ 3.5800e-19,  4.7342e-19,  3.8366e-19,  ..., -1.7448e-19,\n",
      "         -1.4080e-19, -1.9151e-19],\n",
      "        [-3.5644e-19, -4.7135e-19, -3.8199e-19,  ...,  1.7372e-19,\n",
      "          1.4019e-19,  1.9067e-19]])\n",
      "batch:  ['eeeeeeeeee']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eeee']\n",
      "per_prediction_loss:  tensor([61.4902], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-1.2076e-17, -1.3266e-17, -1.1993e-17,  ...,  1.8194e-18,\n",
      "          1.5555e-18,  2.0764e-18],\n",
      "        [ 1.2127e-17,  1.3322e-17,  1.2043e-17,  ..., -1.8271e-18,\n",
      "         -1.5621e-18, -2.0851e-18],\n",
      "        [ 1.2157e-17,  1.3355e-17,  1.2073e-17,  ..., -1.8316e-18,\n",
      "         -1.5660e-18, -2.0903e-18],\n",
      "        ...,\n",
      "        [ 1.2161e-17,  1.3359e-17,  1.2077e-17,  ..., -1.8322e-18,\n",
      "         -1.5665e-18, -2.0910e-18],\n",
      "        [ 1.2177e-17,  1.3377e-17,  1.2093e-17,  ..., -1.8346e-18,\n",
      "         -1.5685e-18, -2.0937e-18],\n",
      "        [-1.2152e-17, -1.3349e-17, -1.2068e-17,  ...,  1.8308e-18,\n",
      "          1.5653e-18,  2.0895e-18]])\n",
      "Epoch 0 iteration 20: loss = -613.902, tp = 29.58 lines/s, ETA 00h00m19s\n",
      "batch:  ['kkkkkkkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kkkkk']\n",
      "per_prediction_loss:  tensor([80.8786], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 1.0375e-09,  1.0676e-09,  1.0304e-09,  ..., -1.3218e-10,\n",
      "         -1.1717e-10, -1.4335e-10],\n",
      "        [-1.0412e-09, -1.0714e-09, -1.0341e-09,  ...,  1.3265e-10,\n",
      "          1.1759e-10,  1.4386e-10],\n",
      "        [-1.0430e-09, -1.0732e-09, -1.0358e-09,  ...,  1.3288e-10,\n",
      "          1.1779e-10,  1.4410e-10],\n",
      "        ...,\n",
      "        [-1.0432e-09, -1.0735e-09, -1.0361e-09,  ...,  1.3291e-10,\n",
      "          1.1782e-10,  1.4414e-10],\n",
      "        [-1.0445e-09, -1.0747e-09, -1.0373e-09,  ...,  1.3307e-10,\n",
      "          1.1796e-10,  1.4431e-10],\n",
      "        [ 1.0426e-09,  1.0728e-09,  1.0355e-09,  ..., -1.3283e-10,\n",
      "         -1.1774e-10, -1.4405e-10]])\n",
      "batch:  ['ooooo']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['oooo']\n",
      "per_prediction_loss:  tensor([59.2661], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 6.9698e-16,  7.2963e-16,  6.8910e-16,  ..., -1.0656e-16,\n",
      "         -9.2942e-17, -1.1801e-16],\n",
      "        [-6.9898e-16, -7.3172e-16, -6.9108e-16,  ...,  1.0687e-16,\n",
      "          9.3209e-17,  1.1835e-16],\n",
      "        [-6.9978e-16, -7.3256e-16, -6.9187e-16,  ...,  1.0699e-16,\n",
      "          9.3316e-17,  1.1849e-16],\n",
      "        ...,\n",
      "        [-6.9996e-16, -7.3275e-16, -6.9205e-16,  ...,  1.0702e-16,\n",
      "          9.3339e-17,  1.1852e-16],\n",
      "        [-7.0057e-16, -7.3338e-16, -6.9265e-16,  ...,  1.0711e-16,\n",
      "          9.3420e-17,  1.1862e-16],\n",
      "        [ 6.9960e-16,  7.3237e-16,  6.9169e-16,  ..., -1.0696e-16,\n",
      "         -9.3291e-17, -1.1845e-16]])\n",
      "Epoch 0 iteration 22: loss = -592.661, tp = 29.47 lines/s, ETA 00h00m19s\n",
      "batch:  ['kkkkkkkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kk']\n",
      "per_prediction_loss:  tensor([93.3656], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-4.4815e-30, -5.4923e-30, -4.5615e-30,  ...,  1.7880e-30,\n",
      "          1.6027e-30,  1.9125e-30],\n",
      "        [ 4.4928e-30,  5.5062e-30,  4.5730e-30,  ..., -1.7925e-30,\n",
      "         -1.6068e-30, -1.9174e-30],\n",
      "        [ 4.4966e-30,  5.5108e-30,  4.5769e-30,  ..., -1.7940e-30,\n",
      "         -1.6081e-30, -1.9190e-30],\n",
      "        ...,\n",
      "        [ 4.4976e-30,  5.5120e-30,  4.5779e-30,  ..., -1.7944e-30,\n",
      "         -1.6085e-30, -1.9194e-30],\n",
      "        [ 4.5010e-30,  5.5162e-30,  4.5813e-30,  ..., -1.7958e-30,\n",
      "         -1.6097e-30, -1.9208e-30],\n",
      "        [-4.4958e-30, -5.5098e-30, -4.5760e-30,  ...,  1.7937e-30,\n",
      "          1.6078e-30,  1.9186e-30]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  ['fffff']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ff']\n",
      "per_prediction_loss:  tensor([71.4551], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-1.6685e-33, -1.9693e-33, -1.7052e-33,  ...,  6.6245e-34,\n",
      "          5.7541e-34,  7.3709e-34],\n",
      "        [ 1.6717e-33,  1.9730e-33,  1.7085e-33,  ..., -6.6372e-34,\n",
      "         -5.7652e-34, -7.3850e-34],\n",
      "        [ 1.6727e-33,  1.9742e-33,  1.7095e-33,  ..., -6.6412e-34,\n",
      "         -5.7686e-34, -7.3895e-34],\n",
      "        ...,\n",
      "        [ 1.6730e-33,  1.9745e-33,  1.7097e-33,  ..., -6.6422e-34,\n",
      "         -5.7695e-34, -7.3906e-34],\n",
      "        [ 1.6740e-33,  1.9757e-33,  1.7107e-33,  ..., -6.6461e-34,\n",
      "         -5.7729e-34, -7.3950e-34],\n",
      "        [-1.6725e-33, -1.9739e-33, -1.7092e-33,  ...,  6.6401e-34,\n",
      "          5.7677e-34,  7.3883e-34]])\n",
      "Epoch 0 iteration 24: loss = -711.551, tp = 29.22 lines/s, ETA 00h00m19s\n",
      "batch:  ['jjjjjjjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjj']\n",
      "per_prediction_loss:  tensor([129.8615], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 3.6570e-15,  3.7294e-15,  3.6228e-15,  ..., -5.9104e-16,\n",
      "         -5.0111e-16, -6.6228e-16],\n",
      "        [-3.6638e-15, -3.7363e-15, -3.6295e-15,  ...,  5.9214e-16,\n",
      "          5.0204e-16,  6.6352e-16],\n",
      "        [-3.6656e-15, -3.7382e-15, -3.6313e-15,  ...,  5.9243e-16,\n",
      "          5.0228e-16,  6.6384e-16],\n",
      "        ...,\n",
      "        [-3.6662e-15, -3.7388e-15, -3.6319e-15,  ...,  5.9252e-16,\n",
      "          5.0236e-16,  6.6395e-16],\n",
      "        [-3.6679e-15, -3.7405e-15, -3.6336e-15,  ...,  5.9280e-16,\n",
      "          5.0260e-16,  6.6426e-16],\n",
      "        [ 3.6652e-15,  3.7378e-15,  3.6309e-15,  ..., -5.9236e-16,\n",
      "         -5.0223e-16, -6.6377e-16]])\n",
      "batch:  ['ppppp']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['pp']\n",
      "per_prediction_loss:  tensor([72.7153], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-7.7551e-38, -8.9330e-38, -7.8045e-38,  ...,  3.0077e-38,\n",
      "          2.5471e-38,  3.2701e-38],\n",
      "        [ 7.7700e-38,  8.9503e-38,  7.8195e-38,  ..., -3.0135e-38,\n",
      "         -2.5520e-38, -3.2764e-38],\n",
      "        [ 7.7734e-38,  8.9541e-38,  7.8229e-38,  ..., -3.0148e-38,\n",
      "         -2.5531e-38, -3.2778e-38],\n",
      "        ...,\n",
      "        [ 7.7750e-38,  8.9560e-38,  7.8245e-38,  ..., -3.0154e-38,\n",
      "         -2.5536e-38, -3.2785e-38],\n",
      "        [ 7.7791e-38,  8.9607e-38,  7.8286e-38,  ..., -3.0170e-38,\n",
      "         -2.5549e-38, -3.2802e-38],\n",
      "        [-7.7726e-38, -8.9532e-38, -7.8221e-38,  ...,  3.0145e-38,\n",
      "          2.5528e-38,  3.2775e-38]])\n",
      "Epoch 0 iteration 26: loss = -725.153, tp = 29.04 lines/s, ETA 00h00m19s\n",
      "batch:  ['aaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaa']\n",
      "per_prediction_loss:  tensor([65.9967], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-9.8904e-27, -1.0392e-26, -9.7671e-27,  ...,  2.0237e-27,\n",
      "          1.7164e-27,  2.2184e-27],\n",
      "        [ 9.9034e-27,  1.0405e-26,  9.7799e-27,  ..., -2.0263e-27,\n",
      "         -1.7187e-27, -2.2213e-27],\n",
      "        [ 9.9065e-27,  1.0409e-26,  9.7830e-27,  ..., -2.0270e-27,\n",
      "         -1.7192e-27, -2.2220e-27],\n",
      "        ...,\n",
      "        [ 9.9076e-27,  1.0410e-26,  9.7841e-27,  ..., -2.0272e-27,\n",
      "         -1.7194e-27, -2.2223e-27],\n",
      "        [ 9.9116e-27,  1.0414e-26,  9.7881e-27,  ..., -2.0280e-27,\n",
      "         -1.7201e-27, -2.2232e-27],\n",
      "        [-9.9059e-27, -1.0408e-26, -9.7824e-27,  ...,  2.0268e-27,\n",
      "          1.7191e-27,  2.2219e-27]])\n",
      "batch:  ['xxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['x']\n",
      "per_prediction_loss:  tensor([77.5147], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-1.9304e-34, -2.3435e-34, -2.0272e-34,  ...,  1.7612e-34,\n",
      "          1.5173e-34,  1.9328e-34],\n",
      "        [ 1.9328e-34,  2.3465e-34,  2.0298e-34,  ..., -1.7635e-34,\n",
      "         -1.5192e-34, -1.9352e-34],\n",
      "        [ 1.9336e-34,  2.3474e-34,  2.0306e-34,  ..., -1.7642e-34,\n",
      "         -1.5198e-34, -1.9360e-34],\n",
      "        ...,\n",
      "        [ 1.9338e-34,  2.3477e-34,  2.0308e-34,  ..., -1.7643e-34,\n",
      "         -1.5200e-34, -1.9362e-34],\n",
      "        [ 1.9346e-34,  2.3486e-34,  2.0316e-34,  ..., -1.7651e-34,\n",
      "         -1.5206e-34, -1.9370e-34],\n",
      "        [-1.9334e-34, -2.3472e-34, -2.0304e-34,  ...,  1.7640e-34,\n",
      "          1.5197e-34,  1.9358e-34]])\n",
      "Epoch 0 iteration 28: loss = -771.147, tp = 29.35 lines/s, ETA 00h00m19s\n",
      "batch:  ['lllll']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['lll']\n",
      "per_prediction_loss:  tensor([43.1498], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-9.6393e-28, -1.0049e-27, -9.4907e-28,  ...,  1.9141e-28,\n",
      "          1.6406e-28,  2.1893e-28],\n",
      "        [ 9.6499e-28,  1.0060e-27,  9.5011e-28,  ..., -1.9162e-28,\n",
      "         -1.6424e-28, -2.1917e-28],\n",
      "        [ 9.6524e-28,  1.0063e-27,  9.5035e-28,  ..., -1.9167e-28,\n",
      "         -1.6428e-28, -2.1922e-28],\n",
      "        ...,\n",
      "        [ 9.6539e-28,  1.0064e-27,  9.5050e-28,  ..., -1.9170e-28,\n",
      "         -1.6431e-28, -2.1926e-28],\n",
      "        [ 9.6566e-28,  1.0067e-27,  9.5077e-28,  ..., -1.9176e-28,\n",
      "         -1.6436e-28, -2.1932e-28],\n",
      "        [-9.6521e-28, -1.0062e-27, -9.5033e-28,  ...,  1.9167e-28,\n",
      "          1.6428e-28,  2.1922e-28]])\n",
      "batch:  ['ppppp']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ppp']\n",
      "per_prediction_loss:  tensor([89.6652], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-7.1434e-27, -7.3592e-27, -7.0303e-27,  ...,  1.4976e-27,\n",
      "          1.2804e-27,  1.6268e-27],\n",
      "        [ 7.1495e-27,  7.3655e-27,  7.0363e-27,  ..., -1.4988e-27,\n",
      "         -1.2815e-27, -1.6281e-27],\n",
      "        [ 7.1510e-27,  7.3671e-27,  7.0377e-27,  ..., -1.4992e-27,\n",
      "         -1.2818e-27, -1.6285e-27],\n",
      "        ...,\n",
      "        [ 7.1514e-27,  7.3675e-27,  7.0381e-27,  ..., -1.4992e-27,\n",
      "         -1.2819e-27, -1.6286e-27],\n",
      "        [ 7.1532e-27,  7.3693e-27,  7.0399e-27,  ..., -1.4996e-27,\n",
      "         -1.2822e-27, -1.6290e-27],\n",
      "        [-7.1505e-27, -7.3666e-27, -7.0373e-27,  ...,  1.4991e-27,\n",
      "          1.2817e-27,  1.6284e-27]])\n",
      "Epoch 0 iteration 30: loss = -896.652, tp = 29.52 lines/s, ETA 00h00m19s\n",
      "batch:  ['ffffffffff']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ffffff']\n",
      "per_prediction_loss:  tensor([162.5321], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 3.3249e-13,  3.3282e-13,  3.3166e-13,  ..., -5.9525e-14,\n",
      "         -5.2327e-14, -6.5787e-14],\n",
      "        [-3.3282e-13, -3.3315e-13, -3.3200e-13,  ...,  5.9586e-14,\n",
      "          5.2380e-14,  6.5854e-14],\n",
      "        [-3.3287e-13, -3.3320e-13, -3.3204e-13,  ...,  5.9594e-14,\n",
      "          5.2388e-14,  6.5864e-14],\n",
      "        ...,\n",
      "        [-3.3290e-13, -3.3323e-13, -3.3208e-13,  ...,  5.9600e-14,\n",
      "          5.2393e-14,  6.5870e-14],\n",
      "        [-3.3300e-13, -3.3333e-13, -3.3217e-13,  ...,  5.9617e-14,\n",
      "          5.2408e-14,  6.5888e-14],\n",
      "        [ 3.3285e-13,  3.3318e-13,  3.3202e-13,  ..., -5.9590e-14,\n",
      "         -5.2384e-14, -6.5859e-14]])\n",
      "batch:  ['ccccc']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['cc']\n",
      "per_prediction_loss:  tensor([43.2375], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 0.0000e+00, -1.4013e-45,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  1.4013e-45,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  1.4013e-45,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  1.4013e-45,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  1.4013e-45,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -1.4013e-45,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]])\n",
      "Epoch 0 iteration 32: loss = -430.375, tp = 29.42 lines/s, ETA 00h00m19s\n",
      "batch:  ['eeeeeeeeee']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eeeee']\n",
      "per_prediction_loss:  tensor([127.6362], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 3.9161e-14,  3.9201e-14,  3.9057e-14,  ..., -7.0721e-15,\n",
      "         -6.2574e-15, -7.7921e-15],\n",
      "        [-3.9191e-14, -3.9231e-14, -3.9087e-14,  ...,  7.0775e-15,\n",
      "          6.2623e-15,  7.7981e-15],\n",
      "        [-3.9195e-14, -3.9236e-14, -3.9092e-14,  ...,  7.0783e-15,\n",
      "          6.2630e-15,  7.7990e-15],\n",
      "        ...,\n",
      "        [-3.9198e-14, -3.9238e-14, -3.9094e-14,  ...,  7.0787e-15,\n",
      "          6.2633e-15,  7.7995e-15],\n",
      "        [-3.9206e-14, -3.9246e-14, -3.9102e-14,  ...,  7.0802e-15,\n",
      "          6.2647e-15,  7.8011e-15],\n",
      "        [ 3.9194e-14,  3.9234e-14,  3.9090e-14,  ..., -7.0780e-15,\n",
      "         -6.2627e-15, -7.7987e-15]])\n",
      "batch:  ['yyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yyy']\n",
      "per_prediction_loss:  tensor([45.4580], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-1.8496e-27, -1.8926e-27, -1.8222e-27,  ...,  3.8273e-28,\n",
      "          3.2442e-28,  4.1877e-28],\n",
      "        [ 1.8509e-27,  1.8940e-27,  1.8234e-27,  ..., -3.8300e-28,\n",
      "         -3.2465e-28, -4.1906e-28],\n",
      "        [ 1.8510e-27,  1.8942e-27,  1.8236e-27,  ..., -3.8304e-28,\n",
      "         -3.2468e-28, -4.1911e-28],\n",
      "        ...,\n",
      "        [ 1.8512e-27,  1.8943e-27,  1.8238e-27,  ..., -3.8307e-28,\n",
      "         -3.2471e-28, -4.1914e-28],\n",
      "        [ 1.8515e-27,  1.8946e-27,  1.8241e-27,  ..., -3.8314e-28,\n",
      "         -3.2477e-28, -4.1921e-28],\n",
      "        [-1.8510e-27, -1.8941e-27, -1.8236e-27,  ...,  3.8303e-28,\n",
      "          3.2467e-28,  4.1909e-28]])\n",
      "Epoch 0 iteration 34: loss = -452.580, tp = 29.34 lines/s, ETA 00h00m19s\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mm']\n",
      "per_prediction_loss:  tensor([92.6748], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-5.6052e-45, -5.6052e-45, -5.6052e-45,  ...,  1.4013e-45,\n",
      "          1.4013e-45,  1.4013e-45],\n",
      "        [ 5.6052e-45,  5.6052e-45,  5.6052e-45,  ..., -1.4013e-45,\n",
      "         -1.4013e-45, -1.4013e-45],\n",
      "        [ 5.6052e-45,  5.6052e-45,  5.6052e-45,  ..., -1.4013e-45,\n",
      "         -1.4013e-45, -1.4013e-45],\n",
      "        ...,\n",
      "        [ 5.6052e-45,  5.6052e-45,  5.6052e-45,  ..., -1.4013e-45,\n",
      "         -1.4013e-45, -1.4013e-45],\n",
      "        [ 5.6052e-45,  5.6052e-45,  5.6052e-45,  ..., -1.4013e-45,\n",
      "         -1.4013e-45, -1.4013e-45],\n",
      "        [-5.6052e-45, -5.6052e-45, -5.6052e-45,  ...,  1.4013e-45,\n",
      "          1.4013e-45,  1.4013e-45]])\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjj']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_prediction_loss:  tensor([107.9075], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 6.2512e-17,  6.2670e-17,  6.2283e-17,  ..., -1.2100e-17,\n",
      "         -1.0482e-17, -1.3452e-17],\n",
      "        [-6.2551e-17, -6.2709e-17, -6.2322e-17,  ...,  1.2108e-17,\n",
      "          1.0488e-17,  1.3460e-17],\n",
      "        [-6.2555e-17, -6.2714e-17, -6.2327e-17,  ...,  1.2109e-17,\n",
      "          1.0489e-17,  1.3461e-17],\n",
      "        ...,\n",
      "        [-6.2559e-17, -6.2717e-17, -6.2330e-17,  ...,  1.2110e-17,\n",
      "          1.0490e-17,  1.3462e-17],\n",
      "        [-6.2569e-17, -6.2727e-17, -6.2340e-17,  ...,  1.2112e-17,\n",
      "          1.0491e-17,  1.3464e-17],\n",
      "        [ 6.2553e-17,  6.2711e-17,  6.2325e-17,  ..., -1.2109e-17,\n",
      "         -1.0489e-17, -1.3461e-17]])\n",
      "Epoch 0 iteration 36: loss = -1080.075, tp = 29.57 lines/s, ETA 00h00m19s\n",
      "batch:  ['yyyyyyyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yyyy']\n",
      "per_prediction_loss:  tensor([85.1412], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 6.7290e-17,  6.7466e-17,  6.7038e-17,  ..., -1.2735e-17,\n",
      "         -1.0839e-17, -1.3928e-17],\n",
      "        [-6.7326e-17, -6.7503e-17, -6.7075e-17,  ...,  1.2742e-17,\n",
      "          1.0845e-17,  1.3935e-17],\n",
      "        [-6.7332e-17, -6.7509e-17, -6.7081e-17,  ...,  1.2743e-17,\n",
      "          1.0846e-17,  1.3936e-17],\n",
      "        ...,\n",
      "        [-6.7336e-17, -6.7513e-17, -6.7085e-17,  ...,  1.2744e-17,\n",
      "          1.0847e-17,  1.3937e-17],\n",
      "        [-6.7346e-17, -6.7523e-17, -6.7095e-17,  ...,  1.2745e-17,\n",
      "          1.0848e-17,  1.3939e-17],\n",
      "        [ 6.7330e-17,  6.7507e-17,  6.7079e-17,  ..., -1.2742e-17,\n",
      "         -1.0846e-17, -1.3936e-17]])\n",
      "batch:  ['sssss']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['sss']\n",
      "per_prediction_loss:  tensor([51.0771], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-2.8355e-27, -2.8792e-27, -2.7963e-27,  ...,  5.9874e-28,\n",
      "          5.1435e-28,  6.4536e-28],\n",
      "        [ 2.8370e-27,  2.8807e-27,  2.7978e-27,  ..., -5.9906e-28,\n",
      "         -5.1463e-28, -6.4571e-28],\n",
      "        [ 2.8372e-27,  2.8809e-27,  2.7980e-27,  ..., -5.9911e-28,\n",
      "         -5.1467e-28, -6.4576e-28],\n",
      "        ...,\n",
      "        [ 2.8374e-27,  2.8811e-27,  2.7982e-27,  ..., -5.9915e-28,\n",
      "         -5.1470e-28, -6.4580e-28],\n",
      "        [ 2.8378e-27,  2.8816e-27,  2.7986e-27,  ..., -5.9924e-28,\n",
      "         -5.1478e-28, -6.4590e-28],\n",
      "        [-2.8371e-27, -2.8809e-27, -2.7979e-27,  ...,  5.9909e-28,\n",
      "          5.1466e-28,  6.4574e-28]])\n",
      "Epoch 0 iteration 38: loss = -509.771, tp = 29.52 lines/s, ETA 00h00m19s\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mm']\n",
      "per_prediction_loss:  tensor([105.1567], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "batch:  ['zzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['z']\n",
      "per_prediction_loss:  tensor([97.0523], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 0 iteration 40: loss = -968.523, tp = 29.84 lines/s, ETA 00h00m18s\n",
      "batch:  ['nnnnnnnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['n']\n",
      "per_prediction_loss:  tensor([186.9943], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "batch:  ['ccccc']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['c']\n",
      "per_prediction_loss:  tensor([75.4315], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 0 iteration 42: loss = -751.315, tp = 29.93 lines/s, ETA 00h00m18s\n",
      "batch:  ['qqqqqqqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['qqqqq']\n",
      "per_prediction_loss:  tensor([233.5948], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 2.2336e-17,  2.2339e-17,  2.2291e-17,  ..., -4.6959e-18,\n",
      "         -4.1535e-18, -5.2566e-18],\n",
      "        [-2.2344e-17, -2.2348e-17, -2.2300e-17,  ...,  4.6978e-18,\n",
      "          4.1551e-18,  5.2587e-18],\n",
      "        [-2.2346e-17, -2.2349e-17, -2.2301e-17,  ...,  4.6980e-18,\n",
      "          4.1554e-18,  5.2590e-18],\n",
      "        ...,\n",
      "        [-2.2346e-17, -2.2350e-17, -2.2302e-17,  ...,  4.6982e-18,\n",
      "          4.1555e-18,  5.2592e-18],\n",
      "        [-2.2349e-17, -2.2353e-17, -2.2304e-17,  ...,  4.6987e-18,\n",
      "          4.1560e-18,  5.2598e-18],\n",
      "        [ 2.2345e-17,  2.2349e-17,  2.2300e-17,  ..., -4.6979e-18,\n",
      "         -4.1552e-18, -5.2588e-18]])\n",
      "batch:  ['aaaaaaaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaa']\n",
      "per_prediction_loss:  tensor([192.0090], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 5.3274e-25,  5.3784e-25,  5.2814e-25,  ..., -1.1849e-25,\n",
      "         -1.0310e-25, -1.2941e-25],\n",
      "        [-5.3302e-25, -5.3812e-25, -5.2842e-25,  ...,  1.1855e-25,\n",
      "          1.0315e-25,  1.2948e-25],\n",
      "        [-5.3303e-25, -5.3814e-25, -5.2843e-25,  ...,  1.1856e-25,\n",
      "          1.0316e-25,  1.2949e-25],\n",
      "        ...,\n",
      "        [-5.3306e-25, -5.3817e-25, -5.2846e-25,  ...,  1.1856e-25,\n",
      "          1.0316e-25,  1.2949e-25],\n",
      "        [-5.3315e-25, -5.3826e-25, -5.2855e-25,  ...,  1.1858e-25,\n",
      "          1.0318e-25,  1.2951e-25],\n",
      "        [ 5.3303e-25,  5.3814e-25,  5.2843e-25,  ..., -1.1856e-25,\n",
      "         -1.0316e-25, -1.2949e-25]])\n",
      "Epoch 0 iteration 44: loss = -1919.090, tp = 29.70 lines/s, ETA 00h00m18s\n",
      "batch:  ['aaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaa']\n",
      "per_prediction_loss:  tensor([119.2525], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 2.8515e-25,  2.8778e-25,  2.8275e-25,  ..., -6.3645e-26,\n",
      "         -5.5407e-26, -6.9505e-26],\n",
      "        [-2.8526e-25, -2.8789e-25, -2.8286e-25,  ...,  6.3669e-26,\n",
      "          5.5428e-26,  6.9531e-26],\n",
      "        [-2.8527e-25, -2.8789e-25, -2.8286e-25,  ...,  6.3671e-26,\n",
      "          5.5429e-26,  6.9533e-26],\n",
      "        ...,\n",
      "        [-2.8528e-25, -2.8791e-25, -2.8288e-25,  ...,  6.3673e-26,\n",
      "          5.5431e-26,  6.9536e-26],\n",
      "        [-2.8531e-25, -2.8794e-25, -2.8291e-25,  ...,  6.3680e-26,\n",
      "          5.5438e-26,  6.9544e-26],\n",
      "        [ 2.8527e-25,  2.8789e-25,  2.8286e-25,  ..., -6.3670e-26,\n",
      "         -5.5428e-26, -6.9532e-26]])\n",
      "batch:  ['lllll']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ll']\n",
      "per_prediction_loss:  tensor([93.4588], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 0 iteration 46: loss = -932.588, tp = 29.91 lines/s, ETA 00h00m18s\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dddd']\n",
      "per_prediction_loss:  tensor([126.9677], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 4.1919e-19,  4.2001e-19,  4.1849e-19,  ..., -8.4111e-20,\n",
      "         -7.2682e-20, -1.0131e-19],\n",
      "        [-4.1935e-19, -4.2017e-19, -4.1865e-19,  ...,  8.4143e-20,\n",
      "          7.2709e-20,  1.0135e-19],\n",
      "        [-4.1936e-19, -4.2018e-19, -4.1866e-19,  ...,  8.4146e-20,\n",
      "          7.2712e-20,  1.0135e-19],\n",
      "        ...,\n",
      "        [-4.1938e-19, -4.2020e-19, -4.1868e-19,  ...,  8.4150e-20,\n",
      "          7.2715e-20,  1.0136e-19],\n",
      "        [-4.1942e-19, -4.2024e-19, -4.1872e-19,  ...,  8.4158e-20,\n",
      "          7.2722e-20,  1.0137e-19],\n",
      "        [ 4.1935e-19,  4.2017e-19,  4.1865e-19,  ..., -8.4143e-20,\n",
      "         -7.2709e-20, -1.0135e-19]])\n",
      "batch:  ['kkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kk']\n",
      "per_prediction_loss:  tensor([123.8465], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 0 iteration 48: loss = -1236.465, tp = 30.11 lines/s, ETA 00h00m18s\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddd']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_prediction_loss:  tensor([232.7107], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 1.9296e-25,  1.9556e-25,  1.9212e-25,  ..., -4.1440e-26,\n",
      "         -3.5835e-26, -4.9870e-26],\n",
      "        [-1.9304e-25, -1.9564e-25, -1.9220e-25,  ...,  4.1457e-26,\n",
      "          3.5850e-26,  4.9891e-26],\n",
      "        [-1.9305e-25, -1.9565e-25, -1.9221e-25,  ...,  4.1459e-26,\n",
      "          3.5851e-26,  4.9893e-26],\n",
      "        ...,\n",
      "        [-1.9306e-25, -1.9566e-25, -1.9222e-25,  ...,  4.1461e-26,\n",
      "          3.5853e-26,  4.9895e-26],\n",
      "        [-1.9308e-25, -1.9568e-25, -1.9224e-25,  ...,  4.1465e-26,\n",
      "          3.5856e-26,  4.9900e-26],\n",
      "        [ 1.9304e-25,  1.9564e-25,  1.9220e-25,  ..., -4.1457e-26,\n",
      "         -3.5850e-26, -4.9891e-26]])\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['tttttt']\n",
      "per_prediction_loss:  tensor([222.9571], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 4.1417e-17,  4.1409e-17,  4.1353e-17,  ..., -8.3901e-18,\n",
      "         -7.3820e-18, -8.6557e-18],\n",
      "        [-4.1435e-17, -4.1427e-17, -4.1371e-17,  ...,  8.3936e-18,\n",
      "          7.3851e-18,  8.6593e-18],\n",
      "        [-4.1436e-17, -4.1429e-17, -4.1372e-17,  ...,  8.3940e-18,\n",
      "          7.3854e-18,  8.6597e-18],\n",
      "        ...,\n",
      "        [-4.1438e-17, -4.1430e-17, -4.1374e-17,  ...,  8.3943e-18,\n",
      "          7.3857e-18,  8.6601e-18],\n",
      "        [-4.1443e-17, -4.1435e-17, -4.1379e-17,  ...,  8.3953e-18,\n",
      "          7.3865e-18,  8.6611e-18],\n",
      "        [ 4.1436e-17,  4.1428e-17,  4.1372e-17,  ..., -8.3939e-18,\n",
      "         -7.3853e-18, -8.6596e-18]])\n",
      "Epoch 0 iteration 50: loss = -2231.571, tp = 29.95 lines/s, ETA 00h00m18s\n",
      "batch:  ['zzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zzz']\n",
      "per_prediction_loss:  tensor([132.0203], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 3.5244e-25,  3.5539e-25,  3.4913e-25,  ..., -7.8378e-26,\n",
      "         -6.8280e-26, -8.6074e-26],\n",
      "        [-3.5256e-25, -3.5551e-25, -3.4925e-25,  ...,  7.8405e-26,\n",
      "          6.8303e-26,  8.6103e-26],\n",
      "        [-3.5257e-25, -3.5552e-25, -3.4926e-25,  ...,  7.8407e-26,\n",
      "          6.8305e-26,  8.6105e-26],\n",
      "        ...,\n",
      "        [-3.5258e-25, -3.5553e-25, -3.4927e-25,  ...,  7.8409e-26,\n",
      "          6.8306e-26,  8.6108e-26],\n",
      "        [-3.5261e-25, -3.5557e-25, -3.4930e-25,  ...,  7.8417e-26,\n",
      "          6.8313e-26,  8.6116e-26],\n",
      "        [ 3.5256e-25,  3.5551e-25,  3.4925e-25,  ..., -7.8405e-26,\n",
      "         -6.8303e-26, -8.6104e-26]])\n",
      "batch:  ['ooooo']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ooo']\n",
      "per_prediction_loss:  tensor([141.3396], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 4.3963e-25,  4.4290e-25,  4.3575e-25,  ..., -9.8158e-26,\n",
      "         -8.7744e-26, -1.0747e-25],\n",
      "        [-4.3982e-25, -4.4308e-25, -4.3593e-25,  ...,  9.8200e-26,\n",
      "          8.7782e-26,  1.0751e-25],\n",
      "        [-4.3983e-25, -4.4310e-25, -4.3594e-25,  ...,  9.8203e-26,\n",
      "          8.7784e-26,  1.0752e-25],\n",
      "        ...,\n",
      "        [-4.3985e-25, -4.4312e-25, -4.3597e-25,  ...,  9.8208e-26,\n",
      "          8.7788e-26,  1.0752e-25],\n",
      "        [-4.3990e-25, -4.4317e-25, -4.3601e-25,  ...,  9.8218e-26,\n",
      "          8.7798e-26,  1.0753e-25],\n",
      "        [ 4.3982e-25,  4.4309e-25,  4.3594e-25,  ..., -9.8202e-26,\n",
      "         -8.7783e-26, -1.0751e-25]])\n",
      "Epoch 0 iteration 52: loss = -1412.396, tp = 30.06 lines/s, ETA 00h00m18s\n",
      "batch:  ['nnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nn']\n",
      "per_prediction_loss:  tensor([148.1792], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "batch:  ['ppppp']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['p']\n",
      "per_prediction_loss:  tensor([159.7619], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 0 iteration 54: loss = -1594.619, tp = 30.11 lines/s, ETA 00h00m18s\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mm']\n",
      "per_prediction_loss:  tensor([155.2542], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddd']\n",
      "per_prediction_loss:  tensor([152.3237], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 1.1287e-25,  1.1423e-25,  1.1242e-25,  ..., -2.4543e-26,\n",
      "         -2.1258e-26, -2.9477e-26],\n",
      "        [-1.1292e-25, -1.1427e-25, -1.1246e-25,  ...,  2.4552e-26,\n",
      "          2.1267e-26,  2.9488e-26],\n",
      "        [-1.1292e-25, -1.1428e-25, -1.1246e-25,  ...,  2.4553e-26,\n",
      "          2.1268e-26,  2.9489e-26],\n",
      "        ...,\n",
      "        [-1.1293e-25, -1.1428e-25, -1.1247e-25,  ...,  2.4554e-26,\n",
      "          2.1269e-26,  2.9491e-26],\n",
      "        [-1.1294e-25, -1.1429e-25, -1.1248e-25,  ...,  2.4557e-26,\n",
      "          2.1271e-26,  2.9494e-26],\n",
      "        [ 1.1292e-25,  1.1427e-25,  1.1246e-25,  ..., -2.4552e-26,\n",
      "         -2.1267e-26, -2.9488e-26]])\n",
      "Epoch 0 iteration 56: loss = -1523.237, tp = 30.19 lines/s, ETA 00h00m17s\n",
      "batch:  ['pppppppppp']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ppppppp']\n",
      "per_prediction_loss:  tensor([286.1185], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 4.2985e-18,  4.2973e-18,  4.2923e-18,  ..., -9.2545e-19,\n",
      "         -8.0528e-19, -1.0024e-18],\n",
      "        [-4.3002e-18, -4.2990e-18, -4.2940e-18,  ...,  9.2582e-19,\n",
      "          8.0561e-19,  1.0028e-18],\n",
      "        [-4.3003e-18, -4.2991e-18, -4.2941e-18,  ...,  9.2585e-19,\n",
      "          8.0563e-19,  1.0029e-18],\n",
      "        ...,\n",
      "        [-4.3005e-18, -4.2993e-18, -4.2943e-18,  ...,  9.2589e-19,\n",
      "          8.0567e-19,  1.0029e-18],\n",
      "        [-4.3010e-18, -4.2998e-18, -4.2948e-18,  ...,  9.2600e-19,\n",
      "          8.0576e-19,  1.0030e-18],\n",
      "        [ 4.3002e-18,  4.2990e-18,  4.2940e-18,  ..., -9.2583e-19,\n",
      "         -8.0561e-19, -1.0029e-18]])\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmmm']\n",
      "per_prediction_loss:  tensor([277.1888], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 1.9405e-16,  1.9403e-16,  1.9377e-16,  ..., -3.7921e-17,\n",
      "         -3.2439e-17, -4.1845e-17],\n",
      "        [-1.9413e-16, -1.9411e-16, -1.9385e-16,  ...,  3.7937e-17,\n",
      "          3.2453e-17,  4.1863e-17],\n",
      "        [-1.9413e-16, -1.9411e-16, -1.9385e-16,  ...,  3.7938e-17,\n",
      "          3.2453e-17,  4.1864e-17],\n",
      "        ...,\n",
      "        [-1.9414e-16, -1.9412e-16, -1.9386e-16,  ...,  3.7940e-17,\n",
      "          3.2455e-17,  4.1865e-17],\n",
      "        [-1.9416e-16, -1.9414e-16, -1.9388e-16,  ...,  3.7944e-17,\n",
      "          3.2458e-17,  4.1870e-17],\n",
      "        [ 1.9413e-16,  1.9411e-16,  1.9385e-16,  ..., -3.7937e-17,\n",
      "         -3.2453e-17, -4.1863e-17]])\n",
      "Epoch 0 iteration 58: loss = -2773.888, tp = 29.86 lines/s, ETA 00h00m18s\n",
      "batch:  ['ssssssssss']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ssss']\n",
      "per_prediction_loss:  tensor([174.4353], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 1.9869e-18,  1.9884e-18,  1.9825e-18,  ..., -4.1289e-19,\n",
      "         -3.5746e-19, -4.4495e-19],\n",
      "        [-1.9876e-18, -1.9891e-18, -1.9832e-18,  ...,  4.1304e-19,\n",
      "          3.5759e-19,  4.4511e-19],\n",
      "        [-1.9876e-18, -1.9891e-18, -1.9833e-18,  ...,  4.1305e-19,\n",
      "          3.5760e-19,  4.4512e-19],\n",
      "        ...,\n",
      "        [-1.9877e-18, -1.9892e-18, -1.9833e-18,  ...,  4.1306e-19,\n",
      "          3.5761e-19,  4.4514e-19],\n",
      "        [-1.9879e-18, -1.9894e-18, -1.9835e-18,  ...,  4.1311e-19,\n",
      "          3.5765e-19,  4.4519e-19],\n",
      "        [ 1.9876e-18,  1.9891e-18,  1.9832e-18,  ..., -4.1304e-19,\n",
      "         -3.5759e-19, -4.4511e-19]])\n",
      "batch:  ['nnnnnnnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnnnnn']\n",
      "per_prediction_loss:  tensor([293.2077], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[ 2.3144e-17,  2.3134e-17,  2.3106e-17,  ..., -4.9427e-18,\n",
      "         -4.0152e-18, -5.0307e-18],\n",
      "        [-2.3153e-17, -2.3143e-17, -2.3115e-17,  ...,  4.9447e-18,\n",
      "          4.0168e-18,  5.0327e-18],\n",
      "        [-2.3153e-17, -2.3143e-17, -2.3115e-17,  ...,  4.9447e-18,\n",
      "          4.0168e-18,  5.0327e-18],\n",
      "        ...,\n",
      "        [-2.3154e-17, -2.3144e-17, -2.3116e-17,  ...,  4.9449e-18,\n",
      "          4.0169e-18,  5.0329e-18],\n",
      "        [-2.3156e-17, -2.3146e-17, -2.3118e-17,  ...,  4.9454e-18,\n",
      "          4.0173e-18,  5.0334e-18],\n",
      "        [ 2.3153e-17,  2.3143e-17,  2.3114e-17,  ..., -4.9446e-18,\n",
      "         -4.0167e-18, -5.0326e-18]])\n",
      "Epoch 0 iteration 60: loss = -2933.077, tp = 29.59 lines/s, ETA 00h00m18s\n",
      "batch:  ['jjjjjjjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjjj']\n",
      "per_prediction_loss:  tensor([306.5880], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 2.3704e-18,  2.3700e-18,  2.3669e-18,  ..., -5.0579e-19,\n",
      "         -4.4231e-19, -5.5992e-19],\n",
      "        [-2.3713e-18, -2.3710e-18, -2.3678e-18,  ...,  5.0599e-19,\n",
      "          4.4248e-19,  5.6014e-19],\n",
      "        [-2.3713e-18, -2.3710e-18, -2.3678e-18,  ...,  5.0600e-19,\n",
      "          4.4249e-19,  5.6015e-19],\n",
      "        ...,\n",
      "        [-2.3714e-18, -2.3711e-18, -2.3679e-18,  ...,  5.0602e-19,\n",
      "          4.4251e-19,  5.6017e-19],\n",
      "        [-2.3717e-18, -2.3714e-18, -2.3682e-18,  ...,  5.0607e-19,\n",
      "          4.4256e-19,  5.6023e-19],\n",
      "        [ 2.3713e-18,  2.3710e-18,  2.3678e-18,  ..., -5.0599e-19,\n",
      "         -4.4249e-19, -5.6014e-19]])\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['tttttt']\n",
      "per_prediction_loss:  tensor([293.4347], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 1.3628e-17,  1.3624e-17,  1.3608e-17,  ..., -2.8190e-18,\n",
      "         -2.4842e-18, -2.9109e-18],\n",
      "        [-1.3632e-17, -1.3629e-17, -1.3613e-17,  ...,  2.8200e-18,\n",
      "          2.4851e-18,  2.9119e-18],\n",
      "        [-1.3633e-17, -1.3630e-17, -1.3614e-17,  ...,  2.8201e-18,\n",
      "          2.4852e-18,  2.9121e-18],\n",
      "        ...,\n",
      "        [-1.3634e-17, -1.3630e-17, -1.3614e-17,  ...,  2.8202e-18,\n",
      "          2.4853e-18,  2.9122e-18],\n",
      "        [-1.3635e-17, -1.3631e-17, -1.3616e-17,  ...,  2.8205e-18,\n",
      "          2.4855e-18,  2.9124e-18],\n",
      "        [ 1.3633e-17,  1.3629e-17,  1.3614e-17,  ..., -2.8201e-18,\n",
      "         -2.4852e-18, -2.9120e-18]])\n",
      "Epoch 0 iteration 62: loss = -2936.347, tp = 29.47 lines/s, ETA 00h00m18s\n",
      "batch:  ['nnnnnnnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nn']\n",
      "per_prediction_loss:  tensor([312.7761], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "batch:  ['jjjjjjjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjjj']\n",
      "per_prediction_loss:  tensor([330.1329], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 2.1397e-18,  2.1394e-18,  2.1366e-18,  ..., -4.5780e-19,\n",
      "         -4.0044e-19, -5.0674e-19],\n",
      "        [-2.1405e-18, -2.1402e-18, -2.1373e-18,  ...,  4.5797e-19,\n",
      "          4.0058e-19,  5.0692e-19],\n",
      "        [-2.1405e-18, -2.1402e-18, -2.1374e-18,  ...,  4.5798e-19,\n",
      "          4.0059e-19,  5.0694e-19],\n",
      "        ...,\n",
      "        [-2.1406e-18, -2.1403e-18, -2.1374e-18,  ...,  4.5799e-19,\n",
      "          4.0060e-19,  5.0695e-19],\n",
      "        [-2.1408e-18, -2.1405e-18, -2.1376e-18,  ...,  4.5803e-19,\n",
      "          4.0064e-19,  5.0700e-19],\n",
      "        [ 2.1405e-18,  2.1402e-18,  2.1373e-18,  ..., -4.5797e-19,\n",
      "         -4.0058e-19, -5.0692e-19]])\n",
      "Epoch 0 iteration 64: loss = -3302.329, tp = 29.47 lines/s, ETA 00h00m18s\n",
      "batch:  ['zzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zzz']\n",
      "per_prediction_loss:  tensor([173.9051], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 5.1660e-25,  5.2009e-25,  5.1240e-25,  ..., -1.1743e-25,\n",
      "         -1.0261e-25, -1.2879e-25],\n",
      "        [-5.1676e-25, -5.2025e-25, -5.1256e-25,  ...,  1.1747e-25,\n",
      "          1.0264e-25,  1.2883e-25],\n",
      "        [-5.1677e-25, -5.2026e-25, -5.1257e-25,  ...,  1.1747e-25,\n",
      "          1.0265e-25,  1.2883e-25],\n",
      "        ...,\n",
      "        [-5.1679e-25, -5.2027e-25, -5.1258e-25,  ...,  1.1747e-25,\n",
      "          1.0265e-25,  1.2884e-25],\n",
      "        [-5.1683e-25, -5.2032e-25, -5.1263e-25,  ...,  1.1748e-25,\n",
      "          1.0266e-25,  1.2885e-25],\n",
      "        [ 5.1676e-25,  5.2025e-25,  5.1256e-25,  ..., -1.1747e-25,\n",
      "         -1.0264e-25, -1.2883e-25]])\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuu']\n",
      "per_prediction_loss:  tensor([90.0437], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 3.0106e-19,  3.0135e-19,  3.0041e-19,  ..., -6.2388e-20,\n",
      "         -5.4694e-20, -6.8081e-20],\n",
      "        [-3.0116e-19, -3.0145e-19, -3.0051e-19,  ...,  6.2408e-20,\n",
      "          5.4711e-20,  6.8103e-20],\n",
      "        [-3.0116e-19, -3.0145e-19, -3.0051e-19,  ...,  6.2409e-20,\n",
      "          5.4712e-20,  6.8104e-20],\n",
      "        ...,\n",
      "        [-3.0117e-19, -3.0146e-19, -3.0052e-19,  ...,  6.2411e-20,\n",
      "          5.4714e-20,  6.8106e-20],\n",
      "        [-3.0120e-19, -3.0148e-19, -3.0054e-19,  ...,  6.2416e-20,\n",
      "          5.4718e-20,  6.8111e-20],\n",
      "        [ 3.0116e-19,  3.0145e-19,  3.0051e-19,  ..., -6.2408e-20,\n",
      "         -5.4711e-20, -6.8103e-20]])\n",
      "Epoch 0 iteration 66: loss = -899.437, tp = 29.54 lines/s, ETA 00h00m18s\n",
      "batch:  ['hhhhhhhhhh']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['hhh']\n",
      "per_prediction_loss:  tensor([90.4857], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 3.3644e-26,  3.3922e-26,  3.3344e-26,  ..., -7.4355e-27,\n",
      "         -6.4834e-27, -8.0636e-27],\n",
      "        [-3.3655e-26, -3.3934e-26, -3.3355e-26,  ...,  7.4381e-27,\n",
      "          6.4856e-27,  8.0663e-27],\n",
      "        [-3.3656e-26, -3.3934e-26, -3.3355e-26,  ...,  7.4381e-27,\n",
      "          6.4856e-27,  8.0664e-27],\n",
      "        ...,\n",
      "        [-3.3657e-26, -3.3936e-26, -3.3356e-26,  ...,  7.4384e-27,\n",
      "          6.4859e-27,  8.0667e-27],\n",
      "        [-3.3660e-26, -3.3939e-26, -3.3360e-26,  ...,  7.4391e-27,\n",
      "          6.4865e-27,  8.0674e-27],\n",
      "        [ 3.3655e-26,  3.3934e-26,  3.3355e-26,  ..., -7.4380e-27,\n",
      "         -6.4855e-27, -8.0662e-27]])\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmmmm']\n",
      "per_prediction_loss:  tensor([342.8089], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 1.8642e-16,  1.8637e-16,  1.8617e-16,  ..., -3.6728e-17,\n",
      "         -3.1446e-17, -4.0518e-17],\n",
      "        [-1.8649e-16, -1.8644e-16, -1.8624e-16,  ...,  3.6742e-17,\n",
      "          3.1458e-17,  4.0532e-17],\n",
      "        [-1.8649e-16, -1.8644e-16, -1.8625e-16,  ...,  3.6743e-17,\n",
      "          3.1458e-17,  4.0533e-17],\n",
      "        ...,\n",
      "        [-1.8650e-16, -1.8645e-16, -1.8625e-16,  ...,  3.6744e-17,\n",
      "          3.1460e-17,  4.0535e-17],\n",
      "        [-1.8652e-16, -1.8647e-16, -1.8627e-16,  ...,  3.6748e-17,\n",
      "          3.1463e-17,  4.0539e-17],\n",
      "        [ 1.8649e-16,  1.8644e-16,  1.8624e-16,  ..., -3.6742e-17,\n",
      "         -3.1458e-17, -4.0533e-17]])\n",
      "Epoch 0 iteration 68: loss = -3430.089, tp = 29.41 lines/s, ETA 00h00m18s\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mm']\n",
      "per_prediction_loss:  tensor([359.9906], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jj']\n",
      "per_prediction_loss:  tensor([206.5961], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 0 iteration 70: loss = -2063.961, tp = 29.52 lines/s, ETA 00h00m17s\n",
      "batch:  ['gggggggggg']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ggg']\n",
      "per_prediction_loss:  tensor([100.7831], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 6.2252e-26,  6.2719e-26,  6.1687e-26,  ..., -1.3635e-26,\n",
      "         -1.2074e-26, -1.5126e-26],\n",
      "        [-6.2290e-26, -6.2756e-26, -6.1724e-26,  ...,  1.3644e-26,\n",
      "          1.2081e-26,  1.5135e-26],\n",
      "        [-6.2288e-26, -6.2755e-26, -6.1722e-26,  ...,  1.3643e-26,\n",
      "          1.2081e-26,  1.5135e-26],\n",
      "        ...,\n",
      "        [-6.2292e-26, -6.2758e-26, -6.1726e-26,  ...,  1.3644e-26,\n",
      "          1.2082e-26,  1.5136e-26],\n",
      "        [-6.2302e-26, -6.2768e-26, -6.1736e-26,  ...,  1.3646e-26,\n",
      "          1.2084e-26,  1.5138e-26],\n",
      "        [ 6.2286e-26,  6.2753e-26,  6.1721e-26,  ..., -1.3643e-26,\n",
      "         -1.2081e-26, -1.5134e-26]])\n",
      "batch:  ['lllll']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['l']\n",
      "per_prediction_loss:  tensor([160.2410], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 0 iteration 72: loss = -1599.410, tp = 29.55 lines/s, ETA 00h00m17s\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmmmm']\n",
      "per_prediction_loss:  tensor([379.3360], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 1.7798e-16,  1.7793e-16,  1.7774e-16,  ..., -3.5140e-17,\n",
      "         -3.0092e-17, -3.8762e-17],\n",
      "        [-1.7805e-16, -1.7800e-16, -1.7781e-16,  ...,  3.5153e-17,\n",
      "          3.0104e-17,  3.8777e-17],\n",
      "        [-1.7805e-16, -1.7800e-16, -1.7782e-16,  ...,  3.5154e-17,\n",
      "          3.0104e-17,  3.8778e-17],\n",
      "        ...,\n",
      "        [-1.7806e-16, -1.7801e-16, -1.7782e-16,  ...,  3.5155e-17,\n",
      "          3.0105e-17,  3.8780e-17],\n",
      "        [-1.7808e-16, -1.7803e-16, -1.7784e-16,  ...,  3.5159e-17,\n",
      "          3.0109e-17,  3.8784e-17],\n",
      "        [ 1.7805e-16,  1.7800e-16,  1.7781e-16,  ..., -3.5153e-17,\n",
      "         -3.0104e-17, -3.8777e-17]])\n",
      "batch:  ['hhhhhhhhhh']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['hhhh']\n",
      "per_prediction_loss:  tensor([132.9205], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 5.5093e-19,  5.5140e-19,  5.4979e-19,  ..., -1.1485e-19,\n",
      "         -1.0017e-19, -1.2454e-19],\n",
      "        [-5.5114e-19, -5.5161e-19, -5.4999e-19,  ...,  1.1489e-19,\n",
      "          1.0020e-19,  1.2459e-19],\n",
      "        [-5.5115e-19, -5.5162e-19, -5.5000e-19,  ...,  1.1490e-19,\n",
      "          1.0021e-19,  1.2459e-19],\n",
      "        ...,\n",
      "        [-5.5117e-19, -5.5164e-19, -5.5002e-19,  ...,  1.1490e-19,\n",
      "          1.0021e-19,  1.2460e-19],\n",
      "        [-5.5123e-19, -5.5170e-19, -5.5009e-19,  ...,  1.1491e-19,\n",
      "          1.0022e-19,  1.2461e-19],\n",
      "        [ 5.5114e-19,  5.5161e-19,  5.4999e-19,  ..., -1.1489e-19,\n",
      "         -1.0020e-19, -1.2459e-19]])\n",
      "Epoch 0 iteration 74: loss = -1328.205, tp = 29.45 lines/s, ETA 00h00m17s\n",
      "batch:  ['nnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnn']\n",
      "per_prediction_loss:  tensor([224.9299], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 6.8107e-25,  6.8306e-25,  6.7152e-25,  ..., -1.5584e-25,\n",
      "         -1.2677e-25, -1.5861e-25],\n",
      "        [-6.8130e-25, -6.8328e-25, -6.7174e-25,  ...,  1.5589e-25,\n",
      "          1.2681e-25,  1.5866e-25],\n",
      "        [-6.8130e-25, -6.8329e-25, -6.7175e-25,  ...,  1.5589e-25,\n",
      "          1.2681e-25,  1.5867e-25],\n",
      "        ...,\n",
      "        [-6.8133e-25, -6.8331e-25, -6.7178e-25,  ...,  1.5589e-25,\n",
      "          1.2681e-25,  1.5867e-25],\n",
      "        [-6.8139e-25, -6.8337e-25, -6.7183e-25,  ...,  1.5591e-25,\n",
      "          1.2682e-25,  1.5868e-25],\n",
      "        [ 6.8129e-25,  6.8327e-25,  6.7174e-25,  ..., -1.5588e-25,\n",
      "         -1.2681e-25, -1.5866e-25]])\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuuu']\n",
      "per_prediction_loss:  tensor([150.0382], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 3.5214e-18,  3.5209e-18,  3.5163e-18,  ..., -7.2767e-19,\n",
      "         -6.3812e-19, -7.9400e-19],\n",
      "        [-3.5226e-18, -3.5221e-18, -3.5175e-18,  ...,  7.2793e-19,\n",
      "          6.3834e-19,  7.9427e-19],\n",
      "        [-3.5227e-18, -3.5222e-18, -3.5176e-18,  ...,  7.2794e-19,\n",
      "          6.3835e-19,  7.9429e-19],\n",
      "        ...,\n",
      "        [-3.5228e-18, -3.5223e-18, -3.5177e-18,  ...,  7.2796e-19,\n",
      "          6.3837e-19,  7.9431e-19],\n",
      "        [-3.5232e-18, -3.5227e-18, -3.5180e-18,  ...,  7.2804e-19,\n",
      "          6.3843e-19,  7.9439e-19],\n",
      "        [ 3.5226e-18,  3.5221e-18,  3.5175e-18,  ..., -7.2793e-19,\n",
      "         -6.3833e-19, -7.9427e-19]])\n",
      "Epoch 0 iteration 76: loss = -1500.382, tp = 29.47 lines/s, ETA 00h00m17s\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['d']\n",
      "per_prediction_loss:  tensor([218.7980], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['']\n",
      "per_prediction_loss:  tensor([236.9999], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-1.9999e-39, -2.4211e-39, -2.1446e-39,  ...,  6.1636e-39,\n",
      "          5.2746e-39,  6.8034e-39],\n",
      "        [ 2.0005e-39,  2.4219e-39,  2.1453e-39,  ..., -6.1655e-39,\n",
      "         -5.2762e-39, -6.8055e-39],\n",
      "        [ 2.0012e-39,  2.4227e-39,  2.1460e-39,  ..., -6.1675e-39,\n",
      "         -5.2780e-39, -6.8078e-39],\n",
      "        ...,\n",
      "        [ 2.0009e-39,  2.4224e-39,  2.1457e-39,  ..., -6.1668e-39,\n",
      "         -5.2773e-39, -6.8070e-39],\n",
      "        [ 2.0014e-39,  2.4229e-39,  2.1462e-39,  ..., -6.1682e-39,\n",
      "         -5.2785e-39, -6.8085e-39],\n",
      "        [-2.0011e-39, -2.4225e-39, -2.1459e-39,  ...,  6.1672e-39,\n",
      "          5.2777e-39,  6.8075e-39]])\n",
      "Epoch 0 iteration 78: loss = -2364.999, tp = 29.62 lines/s, ETA 00h00m17s\n",
      "batch:  ['qqqqqqqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['qqqqq']\n",
      "per_prediction_loss:  tensor([399.9019], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 5.5785e-19,  5.5780e-19,  5.5700e-19,  ..., -1.2435e-19,\n",
      "         -1.1041e-19, -1.3886e-19],\n",
      "        [-5.5807e-19, -5.5802e-19, -5.5722e-19,  ...,  1.2440e-19,\n",
      "          1.1045e-19,  1.3891e-19],\n",
      "        [-5.5809e-19, -5.5804e-19, -5.5724e-19,  ...,  1.2440e-19,\n",
      "          1.1046e-19,  1.3892e-19],\n",
      "        ...,\n",
      "        [-5.5811e-19, -5.5806e-19, -5.5726e-19,  ...,  1.2441e-19,\n",
      "          1.1046e-19,  1.3892e-19],\n",
      "        [-5.5817e-19, -5.5812e-19, -5.5732e-19,  ...,  1.2442e-19,\n",
      "          1.1047e-19,  1.3894e-19],\n",
      "        [ 5.5807e-19,  5.5802e-19,  5.5722e-19,  ..., -1.2440e-19,\n",
      "         -1.1045e-19, -1.3892e-19]])\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuuuu']\n",
      "per_prediction_loss:  tensor([182.3529], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 5.8978e-18,  5.8961e-18,  5.8897e-18,  ..., -1.2190e-18,\n",
      "         -1.0690e-18, -1.3301e-18],\n",
      "        [-5.8998e-18, -5.8981e-18, -5.8918e-18,  ...,  1.2194e-18,\n",
      "          1.0694e-18,  1.3305e-18],\n",
      "        [-5.9000e-18, -5.8983e-18, -5.8919e-18,  ...,  1.2195e-18,\n",
      "          1.0694e-18,  1.3306e-18],\n",
      "        ...,\n",
      "        [-5.9002e-18, -5.8985e-18, -5.8921e-18,  ...,  1.2195e-18,\n",
      "          1.0695e-18,  1.3306e-18],\n",
      "        [-5.9008e-18, -5.8991e-18, -5.8927e-18,  ...,  1.2196e-18,\n",
      "          1.0696e-18,  1.3307e-18],\n",
      "        [ 5.8999e-18,  5.8982e-18,  5.8918e-18,  ..., -1.2194e-18,\n",
      "         -1.0694e-18, -1.3305e-18]])\n",
      "Epoch 0 iteration 80: loss = -1825.529, tp = 29.47 lines/s, ETA 00h00m17s\n",
      "batch:  ['ttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ttt']\n",
      "per_prediction_loss:  tensor([232.5213], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 3.2435e-25,  3.2643e-25,  3.2133e-25,  ..., -7.1878e-26,\n",
      "         -6.3383e-26, -7.4249e-26],\n",
      "        [-3.2446e-25, -3.2655e-25, -3.2144e-25,  ...,  7.1903e-26,\n",
      "          6.3405e-26,  7.4275e-26],\n",
      "        [-3.2447e-25, -3.2655e-25, -3.2145e-25,  ...,  7.1905e-26,\n",
      "          6.3406e-26,  7.4276e-26],\n",
      "        ...,\n",
      "        [-3.2448e-25, -3.2657e-25, -3.2146e-25,  ...,  7.1907e-26,\n",
      "          6.3408e-26,  7.4279e-26],\n",
      "        [-3.2451e-25, -3.2659e-25, -3.2149e-25,  ...,  7.1914e-26,\n",
      "          6.3414e-26,  7.4285e-26],\n",
      "        [ 3.2446e-25,  3.2655e-25,  3.2144e-25,  ..., -7.1904e-26,\n",
      "         -6.3405e-26, -7.4275e-26]])\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ttttt']\n",
      "per_prediction_loss:  tensor([413.3873], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 7.8466e-18,  7.8455e-18,  7.8353e-18,  ..., -1.6386e-18,\n",
      "         -1.4450e-18, -1.6927e-18],\n",
      "        [-7.8495e-18, -7.8484e-18, -7.8382e-18,  ...,  1.6392e-18,\n",
      "          1.4455e-18,  1.6933e-18],\n",
      "        [-7.8497e-18, -7.8486e-18, -7.8384e-18,  ...,  1.6393e-18,\n",
      "          1.4455e-18,  1.6933e-18],\n",
      "        ...,\n",
      "        [-7.8500e-18, -7.8489e-18, -7.8387e-18,  ...,  1.6393e-18,\n",
      "          1.4456e-18,  1.6934e-18],\n",
      "        [-7.8507e-18, -7.8496e-18, -7.8395e-18,  ...,  1.6395e-18,\n",
      "          1.4457e-18,  1.6936e-18],\n",
      "        [ 7.8496e-18,  7.8485e-18,  7.8383e-18,  ..., -1.6392e-18,\n",
      "         -1.4455e-18, -1.6933e-18]])\n",
      "Epoch 0 iteration 82: loss = -4134.873, tp = 29.43 lines/s, ETA 00h00m17s\n",
      "batch:  ['ooooo']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['oooo']\n",
      "per_prediction_loss:  tensor([231.5775], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 1.3163e-19,  1.3171e-19,  1.3137e-19,  ..., -2.8466e-20,\n",
      "         -2.5480e-20, -3.1141e-20],\n",
      "        [-1.3167e-19, -1.3175e-19, -1.3141e-19,  ...,  2.8475e-20,\n",
      "          2.5488e-20,  3.1151e-20],\n",
      "        [-1.3167e-19, -1.3176e-19, -1.3141e-19,  ...,  2.8476e-20,\n",
      "          2.5488e-20,  3.1151e-20],\n",
      "        ...,\n",
      "        [-1.3168e-19, -1.3176e-19, -1.3142e-19,  ...,  2.8477e-20,\n",
      "          2.5489e-20,  3.1152e-20],\n",
      "        [-1.3169e-19, -1.3177e-19, -1.3143e-19,  ...,  2.8480e-20,\n",
      "          2.5491e-20,  3.1155e-20],\n",
      "        [ 1.3167e-19,  1.3175e-19,  1.3141e-19,  ..., -2.8476e-20,\n",
      "         -2.5488e-20, -3.1151e-20]])\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iiii']\n",
      "per_prediction_loss:  tensor([125.0649], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[ 3.8190e-19,  3.8220e-19,  3.8111e-19,  ..., -7.9525e-20,\n",
      "         -6.9655e-20, -8.7284e-20],\n",
      "        [-3.8206e-19, -3.8236e-19, -3.8127e-19,  ...,  7.9558e-20,\n",
      "          6.9684e-20,  8.7321e-20],\n",
      "        [-3.8207e-19, -3.8236e-19, -3.8127e-19,  ...,  7.9559e-20,\n",
      "          6.9685e-20,  8.7322e-20],\n",
      "        ...,\n",
      "        [-3.8208e-19, -3.8238e-19, -3.8129e-19,  ...,  7.9563e-20,\n",
      "          6.9688e-20,  8.7326e-20],\n",
      "        [-3.8213e-19, -3.8243e-19, -3.8133e-19,  ...,  7.9572e-20,\n",
      "          6.9696e-20,  8.7336e-20],\n",
      "        [ 3.8206e-19,  3.8236e-19,  3.8127e-19,  ..., -7.9558e-20,\n",
      "         -6.9685e-20, -8.7322e-20]])\n",
      "Epoch 0 iteration 84: loss = -1250.649, tp = 29.43 lines/s, ETA 00h00m17s\n",
      "batch:  ['zzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zzz']\n",
      "per_prediction_loss:  tensor([245.2150], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 7.4689e-25,  7.5154e-25,  7.4121e-25,  ..., -1.7091e-25,\n",
      "         -1.4948e-25, -1.8737e-25],\n",
      "        [-7.4709e-25, -7.5174e-25, -7.4140e-25,  ...,  1.7096e-25,\n",
      "          1.4952e-25,  1.8742e-25],\n",
      "        [-7.4710e-25, -7.5175e-25, -7.4142e-25,  ...,  1.7096e-25,\n",
      "          1.4953e-25,  1.8742e-25],\n",
      "        ...,\n",
      "        [-7.4712e-25, -7.5177e-25, -7.4144e-25,  ...,  1.7097e-25,\n",
      "          1.4953e-25,  1.8743e-25],\n",
      "        [-7.4718e-25, -7.5183e-25, -7.4149e-25,  ...,  1.7098e-25,\n",
      "          1.4954e-25,  1.8744e-25],\n",
      "        [ 7.4709e-25,  7.5174e-25,  7.4141e-25,  ..., -1.7096e-25,\n",
      "         -1.4952e-25, -1.8742e-25]])\n",
      "batch:  ['aaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['a']\n",
      "per_prediction_loss:  tensor([238.9965], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 0 iteration 86: loss = -2386.965, tp = 29.57 lines/s, ETA 00h00m17s\n",
      "batch:  ['ccccc']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['c']\n",
      "per_prediction_loss:  tensor([201.6284], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uu']\n",
      "per_prediction_loss:  tensor([154.3864], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 0 iteration 88: loss = -1540.864, tp = 29.69 lines/s, ETA 00h00m17s\n",
      "batch:  ['lllll']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['lll']\n",
      "per_prediction_loss:  tensor([226.2458], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 1.3079e-23,  1.3143e-23,  1.3006e-23,  ..., -2.9156e-24,\n",
      "         -2.5616e-24, -3.2736e-24],\n",
      "        [-1.3083e-23, -1.3147e-23, -1.3010e-23,  ...,  2.9165e-24,\n",
      "          2.5623e-24,  3.2746e-24],\n",
      "        [-1.3083e-23, -1.3147e-23, -1.3010e-23,  ...,  2.9165e-24,\n",
      "          2.5623e-24,  3.2746e-24],\n",
      "        ...,\n",
      "        [-1.3083e-23, -1.3147e-23, -1.3010e-23,  ...,  2.9166e-24,\n",
      "          2.5624e-24,  3.2747e-24],\n",
      "        [-1.3084e-23, -1.3148e-23, -1.3011e-23,  ...,  2.9168e-24,\n",
      "          2.5626e-24,  3.2750e-24],\n",
      "        [ 1.3083e-23,  1.3147e-23,  1.3010e-23,  ..., -2.9165e-24,\n",
      "         -2.5623e-24, -3.2746e-24]])\n",
      "batch:  ['ooooo']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ooo']\n",
      "per_prediction_loss:  tensor([259.0805], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 5.5916e-25,  5.6246e-25,  5.5493e-25,  ..., -1.2709e-25,\n",
      "         -1.1376e-25, -1.3902e-25],\n",
      "        [-5.5936e-25, -5.6266e-25, -5.5513e-25,  ...,  1.2714e-25,\n",
      "          1.1380e-25,  1.3907e-25],\n",
      "        [-5.5937e-25, -5.6267e-25, -5.5513e-25,  ...,  1.2714e-25,\n",
      "          1.1380e-25,  1.3907e-25],\n",
      "        ...,\n",
      "        [-5.5939e-25, -5.6269e-25, -5.5516e-25,  ...,  1.2714e-25,\n",
      "          1.1381e-25,  1.3908e-25],\n",
      "        [-5.5944e-25, -5.6275e-25, -5.5521e-25,  ...,  1.2715e-25,\n",
      "          1.1382e-25,  1.3909e-25],\n",
      "        [ 5.5936e-25,  5.6267e-25,  5.5513e-25,  ..., -1.2714e-25,\n",
      "         -1.1380e-25, -1.3907e-25]])\n",
      "Epoch 0 iteration 90: loss = -2590.805, tp = 29.81 lines/s, ETA 00h00m17s\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuu']\n",
      "per_prediction_loss:  tensor([157.6100], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 2.0367e-24,  2.0493e-24,  2.0222e-24,  ..., -4.4622e-25,\n",
      "         -3.9138e-25, -4.8684e-25],\n",
      "        [-2.0374e-24, -2.0501e-24, -2.0230e-24,  ...,  4.4638e-25,\n",
      "          3.9152e-25,  4.8702e-25],\n",
      "        [-2.0375e-24, -2.0502e-24, -2.0231e-24,  ...,  4.4640e-25,\n",
      "          3.9153e-25,  4.8704e-25],\n",
      "        ...,\n",
      "        [-2.0376e-24, -2.0502e-24, -2.0231e-24,  ...,  4.4642e-25,\n",
      "          3.9155e-25,  4.8706e-25],\n",
      "        [-2.0378e-24, -2.0505e-24, -2.0234e-24,  ...,  4.4646e-25,\n",
      "          3.9159e-25,  4.8711e-25],\n",
      "        [ 2.0375e-24,  2.0501e-24,  2.0230e-24,  ..., -4.4639e-25,\n",
      "         -3.9153e-25, -4.8703e-25]])\n",
      "batch:  ['iiiii']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ii']\n",
      "per_prediction_loss:  tensor([132.9646], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 0 iteration 92: loss = -1328.646, tp = 29.89 lines/s, ETA 00h00m16s\n",
      "batch:  ['nnnnnnnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnnn']\n",
      "per_prediction_loss:  tensor([504.5671], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 2.3072e-18,  2.3075e-18,  2.3006e-18,  ..., -5.0229e-19,\n",
      "         -4.0881e-19, -5.1125e-19],\n",
      "        [-2.3081e-18, -2.3084e-18, -2.3015e-18,  ...,  5.0248e-19,\n",
      "          4.0897e-19,  5.1145e-19],\n",
      "        [-2.3081e-18, -2.3085e-18, -2.3016e-18,  ...,  5.0249e-19,\n",
      "          4.0897e-19,  5.1145e-19],\n",
      "        ...,\n",
      "        [-2.3082e-18, -2.3086e-18, -2.3017e-18,  ...,  5.0251e-19,\n",
      "          4.0899e-19,  5.1148e-19],\n",
      "        [-2.3084e-18, -2.3088e-18, -2.3019e-18,  ...,  5.0256e-19,\n",
      "          4.0903e-19,  5.1153e-19],\n",
      "        [ 2.3081e-18,  2.3084e-18,  2.3015e-18,  ..., -5.0248e-19,\n",
      "         -4.0896e-19, -5.1144e-19]])\n",
      "batch:  ['zzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zzz']\n",
      "per_prediction_loss:  tensor([278.3954], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 8.8802e-25,  8.9343e-25,  8.8141e-25,  ..., -2.0334e-25,\n",
      "         -1.7786e-25, -2.2290e-25],\n",
      "        [-8.8828e-25, -8.9370e-25, -8.8167e-25,  ...,  2.0340e-25,\n",
      "          1.7791e-25,  2.2297e-25],\n",
      "        [-8.8829e-25, -8.9371e-25, -8.8168e-25,  ...,  2.0340e-25,\n",
      "          1.7792e-25,  2.2297e-25],\n",
      "        ...,\n",
      "        [-8.8832e-25, -8.9374e-25, -8.8171e-25,  ...,  2.0341e-25,\n",
      "          1.7792e-25,  2.2298e-25],\n",
      "        [-8.8839e-25, -8.9381e-25, -8.8178e-25,  ...,  2.0342e-25,\n",
      "          1.7794e-25,  2.2299e-25],\n",
      "        [ 8.8828e-25,  8.9370e-25,  8.8167e-25,  ..., -2.0340e-25,\n",
      "         -1.7791e-25, -2.2297e-25]])\n",
      "Epoch 0 iteration 94: loss = -2783.954, tp = 29.91 lines/s, ETA 00h00m16s\n",
      "batch:  ['nnnnnnnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnnnnn']\n",
      "per_prediction_loss:  tensor([519.4861], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 1.6858e-17,  1.6851e-17,  1.6833e-17,  ..., -3.6464e-18,\n",
      "         -2.9678e-18, -3.7114e-18],\n",
      "        [-1.6865e-17, -1.6857e-17, -1.6839e-17,  ...,  3.6477e-18,\n",
      "          2.9689e-18,  3.7128e-18],\n",
      "        [-1.6865e-17, -1.6857e-17, -1.6840e-17,  ...,  3.6478e-18,\n",
      "          2.9690e-18,  3.7129e-18],\n",
      "        ...,\n",
      "        [-1.6865e-17, -1.6858e-17, -1.6840e-17,  ...,  3.6479e-18,\n",
      "          2.9691e-18,  3.7130e-18],\n",
      "        [-1.6867e-17, -1.6860e-17, -1.6842e-17,  ...,  3.6483e-18,\n",
      "          2.9694e-18,  3.7134e-18],\n",
      "        [ 1.6864e-17,  1.6857e-17,  1.6839e-17,  ..., -3.6477e-18,\n",
      "         -2.9689e-18, -3.7128e-18]])\n",
      "batch:  ['lllll']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['lll']\n",
      "per_prediction_loss:  tensor([249.6363], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 1.8532e-23,  1.8620e-23,  1.8432e-23,  ..., -4.1326e-24,\n",
      "         -3.6316e-24, -4.6394e-24],\n",
      "        [-1.8538e-23, -1.8626e-23, -1.8438e-23,  ...,  4.1339e-24,\n",
      "          3.6328e-24,  4.6409e-24],\n",
      "        [-1.8539e-23, -1.8626e-23, -1.8438e-23,  ...,  4.1340e-24,\n",
      "          3.6329e-24,  4.6410e-24],\n",
      "        ...,\n",
      "        [-1.8539e-23, -1.8627e-23, -1.8439e-23,  ...,  4.1342e-24,\n",
      "          3.6330e-24,  4.6412e-24],\n",
      "        [-1.8541e-23, -1.8629e-23, -1.8441e-23,  ...,  4.1345e-24,\n",
      "          3.6334e-24,  4.6416e-24],\n",
      "        [ 1.8538e-23,  1.8626e-23,  1.8438e-23,  ..., -4.1340e-24,\n",
      "         -3.6329e-24, -4.6409e-24]])\n",
      "Epoch 0 iteration 96: loss = -2496.363, tp = 29.91 lines/s, ETA 00h00m16s\n",
      "batch:  ['ccccc']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ccc']\n",
      "per_prediction_loss:  tensor([247.8160], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 1.4219e-23,  1.4288e-23,  1.4144e-23,  ..., -3.1918e-24,\n",
      "         -2.7975e-24, -3.4627e-24],\n",
      "        [-1.4224e-23, -1.4293e-23, -1.4149e-23,  ...,  3.1929e-24,\n",
      "          2.7984e-24,  3.4639e-24],\n",
      "        [-1.4224e-23, -1.4293e-23, -1.4149e-23,  ...,  3.1929e-24,\n",
      "          2.7984e-24,  3.4639e-24],\n",
      "        ...,\n",
      "        [-1.4224e-23, -1.4294e-23, -1.4150e-23,  ...,  3.1930e-24,\n",
      "          2.7985e-24,  3.4641e-24],\n",
      "        [-1.4226e-23, -1.4295e-23, -1.4151e-23,  ...,  3.1933e-24,\n",
      "          2.7988e-24,  3.4644e-24],\n",
      "        [ 1.4224e-23,  1.4293e-23,  1.4149e-23,  ..., -3.1929e-24,\n",
      "         -2.7984e-24, -3.4639e-24]])\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmm']\n",
      "per_prediction_loss:  tensor([307.4876], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[ 3.6041e-23,  3.6280e-23,  3.5802e-23,  ..., -7.5629e-24,\n",
      "         -6.4793e-24, -8.3411e-24],\n",
      "        [-3.6054e-23, -3.6294e-23, -3.5816e-23,  ...,  7.5659e-24,\n",
      "          6.4818e-24,  8.3443e-24],\n",
      "        [-3.6055e-23, -3.6295e-23, -3.5816e-23,  ...,  7.5660e-24,\n",
      "          6.4819e-24,  8.3445e-24],\n",
      "        ...,\n",
      "        [-3.6057e-23, -3.6296e-23, -3.5818e-23,  ...,  7.5664e-24,\n",
      "          6.4822e-24,  8.3449e-24],\n",
      "        [-3.6061e-23, -3.6300e-23, -3.5822e-23,  ...,  7.5672e-24,\n",
      "          6.4829e-24,  8.3458e-24],\n",
      "        [ 3.6054e-23,  3.6294e-23,  3.5816e-23,  ..., -7.5659e-24,\n",
      "         -6.4818e-24, -8.3443e-24]])\n",
      "Epoch 0 iteration 98: loss = -3074.876, tp = 29.98 lines/s, ETA 00h00m16s\n",
      "batch:  ['zzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zz']\n",
      "per_prediction_loss:  tensor([301.9510], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "batch:  ['eeeee']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eee']\n",
      "per_prediction_loss:  tensor([272.3947], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 7.6578e-25,  7.7035e-25,  7.6006e-25,  ..., -1.7436e-25,\n",
      "         -1.5565e-25, -1.9082e-25],\n",
      "        [-7.6606e-25, -7.7063e-25, -7.6034e-25,  ...,  1.7442e-25,\n",
      "          1.5570e-25,  1.9089e-25],\n",
      "        [-7.6608e-25, -7.7065e-25, -7.6036e-25,  ...,  1.7443e-25,\n",
      "          1.5571e-25,  1.9090e-25],\n",
      "        ...,\n",
      "        [-7.6610e-25, -7.7067e-25, -7.6038e-25,  ...,  1.7443e-25,\n",
      "          1.5571e-25,  1.9090e-25],\n",
      "        [-7.6617e-25, -7.7074e-25, -7.6045e-25,  ...,  1.7445e-25,\n",
      "          1.5573e-25,  1.9092e-25],\n",
      "        [ 7.6607e-25,  7.7063e-25,  7.6035e-25,  ..., -1.7442e-25,\n",
      "         -1.5571e-25, -1.9089e-25]])\n",
      "Epoch 0 iteration 100: loss = -2722.947, tp = 30.08 lines/s, ETA 00h00m16s\n",
      "batch:  ['wwwwwwwwww']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['wwwww']\n",
      "per_prediction_loss:  tensor([201.2578], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 3.7628e-18,  3.7621e-18,  3.7573e-18,  ..., -7.7968e-19,\n",
      "         -6.8453e-19, -8.4873e-19],\n",
      "        [-3.7643e-18, -3.7636e-18, -3.7588e-18,  ...,  7.8000e-19,\n",
      "          6.8481e-19,  8.4908e-19],\n",
      "        [-3.7645e-18, -3.7638e-18, -3.7590e-18,  ...,  7.8002e-19,\n",
      "          6.8483e-19,  8.4910e-19],\n",
      "        ...,\n",
      "        [-3.7646e-18, -3.7639e-18, -3.7591e-18,  ...,  7.8005e-19,\n",
      "          6.8485e-19,  8.4914e-19],\n",
      "        [-3.7651e-18, -3.7644e-18, -3.7596e-18,  ...,  7.8015e-19,\n",
      "          6.8494e-19,  8.4924e-19],\n",
      "        [ 3.7644e-18,  3.7637e-18,  3.7589e-18,  ..., -7.8001e-19,\n",
      "         -6.8482e-19, -8.4909e-19]])\n",
      "batch:  ['jjjjjjjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjjjjjj']\n",
      "per_prediction_loss:  tensor([570.2302], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 2.1588e-18,  2.1581e-18,  2.1561e-18,  ..., -4.6578e-19,\n",
      "         -4.0773e-19, -5.1536e-19],\n",
      "        [-2.1596e-18, -2.1590e-18, -2.1569e-18,  ...,  4.6596e-19,\n",
      "          4.0788e-19,  5.1556e-19],\n",
      "        [-2.1597e-18, -2.1590e-18, -2.1569e-18,  ...,  4.6597e-19,\n",
      "          4.0789e-19,  5.1557e-19],\n",
      "        ...,\n",
      "        [-2.1598e-18, -2.1591e-18, -2.1570e-18,  ...,  4.6599e-19,\n",
      "          4.0791e-19,  5.1559e-19],\n",
      "        [-2.1600e-18, -2.1593e-18, -2.1572e-18,  ...,  4.6603e-19,\n",
      "          4.0795e-19,  5.1564e-19],\n",
      "        [ 2.1596e-18,  2.1590e-18,  2.1569e-18,  ..., -4.6596e-19,\n",
      "         -4.0789e-19, -5.1556e-19]])\n",
      "Epoch 0 iteration 102: loss = -5705.302, tp = 29.93 lines/s, ETA 00h00m16s\n",
      "batch:  ['wwwwwwwwww']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['wwwwww']\n",
      "per_prediction_loss:  tensor([221.4848], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 6.8390e-18,  6.8368e-18,  6.8297e-18,  ..., -1.4160e-18,\n",
      "         -1.2432e-18, -1.5414e-18],\n",
      "        [-6.8417e-18, -6.8395e-18, -6.8324e-18,  ...,  1.4165e-18,\n",
      "          1.2437e-18,  1.5420e-18],\n",
      "        [-6.8418e-18, -6.8396e-18, -6.8325e-18,  ...,  1.4166e-18,\n",
      "          1.2437e-18,  1.5420e-18],\n",
      "        ...,\n",
      "        [-6.8421e-18, -6.8400e-18, -6.8328e-18,  ...,  1.4166e-18,\n",
      "          1.2438e-18,  1.5421e-18],\n",
      "        [-6.8429e-18, -6.8407e-18, -6.8336e-18,  ...,  1.4168e-18,\n",
      "          1.2439e-18,  1.5423e-18],\n",
      "        [ 6.8417e-18,  6.8396e-18,  6.8324e-18,  ..., -1.4166e-18,\n",
      "         -1.2437e-18, -1.5420e-18]])\n",
      "batch:  ['wwwww']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['wwww']\n",
      "per_prediction_loss:  tensor([158.3979], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 5.6733e-19,  5.6770e-19,  5.6607e-19,  ..., -1.1832e-19,\n",
      "         -1.0388e-19, -1.2879e-19],\n",
      "        [-5.6751e-19, -5.6789e-19, -5.6626e-19,  ...,  1.1836e-19,\n",
      "          1.0391e-19,  1.2884e-19],\n",
      "        [-5.6753e-19, -5.6791e-19, -5.6628e-19,  ...,  1.1836e-19,\n",
      "          1.0392e-19,  1.2884e-19],\n",
      "        ...,\n",
      "        [-5.6755e-19, -5.6793e-19, -5.6630e-19,  ...,  1.1836e-19,\n",
      "          1.0392e-19,  1.2885e-19],\n",
      "        [-5.6760e-19, -5.6798e-19, -5.6635e-19,  ...,  1.1837e-19,\n",
      "          1.0393e-19,  1.2886e-19],\n",
      "        [ 5.6752e-19,  5.6790e-19,  5.6627e-19,  ..., -1.1836e-19,\n",
      "         -1.0391e-19, -1.2884e-19]])\n",
      "Epoch 0 iteration 104: loss = -1584.979, tp = 29.89 lines/s, ETA 00h00m16s\n",
      "batch:  ['xxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['x']\n",
      "per_prediction_loss:  tensor([312.3155], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "batch:  ['zzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zzzz']\n",
      "per_prediction_loss:  tensor([344.0525], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 2.4324e-19,  2.4340e-19,  2.4278e-19,  ..., -5.3143e-20,\n",
      "         -4.6488e-20, -5.8254e-20],\n",
      "        [-2.4330e-19, -2.4346e-19, -2.4285e-19,  ...,  5.3157e-20,\n",
      "          4.6500e-20,  5.8269e-20],\n",
      "        [-2.4331e-19, -2.4347e-19, -2.4285e-19,  ...,  5.3158e-20,\n",
      "          4.6501e-20,  5.8270e-20],\n",
      "        ...,\n",
      "        [-2.4332e-19, -2.4347e-19, -2.4286e-19,  ...,  5.3159e-20,\n",
      "          4.6502e-20,  5.8271e-20],\n",
      "        [-2.4333e-19, -2.4349e-19, -2.4287e-19,  ...,  5.3163e-20,\n",
      "          4.6505e-20,  5.8275e-20],\n",
      "        [ 2.4331e-19,  2.4346e-19,  2.4285e-19,  ..., -5.3157e-20,\n",
      "         -4.6500e-20, -5.8269e-20]])\n",
      "Epoch 0 iteration 106: loss = -3441.525, tp = 29.99 lines/s, ETA 00h00m16s\n",
      "batch:  ['lllll']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['l']\n",
      "per_prediction_loss:  tensor([314.4051], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "batch:  ['zzzzzzzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zzzzzzzz']\n",
      "per_prediction_loss:  tensor([599.8680], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 2.5822e-18,  2.5814e-18,  2.5789e-18,  ..., -5.6066e-19,\n",
      "         -4.9045e-19, -6.1457e-19],\n",
      "        [-2.5831e-18, -2.5823e-18, -2.5798e-18,  ...,  5.6086e-19,\n",
      "          4.9063e-19,  6.1479e-19],\n",
      "        [-2.5831e-18, -2.5824e-18, -2.5799e-18,  ...,  5.6087e-19,\n",
      "          4.9064e-19,  6.1480e-19],\n",
      "        ...,\n",
      "        [-2.5832e-18, -2.5825e-18, -2.5800e-18,  ...,  5.6089e-19,\n",
      "          4.9066e-19,  6.1483e-19],\n",
      "        [-2.5835e-18, -2.5827e-18, -2.5802e-18,  ...,  5.6095e-19,\n",
      "          4.9071e-19,  6.1489e-19],\n",
      "        [ 2.5831e-18,  2.5823e-18,  2.5798e-18,  ..., -5.6086e-19,\n",
      "         -4.9063e-19, -6.1479e-19]])\n",
      "Epoch 0 iteration 108: loss = -6003.680, tp = 29.98 lines/s, ETA 00h00m16s\n",
      "batch:  ['ttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['tt']\n",
      "per_prediction_loss:  tensor([339.5078], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmm']\n",
      "per_prediction_loss:  tensor([375.8827], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 1.6578e-17,  1.6590e-17,  1.6549e-17,  ..., -3.3114e-18,\n",
      "         -2.8371e-18, -3.6520e-18],\n",
      "        [-1.6583e-17, -1.6595e-17, -1.6554e-17,  ...,  3.3125e-18,\n",
      "          2.8380e-18,  3.6532e-18],\n",
      "        [-1.6583e-17, -1.6595e-17, -1.6554e-17,  ...,  3.3125e-18,\n",
      "          2.8380e-18,  3.6533e-18],\n",
      "        ...,\n",
      "        [-1.6584e-17, -1.6596e-17, -1.6555e-17,  ...,  3.3127e-18,\n",
      "          2.8381e-18,  3.6534e-18],\n",
      "        [-1.6586e-17, -1.6598e-17, -1.6557e-17,  ...,  3.3130e-18,\n",
      "          2.8384e-18,  3.6537e-18],\n",
      "        [ 1.6583e-17,  1.6595e-17,  1.6554e-17,  ..., -3.3125e-18,\n",
      "         -2.8380e-18, -3.6532e-18]])\n",
      "Epoch 0 iteration 110: loss = -3758.827, tp = 30.04 lines/s, ETA 00h00m16s\n",
      "batch:  ['zzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zzzz']\n",
      "per_prediction_loss:  tensor([360.5261], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[ 2.7874e-19,  2.7892e-19,  2.7822e-19,  ..., -6.0910e-20,\n",
      "         -5.3284e-20, -6.6767e-20],\n",
      "        [-2.7882e-19, -2.7900e-19, -2.7830e-19,  ...,  6.0928e-20,\n",
      "          5.3300e-20,  6.6787e-20],\n",
      "        [-2.7883e-19, -2.7901e-19, -2.7831e-19,  ...,  6.0930e-20,\n",
      "          5.3301e-20,  6.6788e-20],\n",
      "        ...,\n",
      "        [-2.7884e-19, -2.7902e-19, -2.7832e-19,  ...,  6.0931e-20,\n",
      "          5.3302e-20,  6.6790e-20],\n",
      "        [-2.7886e-19, -2.7904e-19, -2.7834e-19,  ...,  6.0937e-20,\n",
      "          5.3307e-20,  6.6796e-20],\n",
      "        [ 2.7883e-19,  2.7900e-19,  2.7831e-19,  ..., -6.0929e-20,\n",
      "         -5.3300e-20, -6.6787e-20]])\n",
      "batch:  ['qqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['']\n",
      "per_prediction_loss:  tensor([369.1162], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 112: loss = -3687.162, tp = 30.04 lines/s, ETA 00h00m16s\n",
      "batch:  ['rrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rrr']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['kkkkkkkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kkkkk']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 114: loss = nan, tp = 30.01 lines/s, ETA 00h00m16s\n",
      "batch:  ['kkkkkkkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kkkk']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 116: loss = nan, tp = 30.00 lines/s, ETA 00h00m16s\n",
      "batch:  ['ffffffffff']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ffffff']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['m']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 118: loss = nan, tp = 29.95 lines/s, ETA 00h00m16s\n",
      "batch:  ['wwwwwwwwww']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['wwwwwww']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 120: loss = nan, tp = 29.87 lines/s, ETA 00h00m16s\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['tt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 122: loss = nan, tp = 29.80 lines/s, ETA 00h00m16s\n",
      "batch:  ['xxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 124: loss = nan, tp = 29.89 lines/s, ETA 00h00m15s\n",
      "batch:  ['ffffffffff']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ffffff']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 126: loss = nan, tp = 29.88 lines/s, ETA 00h00m15s\n",
      "batch:  ['nnnnnnnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnnnn']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjjjjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjjjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 128: loss = nan, tp = 29.73 lines/s, ETA 00h00m15s\n",
      "batch:  ['iiiii']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iiii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 130: loss = nan, tp = 29.72 lines/s, ETA 00h00m15s\n",
      "batch:  ['oooooooooo']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['oooooo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['xxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xx']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 132: loss = nan, tp = 29.74 lines/s, ETA 00h00m15s\n",
      "batch:  ['ssssssssss']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ssssss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 134: loss = nan, tp = 29.74 lines/s, ETA 00h00m15s\n",
      "batch:  ['fffff']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['fff']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 136: loss = nan, tp = 29.77 lines/s, ETA 00h00m15s\n",
      "batch:  ['qqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['qq']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['hhhhhhhhhh']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['hhhhhhh']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 138: loss = nan, tp = 29.73 lines/s, ETA 00h00m15s\n",
      "batch:  ['oooooooooo']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['oooo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 140: loss = nan, tp = 29.66 lines/s, ETA 00h00m15s\n",
      "batch:  ['ccccc']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['cc']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['tt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 142: loss = nan, tp = 29.64 lines/s, ETA 00h00m15s\n",
      "batch:  ['ccccc']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['cccc']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['aaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaaa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 144: loss = nan, tp = 29.64 lines/s, ETA 00h00m15s\n",
      "batch:  ['ssssssssss']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ssss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ssssssssss']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ssssssss']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 146: loss = nan, tp = 29.54 lines/s, ETA 00h00m15s\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['t']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['kkkkkkkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kk']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 148: loss = nan, tp = 29.47 lines/s, ETA 00h00m15s\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['rrrrrrrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rrrrr']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 150: loss = nan, tp = 29.43 lines/s, ETA 00h00m15s\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['eeeeeeeeee']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 152: loss = nan, tp = 29.37 lines/s, ETA 00h00m15s\n",
      "batch:  ['eeeee']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dddddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 154: loss = nan, tp = 29.32 lines/s, ETA 00h00m15s\n",
      "batch:  ['qqqqqqqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['qqqqqq']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 156: loss = nan, tp = 29.31 lines/s, ETA 00h00m15s\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 158: loss = nan, tp = 29.34 lines/s, ETA 00h00m15s\n",
      "batch:  ['vvvvv']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['vvv']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['hhhhhhhhhh']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['hhh']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 160: loss = nan, tp = 29.36 lines/s, ETA 00h00m14s\n",
      "batch:  ['jjjjjjjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 162: loss = nan, tp = 29.37 lines/s, ETA 00h00m14s\n",
      "batch:  ['yyyyyyyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yyyy']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmmmmmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 164: loss = nan, tp = 29.31 lines/s, ETA 00h00m14s\n",
      "batch:  ['zzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ssssssssss']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ssssss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 166: loss = nan, tp = 29.29 lines/s, ETA 00h00m14s\n",
      "batch:  ['lllll']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['lllll']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['fffff']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['f']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 168: loss = nan, tp = 29.32 lines/s, ETA 00h00m14s\n",
      "batch:  ['yyyyyyyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yyyyy']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['yyyyyyyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yyyyyy']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 170: loss = nan, tp = 29.22 lines/s, ETA 00h00m14s\n",
      "batch:  ['fffff']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['fff']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 172: loss = nan, tp = 29.23 lines/s, ETA 00h00m14s\n",
      "batch:  ['aaaaaaaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaaa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['yyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yyyy']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 174: loss = nan, tp = 29.22 lines/s, ETA 00h00m14s\n",
      "batch:  ['wwwww']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 176: loss = nan, tp = 29.30 lines/s, ETA 00h00m14s\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['zzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zz']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 178: loss = nan, tp = 29.35 lines/s, ETA 00h00m14s\n",
      "batch:  ['eeeee']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['kkkkkkkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kkkk']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 180: loss = nan, tp = 29.36 lines/s, ETA 00h00m14s\n",
      "batch:  ['sssss']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ssss']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['nnnnnnnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnnnn']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 182: loss = nan, tp = 29.35 lines/s, ETA 00h00m14s\n",
      "batch:  ['fffff']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ffff']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['rrrrrrrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rrr']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 184: loss = nan, tp = 29.36 lines/s, ETA 00h00m14s\n",
      "batch:  ['rrrrrrrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rrrrrr']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['yyyyyyyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yyyyyy']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 186: loss = nan, tp = 29.31 lines/s, ETA 00h00m14s\n",
      "batch:  ['wwwwwwwwww']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['wwwwwww']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['yyyyyyyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yyyyyy']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 188: loss = nan, tp = 29.25 lines/s, ETA 00h00m14s\n",
      "batch:  ['yyyyyyyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yyyyyy']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['wwwww']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['w']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 190: loss = nan, tp = 29.26 lines/s, ETA 00h00m13s\n",
      "batch:  ['nnnnnnnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnnn']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjjjjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 192: loss = nan, tp = 29.24 lines/s, ETA 00h00m13s\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['tttttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['yyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['y']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 194: loss = nan, tp = 29.24 lines/s, ETA 00h00m13s\n",
      "batch:  ['xxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xxx']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['aaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 196: loss = nan, tp = 29.28 lines/s, ETA 00h00m13s\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['xxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xxxx']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 198: loss = nan, tp = 29.33 lines/s, ETA 00h00m13s\n",
      "batch:  ['kkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kk']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iiiii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 200: loss = nan, tp = 29.34 lines/s, ETA 00h00m13s\n",
      "batch:  ['eeeeeeeeee']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eeeeeee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['xxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['x']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 1: loss = nan, tp = 29.34 lines/s, ETA 00h00m13s\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['tttttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['bbbbb']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['b']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 3: loss = nan, tp = 29.35 lines/s, ETA 00h00m13s\n",
      "batch:  ['aaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['a']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['oooooooooo']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['oooooooo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 5: loss = nan, tp = 29.35 lines/s, ETA 00h00m13s\n",
      "batch:  ['qqqqqqqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['qqqqq']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['eeeeeeeeee']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eeee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 7: loss = nan, tp = 29.33 lines/s, ETA 00h00m13s\n",
      "batch:  ['xxxxxxxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xxxxxx']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['tttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 9: loss = nan, tp = 29.29 lines/s, ETA 00h00m13s\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ttttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ooooo']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['oo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 11: loss = nan, tp = 29.26 lines/s, ETA 00h00m13s\n",
      "batch:  ['qqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['qq']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 13: loss = nan, tp = 29.31 lines/s, ETA 00h00m13s\n",
      "batch:  ['ccccc']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['cc']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 15: loss = nan, tp = 29.36 lines/s, ETA 00h00m13s\n",
      "batch:  ['aaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  ['wwwwwwwwww']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['wwwwwwww']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 17: loss = nan, tp = 29.34 lines/s, ETA 00h00m12s\n",
      "batch:  ['eeeee']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['wwwwwwwwww']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['wwww']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 19: loss = nan, tp = 29.34 lines/s, ETA 00h00m12s\n",
      "batch:  ['ssssssssss']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ssssss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 21: loss = nan, tp = 29.34 lines/s, ETA 00h00m12s\n",
      "batch:  ['hhhhhhhhhh']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['hhhhhhh']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 23: loss = nan, tp = 29.34 lines/s, ETA 00h00m12s\n",
      "batch:  ['ssssssssss']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ssssss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ccccc']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ccc']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 25: loss = nan, tp = 29.34 lines/s, ETA 00h00m12s\n",
      "batch:  ['ccccc']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['cccc']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 27: loss = nan, tp = 29.35 lines/s, ETA 00h00m12s\n",
      "batch:  ['zzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['z']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['xxxxxxxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xxxx']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 29: loss = nan, tp = 29.35 lines/s, ETA 00h00m12s\n",
      "batch:  ['ffffffffff']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['fffff']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['nnnnnnnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnnnn']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 31: loss = nan, tp = 29.32 lines/s, ETA 00h00m12s\n",
      "batch:  ['yyyyyyyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yyyyy']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ttttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 33: loss = nan, tp = 29.30 lines/s, ETA 00h00m12s\n",
      "batch:  ['kkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kkk']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  ['xxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['x']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 35: loss = nan, tp = 29.33 lines/s, ETA 00h00m12s\n",
      "batch:  ['fffff']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['fff']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['aaaaaaaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaaaaa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 37: loss = nan, tp = 29.34 lines/s, ETA 00h00m12s\n",
      "batch:  ['fffff']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['fff']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['aaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 39: loss = nan, tp = 29.39 lines/s, ETA 00h00m12s\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['eeeeeeeeee']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eeeeee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 41: loss = nan, tp = 29.39 lines/s, ETA 00h00m12s\n",
      "batch:  ['ssssssssss']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['sssss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjjjjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 43: loss = nan, tp = 29.35 lines/s, ETA 00h00m12s\n",
      "batch:  ['zzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zz']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['sssss']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['sss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 45: loss = nan, tp = 29.40 lines/s, ETA 00h00m12s\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['oooooooooo']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ooo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 47: loss = nan, tp = 29.39 lines/s, ETA 00h00m11s\n",
      "batch:  ['yyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yyy']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 49: loss = nan, tp = 29.39 lines/s, ETA 00h00m11s\n",
      "batch:  ['fffff']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['fff']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['nnnnnnnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnnnn']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 51: loss = nan, tp = 29.40 lines/s, ETA 00h00m11s\n",
      "batch:  ['sssss']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ffffffffff']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ffff']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 53: loss = nan, tp = 29.40 lines/s, ETA 00h00m11s\n",
      "batch:  ['pppppppppp']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ppppppp']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['aaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaaa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 55: loss = nan, tp = 29.38 lines/s, ETA 00h00m11s\n",
      "batch:  ['llllllllll']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['lll']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 57: loss = nan, tp = 29.35 lines/s, ETA 00h00m11s\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['xxxxxxxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xxx']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 59: loss = nan, tp = 29.32 lines/s, ETA 00h00m11s\n",
      "batch:  ['xxxxxxxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xxx']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjjjjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 61: loss = nan, tp = 29.30 lines/s, ETA 00h00m11s\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iiiii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['d']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 63: loss = nan, tp = 29.28 lines/s, ETA 00h00m11s\n",
      "batch:  ['nnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnnn']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['llllllllll']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['lllll']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 65: loss = nan, tp = 29.27 lines/s, ETA 00h00m11s\n",
      "batch:  ['yyyyyyyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yyyyy']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iiii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 67: loss = nan, tp = 29.23 lines/s, ETA 00h00m11s\n",
      "batch:  ['sssss']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['sss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 69: loss = nan, tp = 29.22 lines/s, ETA 00h00m11s\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['tttt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['nnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnnn']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 71: loss = nan, tp = 29.23 lines/s, ETA 00h00m11s\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['lllll']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['lllll']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 73: loss = nan, tp = 29.23 lines/s, ETA 00h00m11s\n",
      "batch:  ['wwwwwwwwww']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['wwww']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['lllll']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ll']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 75: loss = nan, tp = 29.24 lines/s, ETA 00h00m11s\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iiii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['vvvvv']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['vvvv']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 77: loss = nan, tp = 29.24 lines/s, ETA 00h00m10s\n",
      "batch:  ['eeeee']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['wwwww']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ww']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 79: loss = nan, tp = 29.27 lines/s, ETA 00h00m10s\n",
      "batch:  ['wwwwwwwwww']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['www']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ffffffffff']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ffffff']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 81: loss = nan, tp = 29.26 lines/s, ETA 00h00m10s\n",
      "batch:  ['qqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['qqq']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 83: loss = nan, tp = 29.19 lines/s, ETA 00h00m10s\n",
      "batch:  ['ppppp']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['p']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 85: loss = nan, tp = 29.19 lines/s, ETA 00h00m10s\n",
      "batch:  ['wwwww']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['wwww']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['xxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xxxx']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 87: loss = nan, tp = 29.21 lines/s, ETA 00h00m10s\n",
      "batch:  ['qqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['qqqq']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['j']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 89: loss = nan, tp = 29.20 lines/s, ETA 00h00m10s\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuuuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['zzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zzz']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 91: loss = nan, tp = 29.17 lines/s, ETA 00h00m10s\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dddddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 93: loss = nan, tp = 29.14 lines/s, ETA 00h00m10s\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 95: loss = nan, tp = 29.15 lines/s, ETA 00h00m10s\n",
      "batch:  ['nnnnnnnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnnnn']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['kkkkkkkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kkkkk']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 97: loss = nan, tp = 29.14 lines/s, ETA 00h00m10s\n",
      "batch:  ['ccccc']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['llllllllll']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['llll']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 99: loss = nan, tp = 29.17 lines/s, ETA 00h00m10s\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['rrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rr']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 101: loss = nan, tp = 29.21 lines/s, ETA 00h00m10s\n",
      "batch:  ['yyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['y']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['j']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 103: loss = nan, tp = 29.27 lines/s, ETA 00h00m10s\n",
      "batch:  ['lllll']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ccccc']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['cccc']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 105: loss = nan, tp = 29.31 lines/s, ETA 00h00m09s\n",
      "batch:  ['pppppppppp']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['pppp']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['j']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 iteration 107: loss = nan, tp = 29.33 lines/s, ETA 00h00m09s\n",
      "batch:  ['sssss']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['s']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ccccc']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ccc']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 109: loss = nan, tp = 29.36 lines/s, ETA 00h00m09s\n",
      "batch:  ['aaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['rrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rrrr']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 111: loss = nan, tp = 29.38 lines/s, ETA 00h00m09s\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['bbbbbbbbbb']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['bbbbb']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 113: loss = nan, tp = 29.35 lines/s, ETA 00h00m09s\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ooooo']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['oooo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 115: loss = nan, tp = 29.30 lines/s, ETA 00h00m09s\n",
      "batch:  ['wwwww']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ww']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['yyyyyyyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yyyyyy']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 117: loss = nan, tp = 29.25 lines/s, ETA 00h00m09s\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['tttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 119: loss = nan, tp = 29.21 lines/s, ETA 00h00m09s\n",
      "batch:  ['nnnnnnnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnnnn']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['nnnnnnnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnnn']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 121: loss = nan, tp = 29.15 lines/s, ETA 00h00m09s\n",
      "batch:  ['rrrrrrrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rrrrrrr']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['eeeeeeeeee']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eeeeee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 123: loss = nan, tp = 29.12 lines/s, ETA 00h00m09s\n",
      "batch:  ['eeeeeeeeee']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eeeeeee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 125: loss = nan, tp = 29.12 lines/s, ETA 00h00m09s\n",
      "batch:  ['ssssssssss']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['sssssss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['aaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['a']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 127: loss = nan, tp = 29.11 lines/s, ETA 00h00m09s\n",
      "batch:  ['aaaaaaaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaaa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['pppppppppp']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ppppp']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 129: loss = nan, tp = 29.10 lines/s, ETA 00h00m09s\n",
      "batch:  ['lllll']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['lll']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['zzzzzzzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zzzz']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 131: loss = nan, tp = 29.10 lines/s, ETA 00h00m09s\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iiiii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 133: loss = nan, tp = 29.07 lines/s, ETA 00h00m09s\n",
      "batch:  ['kkkkkkkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kkkkk']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['hhhhh']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['hhh']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 135: loss = nan, tp = 29.08 lines/s, ETA 00h00m09s\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmmmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 137: loss = nan, tp = 29.08 lines/s, ETA 00h00m08s\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iiii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 139: loss = nan, tp = 29.08 lines/s, ETA 00h00m08s\n",
      "batch:  ['wwwwwwwwww']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['wwww']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ooooo']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['oo']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 141: loss = nan, tp = 29.10 lines/s, ETA 00h00m08s\n",
      "batch:  ['lllll']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ll']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['aaaaaaaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaaaaa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 143: loss = nan, tp = 29.09 lines/s, ETA 00h00m08s\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['aaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 145: loss = nan, tp = 29.12 lines/s, ETA 00h00m08s\n",
      "batch:  ['rrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['r']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 147: loss = nan, tp = 29.15 lines/s, ETA 00h00m08s\n",
      "batch:  ['ssssssssss']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ssssss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['aaaaaaaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaaaa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 149: loss = nan, tp = 29.12 lines/s, ETA 00h00m08s\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['vvvvv']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['vv']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 151: loss = nan, tp = 29.15 lines/s, ETA 00h00m08s\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['tttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['fffff']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['fff']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 153: loss = nan, tp = 29.15 lines/s, ETA 00h00m08s\n",
      "batch:  ['rrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rr']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['kkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 155: loss = nan, tp = 29.18 lines/s, ETA 00h00m08s\n",
      "batch:  ['kkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kk']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['rrrrrrrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rrrr']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 157: loss = nan, tp = 29.19 lines/s, ETA 00h00m08s\n",
      "batch:  ['xxxxxxxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xxxxxxxx']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ssssssssss']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 159: loss = nan, tp = 29.17 lines/s, ETA 00h00m08s\n",
      "batch:  ['hhhhhhhhhh']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_batch_strings: ['hhhhhh']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['rrrrrrrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rrrrrr']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 161: loss = nan, tp = 29.13 lines/s, ETA 00h00m08s\n",
      "batch:  ['xxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['x']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['zzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zz']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 163: loss = nan, tp = 29.15 lines/s, ETA 00h00m08s\n",
      "batch:  ['ffffffffff']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ffffff']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddddddddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 165: loss = nan, tp = 29.12 lines/s, ETA 00h00m08s\n",
      "batch:  ['zzzzzzzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zzzzzz']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 167: loss = nan, tp = 29.10 lines/s, ETA 00h00m07s\n",
      "batch:  ['zzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zzz']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 169: loss = nan, tp = 29.13 lines/s, ETA 00h00m07s\n",
      "batch:  ['vvvvv']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['vvvv']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['yyyyyyyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yyyyyy']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 171: loss = nan, tp = 29.13 lines/s, ETA 00h00m07s\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['oooooooooo']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ooooo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 173: loss = nan, tp = 29.10 lines/s, ETA 00h00m07s\n",
      "batch:  ['xxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xxx']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['kkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kk']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 175: loss = nan, tp = 29.13 lines/s, ETA 00h00m07s\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['xxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xxxx']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 177: loss = nan, tp = 29.13 lines/s, ETA 00h00m07s\n",
      "batch:  ['eeeeeeeeee']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eeee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['m']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 179: loss = nan, tp = 29.13 lines/s, ETA 00h00m07s\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['lllll']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ll']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 181: loss = nan, tp = 29.14 lines/s, ETA 00h00m07s\n",
      "batch:  ['wwwwwwwwww']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['wwww']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['aaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 183: loss = nan, tp = 29.15 lines/s, ETA 00h00m07s\n",
      "batch:  ['jjjjjjjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjjjjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['kkkkkkkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kkkkkk']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 185: loss = nan, tp = 29.12 lines/s, ETA 00h00m07s\n",
      "batch:  ['lllll']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['llll']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['aaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 187: loss = nan, tp = 29.14 lines/s, ETA 00h00m07s\n",
      "batch:  ['bbbbbbbbbb']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['b']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['kkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kkk']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 189: loss = nan, tp = 29.15 lines/s, ETA 00h00m07s\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 191: loss = nan, tp = 29.18 lines/s, ETA 00h00m07s\n",
      "batch:  ['fffff']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddddddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 193: loss = nan, tp = 29.19 lines/s, ETA 00h00m07s\n",
      "batch:  ['jjjjjjjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['pppppppppp']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['pppp']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 195: loss = nan, tp = 29.18 lines/s, ETA 00h00m06s\n",
      "batch:  ['ooooo']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['oooo']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iiii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 197: loss = nan, tp = 29.17 lines/s, ETA 00h00m06s\n",
      "batch:  ['xxxxxxxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xxxxx']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['lllll']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['llll']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 199: loss = nan, tp = 29.18 lines/s, ETA 00h00m06s\n",
      "batch:  ['eeeeeeeeee']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['zzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zz']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 0: loss = nan, tp = 29.18 lines/s, ETA 00h00m06s\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['aaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 2: loss = nan, tp = 29.20 lines/s, ETA 00h00m06s\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['aaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 4: loss = nan, tp = 29.20 lines/s, ETA 00h00m06s\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['j']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 6: loss = nan, tp = 29.23 lines/s, ETA 00h00m06s\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjjjjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 8: loss = nan, tp = 29.22 lines/s, ETA 00h00m06s\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuuuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['fffff']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['f']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 10: loss = nan, tp = 29.23 lines/s, ETA 00h00m06s\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['tttttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['eeeeeeeeee']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eeee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 12: loss = nan, tp = 29.21 lines/s, ETA 00h00m06s\n",
      "batch:  ['xxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xx']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 14: loss = nan, tp = 29.23 lines/s, ETA 00h00m06s\n",
      "batch:  ['nnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnn']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['rrrrrrrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rrrr']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 16: loss = nan, tp = 29.22 lines/s, ETA 00h00m06s\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['tttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 18: loss = nan, tp = 29.20 lines/s, ETA 00h00m06s\n",
      "batch:  ['wwwwwwwwww']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['wwwwww']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['wwwww']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ww']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 20: loss = nan, tp = 29.19 lines/s, ETA 00h00m06s\n",
      "batch:  ['eeeeeeeeee']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eeeee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['nnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnn']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 22: loss = nan, tp = 29.21 lines/s, ETA 00h00m05s\n",
      "batch:  ['ffffffffff']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ffff']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iiiiii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 24: loss = nan, tp = 29.19 lines/s, ETA 00h00m05s\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['kkkkkkkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kkkkk']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 26: loss = nan, tp = 29.17 lines/s, ETA 00h00m05s\n",
      "batch:  ['wwwwwwwwww']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['wwwwwww']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['eeeee']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 28: loss = nan, tp = 29.18 lines/s, ETA 00h00m05s\n",
      "batch:  ['qqqqqqqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['qqqq']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 iteration 30: loss = nan, tp = 29.17 lines/s, ETA 00h00m05s\n",
      "batch:  ['ffffffffff']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['fff']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['lllll']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ll']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 32: loss = nan, tp = 29.17 lines/s, ETA 00h00m05s\n",
      "batch:  ['yyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['bbbbb']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['bbbb']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 34: loss = nan, tp = 29.19 lines/s, ETA 00h00m05s\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['oooooooooo']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['oooooo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 36: loss = nan, tp = 29.20 lines/s, ETA 00h00m05s\n",
      "batch:  ['ssssssssss']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['sssss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 38: loss = nan, tp = 29.18 lines/s, ETA 00h00m05s\n",
      "batch:  ['zzzzzzzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zzzzzz']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['aaaaaaaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 40: loss = nan, tp = 29.17 lines/s, ETA 00h00m05s\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['eeeeeeeeee']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eeeee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 42: loss = nan, tp = 29.17 lines/s, ETA 00h00m05s\n",
      "batch:  ['yyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yyyy']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['xxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xx']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 44: loss = nan, tp = 29.19 lines/s, ETA 00h00m05s\n",
      "batch:  ['hhhhhhhhhh']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['hhhhh']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['hhhhhhhhhh']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['hhhhhh']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 46: loss = nan, tp = 29.17 lines/s, ETA 00h00m05s\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iiii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 iteration 48: loss = nan, tp = 29.17 lines/s, ETA 00h00m05s\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['m']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ttttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 50: loss = nan, tp = 29.18 lines/s, ETA 00h00m05s\n",
      "batch:  ['xxxxxxxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xxxx']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 52: loss = nan, tp = 29.16 lines/s, ETA 00h00m04s\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dddddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddddddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 54: loss = nan, tp = 29.14 lines/s, ETA 00h00m04s\n",
      "batch:  ['aaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['yyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yy']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 56: loss = nan, tp = 29.16 lines/s, ETA 00h00m04s\n",
      "batch:  ['zzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['z']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['tt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 58: loss = nan, tp = 29.17 lines/s, ETA 00h00m04s\n",
      "batch:  ['xxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['x']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['kkkkkkkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kkkk']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 60: loss = nan, tp = 29.18 lines/s, ETA 00h00m04s\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['aaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaaa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 62: loss = nan, tp = 29.20 lines/s, ETA 00h00m04s\n",
      "batch:  ['rrrrrrrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rrrrr']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['kkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kkkk']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 64: loss = nan, tp = 29.19 lines/s, ETA 00h00m04s\n",
      "batch:  ['ccccc']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['cc']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ssssssssss']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['sssss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 66: loss = nan, tp = 29.20 lines/s, ETA 00h00m04s\n",
      "batch:  ['sssss']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['sss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['rrrrrrrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rrrrrrr']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 68: loss = nan, tp = 29.19 lines/s, ETA 00h00m04s\n",
      "batch:  ['qqqqqqqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['qqqq']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['iiiii']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iiii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 70: loss = nan, tp = 29.19 lines/s, ETA 00h00m04s\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 72: loss = nan, tp = 29.18 lines/s, ETA 00h00m04s\n",
      "batch:  ['nnnnnnnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnnnnn']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['pppppppppp']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ppppppp']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 74: loss = nan, tp = 29.16 lines/s, ETA 00h00m04s\n",
      "batch:  ['aaaaaaaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['xxxxxxxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xxxxx']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 76: loss = nan, tp = 29.15 lines/s, ETA 00h00m04s\n",
      "batch:  ['zzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zz']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 78: loss = nan, tp = 29.17 lines/s, ETA 00h00m04s\n",
      "batch:  ['nnnnnnnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nn']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 80: loss = nan, tp = 29.18 lines/s, ETA 00h00m04s\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 82: loss = nan, tp = 29.18 lines/s, ETA 00h00m03s\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 84: loss = nan, tp = 29.18 lines/s, ETA 00h00m03s\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['lllll']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ll']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 86: loss = nan, tp = 29.20 lines/s, ETA 00h00m03s\n",
      "batch:  ['ccccc']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['cc']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 88: loss = nan, tp = 29.21 lines/s, ETA 00h00m03s\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['tttttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ssssssssss']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['sssss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 90: loss = nan, tp = 29.20 lines/s, ETA 00h00m03s\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['tttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 92: loss = nan, tp = 29.22 lines/s, ETA 00h00m03s\n",
      "batch:  ['oooooooooo']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['oooooo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['pppppppppp']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['pppp']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 94: loss = nan, tp = 29.21 lines/s, ETA 00h00m03s\n",
      "batch:  ['rrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rrrr']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 96: loss = nan, tp = 29.23 lines/s, ETA 00h00m03s\n",
      "batch:  ['kkkkkkkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kk']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['lllll']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['lll']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 98: loss = nan, tp = 29.23 lines/s, ETA 00h00m03s\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iiiii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['oooooooooo']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ooooo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 100: loss = nan, tp = 29.22 lines/s, ETA 00h00m03s\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['m']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ffffffffff']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['fffff']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 102: loss = nan, tp = 29.22 lines/s, ETA 00h00m03s\n",
      "batch:  ['zzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zzzz']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['tt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 104: loss = nan, tp = 29.23 lines/s, ETA 00h00m03s\n",
      "batch:  ['nnnnnnnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnnnnnn']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 106: loss = nan, tp = 29.22 lines/s, ETA 00h00m03s\n",
      "batch:  ['bbbbb']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['bbb']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['yyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['y']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 108: loss = nan, tp = 29.24 lines/s, ETA 00h00m03s\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuuuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 110: loss = nan, tp = 29.24 lines/s, ETA 00h00m02s\n",
      "batch:  ['zzzzzzzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zzzzz']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['rrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rrr']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 112: loss = nan, tp = 29.24 lines/s, ETA 00h00m02s\n",
      "batch:  ['yyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yyy']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjjjjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjjjjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 114: loss = nan, tp = 29.24 lines/s, ETA 00h00m02s\n",
      "batch:  ['qqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['qqq']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddddddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 116: loss = nan, tp = 29.23 lines/s, ETA 00h00m02s\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['m']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 118: loss = nan, tp = 29.24 lines/s, ETA 00h00m02s\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['bbbbb']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['bb']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 120: loss = nan, tp = 29.25 lines/s, ETA 00h00m02s\n",
      "batch:  ['aaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjjjjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjjjjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 iteration 122: loss = nan, tp = 29.24 lines/s, ETA 00h00m02s\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['tttttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['pppppppppp']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ppppp']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 124: loss = nan, tp = 29.23 lines/s, ETA 00h00m02s\n",
      "batch:  ['oooooooooo']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ooooo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 126: loss = nan, tp = 29.21 lines/s, ETA 00h00m02s\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['j']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['fffff']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ff']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 128: loss = nan, tp = 29.24 lines/s, ETA 00h00m02s\n",
      "batch:  ['sssss']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['s']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['nnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnn']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 130: loss = nan, tp = 29.25 lines/s, ETA 00h00m02s\n",
      "batch:  ['ssssssssss']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['sssss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 132: loss = nan, tp = 29.26 lines/s, ETA 00h00m02s\n",
      "batch:  ['ccccc']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['cc']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['rrrrrrrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rrrrrrr']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 134: loss = nan, tp = 29.25 lines/s, ETA 00h00m02s\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 136: loss = nan, tp = 29.25 lines/s, ETA 00h00m02s\n",
      "batch:  ['qqqqqqqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['qqqqqq']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjjjjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 138: loss = nan, tp = 29.24 lines/s, ETA 00h00m02s\n",
      "batch:  ['rrrrrrrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rrrrr']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iiiiii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 140: loss = nan, tp = 29.22 lines/s, ETA 00h00m01s\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['tt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 142: loss = nan, tp = 29.22 lines/s, ETA 00h00m01s\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ttttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iiii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 144: loss = nan, tp = 29.20 lines/s, ETA 00h00m01s\n",
      "batch:  ['hhhhhhhhhh']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['hhhhhhhhh']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iiiiiiii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 146: loss = nan, tp = 29.17 lines/s, ETA 00h00m01s\n",
      "batch:  ['kkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kkkk']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['kkkkkkkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kkkkk']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 148: loss = nan, tp = 29.17 lines/s, ETA 00h00m01s\n",
      "batch:  ['kkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kkkk']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 150: loss = nan, tp = 29.19 lines/s, ETA 00h00m01s\n",
      "batch:  ['qqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['qqq']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['pppppppppp']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ppp']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 152: loss = nan, tp = 29.20 lines/s, ETA 00h00m01s\n",
      "batch:  ['gggggggggg']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['gggggg']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 154: loss = nan, tp = 29.18 lines/s, ETA 00h00m01s\n",
      "batch:  ['eeeeeeeeee']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmmmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 156: loss = nan, tp = 29.17 lines/s, ETA 00h00m01s\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['sssss']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['sss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 158: loss = nan, tp = 29.18 lines/s, ETA 00h00m01s\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['d']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 160: loss = nan, tp = 29.20 lines/s, ETA 00h00m01s\n",
      "batch:  ['qqqqqqqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['qqqq']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['eeeeeeeeee']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eeeeee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 162: loss = nan, tp = 29.19 lines/s, ETA 00h00m01s\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmmmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 164: loss = nan, tp = 29.18 lines/s, ETA 00h00m01s\n",
      "batch:  ['yyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yy']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dddddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 166: loss = nan, tp = 29.18 lines/s, ETA 00h00m01s\n",
      "batch:  ['wwwwwwwwww']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['wwwww']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['aaaaaaaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaaaaa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 168: loss = nan, tp = 29.17 lines/s, ETA 00h00m00s\n",
      "batch:  ['ffffffffff']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['fffffff']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['bbbbb']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['bb']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 170: loss = nan, tp = 29.17 lines/s, ETA 00h00m00s\n",
      "batch:  ['vvvvvvvvvv']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['vvvv']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ttttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 172: loss = nan, tp = 29.15 lines/s, ETA 00h00m00s\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['j']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ppppp']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['pppp']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 174: loss = nan, tp = 29.17 lines/s, ETA 00h00m00s\n",
      "batch:  ['aaaaaaaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaaa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 176: loss = nan, tp = 29.18 lines/s, ETA 00h00m00s\n",
      "batch:  ['wwwwwwwwww']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['wwww']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['eeeeeeeeee']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eeeeeee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 178: loss = nan, tp = 29.16 lines/s, ETA 00h00m00s\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['yyyyyyyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yyy']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 180: loss = nan, tp = 29.15 lines/s, ETA 00h00m00s\n",
      "batch:  ['nnnnnnnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnnnnnn']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['hhhhh']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['hh']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 182: loss = nan, tp = 29.14 lines/s, ETA 00h00m00s\n",
      "batch:  ['rrrrrrrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rrrrrrr']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['sssss']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ssss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 184: loss = nan, tp = 29.15 lines/s, ETA 00h00m00s\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 186: loss = nan, tp = 29.17 lines/s, ETA 00h00m00s\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dddddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['j']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 188: loss = nan, tp = 29.17 lines/s, ETA 00h00m00s\n",
      "batch:  ['ooooo']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['oo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 190: loss = nan, tp = 29.19 lines/s, ETA 00h00m00s\n",
      "batch:  ['kkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['k']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['rrrrrrrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rrrrr']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 192: loss = nan, tp = 29.18 lines/s, ETA 00h00m00s\n",
      "batch:  ['nnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nn']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['bbbbb']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['bb']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 194: loss = nan, tp = 29.20 lines/s, ETA 00h00m00s\n",
      "batch:  ['xxxxxxxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4992]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xxxx']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 196: loss = nan, tp = 29.19 lines/s, ETA 00h00m00s\n",
      "batch:  ['vvvvvvvvvv']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992, 0.4992,\n",
      "         0.4992, 0.4992, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['vvvvv']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['gggggggggg']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ggggg']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 198: loss = nan, tp = 29.17 lines/s, ETA 00h00m00s\n",
      "batch:  ['jjjjjjjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993,\n",
      "         0.4993, 0.4993, 0.4993]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['qqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4992, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993, 0.4993]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['qqqq']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 2 iteration 200: loss = nan, tp = 29.17 lines/s, ETA 00h00m00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f65a7045760>]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'learning_rate': 5e-3,\n",
    "    'verbose': True,\n",
    "    'batch_size': 1,\n",
    "    'init_scale': 0.01,\n",
    "    'epochs': 3,\n",
    "    'log_every':2\n",
    "}\n",
    "\n",
    "train_loss_history = train(nencoder, decoder, dumb_dataset, parameters, device)\n",
    "plt.plot(train_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., grad_fn=<SumBackward0>)\n",
      "tensor([1., 1.], grad_fn=<BernoulliBackward0>)\n",
      "None\n",
      "tensor(-0.3185, grad_fn=<SqueezeBackward1>)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ps = torch.tensor([0.3, 0.8], requires_grad=True)\n",
    "bs = torch.bernoulli(ps)\n",
    "s = bs.sum()\n",
    "print(s)\n",
    "s.backward()\n",
    "print(bs)\n",
    "print(bs.grad)\n",
    "\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "b = Categorical(ps)\n",
    "mask = b.sample()\n",
    "# next_state, reward = env.step(action)\n",
    "# loss = -m.log_prob(action) * reward\n",
    "loss = b.log_prob(mask)\n",
    "loss.backward()\n",
    "print(loss)\n",
    "print(mask.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs._grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String: 'ffffffffff'\n",
      "Encoded: 'ffffffffff'\n",
      "Decoded: 'd}{}\\x13d}}|d}\\x13\\x13d})dddA}}\\x13})}7d}}}|}}\\x13d}\\x13d}A|\\x13}}Z(AA|\\x13}}{}}}K}}AP}d}}\\x13}d}}}}\\x13d}j}}\\x13Z\\x13A}j}}}\\x13dd})}\\x13}Z\\x13}|d\\x13\\x13}}}t})}\\x13d}}dZ}}|}}d}Z}}dZAA}\\x13})}dd}}dt}\\x04d}dA}}}}A})d}}d}d}A}}\\x04}A}A\\x13d|A}}}}\\x13A}\\x13d}}}dA}}})d}d}{)\\x13|A'\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "SPLIT = 'train'\n",
    "\n",
    "import copy\n",
    "\n",
    "s = random.choice(dumb_dataset[SPLIT])\n",
    "compressed = encoder.encode(s)\n",
    "decompressed = decoder([compressed])\n",
    "\n",
    "print('String:', repr(s))\n",
    "print('Encoded:', repr(compressed))\n",
    "print('Decoded:', repr(decompressed[0]))\n",
    "print(len(decompressed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    }
   ],
   "source": [
    "def top1accuracy(dataset):\n",
    "    return len(list(filter(lambda s: s == decoder([encoder.encode(s)])[0],\n",
    "                         dataset)))/len(dataset)\n",
    "print(top1accuracy(dumb_dataset[SPLIT]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
