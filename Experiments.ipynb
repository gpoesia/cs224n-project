{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import baseline\n",
    "from encoder import *\n",
    "from baseline import *\n",
    "from decoder import *\n",
    "from alphabet import *\n",
    "from train import *\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76738 training examples, 9590 validation examples, 9616 test exampless\n"
     ]
    }
   ],
   "source": [
    "LANGUAGE = 'Python'\n",
    "\n",
    "def filter_ascii(strings):\n",
    "    'Returns only the strings that can be encoded in ASCII.'\n",
    "    l = []\n",
    "    for s in strings:\n",
    "        try:\n",
    "            s.encode('ascii')\n",
    "            if len(s) <= 80:\n",
    "                l.append(s)\n",
    "        except UnicodeEncodeError:\n",
    "            pass\n",
    "        \n",
    "    return l\n",
    "\n",
    "with open('dataset/medium.json') as f:\n",
    "    multilang_dataset = json.load(f)\n",
    "    dataset = multilang_dataset[LANGUAGE]\n",
    "    \n",
    "    dataset['train'] = filter_ascii(dataset['train'])\n",
    "    dataset['dev'] = filter_ascii(dataset['dev'])\n",
    "    dataset['test'] = filter_ascii(dataset['test'])\n",
    "    \n",
    "    tiny_dataset = {\n",
    "        'train': dataset['train'][:50],\n",
    "        'dev': dataset['train'][:50],\n",
    "        'test': dataset['train'][:50],\n",
    "    }\n",
    "    \n",
    "    print('{} training examples, {} validation examples, {} test exampless'.format(\n",
    "        len(dataset['train']), \n",
    "        len(dataset['dev']),\n",
    "        len(dataset['test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dumb_dataset():\n",
    "    'Returns a dataset where all examples are the same string, which consists of 10 times the same letter.'\n",
    "\n",
    "    SIZE = 200\n",
    "    l = []\n",
    "\n",
    "    for i in range(SIZE):\n",
    "        l.append(random.choice('abcdefghijklmnopqrstuvwxyz') * random.choice([5, 10]))\n",
    "        \n",
    "    return {'train': l, 'dev': l, 'test': l}\n",
    "\n",
    "dumb_dataset = generate_dumb_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "hello darkness\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(0) if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "alphabet = AsciiOneHotEncoding(device)\n",
    "encoder = baseline.UniformEncoder(0.9)\n",
    "decoder = AutoCompleteDecoderModel(alphabet, hidden_size=64)\n",
    "nencoder = NeuralEncoder(alphabet, epsilon=0.5, hidden_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4914, 0.4903, 0.4899, 0.4910, 0.4916, 0.4899, 0.4911]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nencoder([\"hello\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(nencoder.output_proj.weight._grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  ['xxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xxx']\n",
      "per_prediction_loss:  tensor([29.1121], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-1.8264e-13,  2.7626e-13, -1.7167e-13,  ..., -3.6948e-13,\n",
      "          1.6041e-13, -1.5306e-13],\n",
      "        [ 8.3306e-14, -2.0668e-13,  5.3575e-14,  ...,  1.9181e-13,\n",
      "         -1.1889e-13,  5.6185e-14],\n",
      "        [ 1.3395e-13, -6.2295e-13, -2.9655e-15,  ...,  3.9274e-13,\n",
      "         -3.5605e-13,  4.1427e-14],\n",
      "        ...,\n",
      "        [ 5.9640e-14, -3.2437e-14,  7.3775e-14,  ...,  1.0395e-13,\n",
      "         -1.9621e-14,  5.9733e-14],\n",
      "        [ 1.3061e-13, -1.8912e-13,  1.2536e-13,  ...,  2.6191e-13,\n",
      "         -1.0997e-13,  1.1094e-13],\n",
      "        [ 1.8296e-14,  1.0407e-13,  5.7596e-14,  ..., -1.3447e-15,\n",
      "          5.8724e-14,  3.7449e-14]])\n",
      "Epoch 0 iteration 0: loss = -4.248, tp = 27.27 lines/s, ETA 00h00m21s\n",
      "batch:  ['yyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yy']\n",
      "per_prediction_loss:  tensor([29.1120], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 8.2161e-14,  3.7885e-12, -7.2943e-13,  ...,  3.3165e-13,\n",
      "          1.3280e-12,  3.9170e-12],\n",
      "        [ 4.4907e-13, -3.4593e-12,  4.8237e-13,  ..., -5.7457e-13,\n",
      "         -1.3977e-12, -3.5848e-12],\n",
      "        [ 8.8580e-13, -4.5806e-12,  5.3668e-13,  ..., -9.1186e-13,\n",
      "         -1.9537e-12, -4.7511e-12],\n",
      "        ...,\n",
      "        [ 5.2238e-13, -4.2310e-12,  5.9939e-13,  ..., -6.8872e-13,\n",
      "         -1.7001e-12, -4.3843e-12],\n",
      "        [ 2.0062e-13,  9.5795e-13, -2.4746e-13,  ..., -9.3567e-15,\n",
      "          2.7224e-13,  9.8752e-13],\n",
      "        [-5.5251e-14, -9.0026e-13,  1.8584e-13,  ..., -6.0660e-14,\n",
      "         -3.0290e-13, -9.2923e-13]])\n",
      "batch:  ['bbbbb']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['bb']\n",
      "per_prediction_loss:  tensor([29.1134], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-3.8505e-11,  4.0130e-11, -1.1171e-11,  ..., -7.3817e-12,\n",
      "          3.0208e-12, -1.5378e-11],\n",
      "        [ 8.6149e-11, -8.8953e-11,  2.5260e-11,  ...,  1.5935e-11,\n",
      "         -7.0632e-12,  3.4059e-11],\n",
      "        [ 2.9415e-11, -3.0675e-11,  8.5282e-12,  ...,  5.6524e-12,\n",
      "         -2.3007e-12,  1.1756e-11],\n",
      "        ...,\n",
      "        [ 7.2488e-11, -7.5279e-11,  2.1116e-11,  ...,  1.3710e-11,\n",
      "         -5.7846e-12,  2.8839e-11],\n",
      "        [-1.3937e-11,  1.4375e-11, -4.0913e-12,  ..., -2.5661e-12,\n",
      "          1.1490e-12, -5.5030e-12],\n",
      "        [-3.6437e-11,  3.7629e-11, -1.0682e-11,  ..., -6.7441e-12,\n",
      "          2.9852e-12, -1.4408e-11]])\n",
      "Epoch 0 iteration 2: loss = -3.534, tp = 27.55 lines/s, ETA 00h00m21s\n",
      "batch:  ['rrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['r']\n",
      "per_prediction_loss:  tensor([29.1135], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 1.3360e-12, -9.5671e-12, -1.5884e-12,  ...,  2.3772e-12,\n",
      "         -1.3156e-13, -5.7266e-12],\n",
      "        [ 1.3374e-10, -9.2990e-11, -2.8314e-11,  ..., -7.8608e-12,\n",
      "         -5.0379e-11, -2.1387e-11],\n",
      "        [ 1.4332e-10, -9.9081e-11, -3.0255e-11,  ..., -8.5854e-12,\n",
      "         -5.4012e-11, -2.2556e-11],\n",
      "        ...,\n",
      "        [ 1.5258e-10, -9.3818e-11, -3.0447e-11,  ..., -1.2456e-11,\n",
      "         -5.8003e-11, -1.6570e-11],\n",
      "        [ 4.1288e-11, -3.0497e-11, -9.0112e-12,  ..., -1.9179e-12,\n",
      "         -1.5476e-11, -7.7446e-12],\n",
      "        [-6.6738e-11,  4.0184e-11,  1.3189e-11,  ...,  5.6902e-12,\n",
      "          2.5407e-11,  6.7038e-12]])\n",
      "batch:  ['nnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nn']\n",
      "per_prediction_loss:  tensor([29.1298], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-2.2509e-10,  1.0218e-10,  1.7276e-10,  ...,  3.5867e-11,\n",
      "          1.3468e-10,  7.4215e-11],\n",
      "        [ 9.1213e-10, -4.1879e-10, -6.9937e-10,  ..., -1.4297e-10,\n",
      "         -5.4486e-10, -3.0453e-10],\n",
      "        [ 7.3324e-10, -3.3559e-10, -5.6237e-10,  ..., -1.1546e-10,\n",
      "         -4.3819e-10, -2.4396e-10],\n",
      "        ...,\n",
      "        [ 1.0523e-09, -4.7770e-10, -8.0766e-10,  ..., -1.6765e-10,\n",
      "         -6.2960e-10, -3.4699e-10],\n",
      "        [ 3.4612e-10, -1.5821e-10, -2.6549e-10,  ..., -5.4613e-11,\n",
      "         -2.0689e-10, -1.1499e-10],\n",
      "        [-7.9783e-10,  3.6366e-10,  6.1213e-10,  ...,  1.2637e-10,\n",
      "          4.7707e-10,  2.6426e-10]])\n",
      "Epoch 0 iteration 4: loss = -3.821, tp = 30.36 lines/s, ETA 00h00m19s\n",
      "batch:  ['bbbbb']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['bbb']\n",
      "per_prediction_loss:  tensor([29.3002], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-7.2861e-09, -3.9390e-10,  1.1317e-08,  ...,  4.2112e-09,\n",
      "          1.2164e-08,  4.6919e-09],\n",
      "        [ 1.2406e-08,  6.8715e-10, -1.9267e-08,  ..., -7.1718e-09,\n",
      "         -2.0708e-08, -7.9852e-09],\n",
      "        [ 7.9524e-09,  4.4053e-10, -1.2351e-08,  ..., -4.5972e-09,\n",
      "         -1.3274e-08, -5.1185e-09],\n",
      "        ...,\n",
      "        [ 1.3140e-08,  6.8407e-10, -2.0412e-08,  ..., -7.5927e-09,\n",
      "         -2.1941e-08, -8.4676e-09],\n",
      "        [ 7.8300e-09,  4.1190e-10, -1.2162e-08,  ..., -4.5246e-09,\n",
      "         -1.3074e-08, -5.0447e-09],\n",
      "        [-1.1058e-08, -5.8207e-10,  1.7176e-08,  ...,  6.3899e-09,\n",
      "          1.8463e-08,  7.1241e-09]])\n",
      "batch:  ['rrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rr']\n",
      "per_prediction_loss:  tensor([29.4656], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-2.9509e-08,  1.6355e-08,  3.4538e-08,  ...,  1.0459e-08,\n",
      "          4.9402e-08,  3.8943e-08],\n",
      "        [ 4.8037e-08, -2.6626e-08, -5.6222e-08,  ..., -1.7025e-08,\n",
      "         -8.0420e-08, -6.3396e-08],\n",
      "        [ 4.3953e-08, -2.4373e-08, -5.1444e-08,  ..., -1.5577e-08,\n",
      "         -7.3589e-08, -5.8018e-08],\n",
      "        ...,\n",
      "        [ 5.6837e-08, -3.1525e-08, -6.6524e-08,  ..., -2.0143e-08,\n",
      "         -9.5163e-08, -7.5031e-08],\n",
      "        [ 4.4423e-08, -2.4627e-08, -5.1993e-08,  ..., -1.5744e-08,\n",
      "         -7.4372e-08, -5.8630e-08],\n",
      "        [-4.7585e-08,  2.6387e-08,  5.5695e-08,  ...,  1.6865e-08,\n",
      "          7.9670e-08,  6.2811e-08]])\n",
      "Epoch 0 iteration 6: loss = -5.120, tp = 30.69 lines/s, ETA 00h00m19s\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddd']\n",
      "per_prediction_loss:  tensor([29.3951], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-1.0550e-07,  1.0963e-07,  1.1226e-07,  ...,  3.9808e-09,\n",
      "          1.3240e-07,  7.1767e-08],\n",
      "        [ 1.7020e-07, -1.7686e-07, -1.8110e-07,  ..., -6.4258e-09,\n",
      "         -2.1361e-07, -1.1578e-07],\n",
      "        [ 1.3736e-07, -1.4274e-07, -1.4616e-07,  ..., -5.1833e-09,\n",
      "         -1.7240e-07, -9.3442e-08],\n",
      "        ...,\n",
      "        [ 1.5354e-07, -1.5955e-07, -1.6337e-07,  ..., -5.7893e-09,\n",
      "         -1.9269e-07, -1.0444e-07],\n",
      "        [ 1.3843e-07, -1.4386e-07, -1.4730e-07,  ..., -5.2207e-09,\n",
      "         -1.7374e-07, -9.4168e-08],\n",
      "        [-1.4191e-07,  1.4747e-07,  1.5100e-07,  ...,  5.3514e-09,\n",
      "          1.7810e-07,  9.6530e-08]])\n",
      "batch:  ['vvvvvvvvvv']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['vvvvvv']\n",
      "per_prediction_loss:  tensor([55.2291], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-2.3910e-06,  1.6671e-06,  1.9758e-06,  ..., -4.3194e-07,\n",
      "          2.3292e-06,  1.5407e-06],\n",
      "        [ 2.7931e-06, -1.9455e-06, -2.3069e-06,  ...,  5.0359e-07,\n",
      "         -2.7187e-06, -1.7977e-06],\n",
      "        [ 2.5946e-06, -1.8084e-06, -2.1436e-06,  ...,  4.6837e-07,\n",
      "         -2.5267e-06, -1.6712e-06],\n",
      "        ...,\n",
      "        [ 2.7805e-06, -1.9361e-06, -2.2961e-06,  ...,  5.0095e-07,\n",
      "         -2.7058e-06, -1.7889e-06],\n",
      "        [ 2.2876e-06, -1.5924e-06, -1.8888e-06,  ...,  4.1186e-07,\n",
      "         -2.2256e-06, -1.4712e-06],\n",
      "        [-2.6341e-06,  1.8343e-06,  2.1753e-06,  ..., -4.7461e-07,\n",
      "          2.5634e-06,  1.6948e-06]])\n",
      "Epoch 0 iteration 8: loss = -10.664, tp = 26.72 lines/s, ETA 00h00m22s\n",
      "batch:  ['cccccccccc']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['cc']\n",
      "per_prediction_loss:  tensor([55.6920], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-2.3680e-06,  2.5575e-06,  2.4326e-06,  ..., -5.7451e-07,\n",
      "          2.3050e-06,  1.8967e-06],\n",
      "        [ 2.4787e-06, -2.6772e-06, -2.5464e-06,  ...,  6.0142e-07,\n",
      "         -2.4128e-06, -1.9854e-06],\n",
      "        [ 2.4091e-06, -2.6019e-06, -2.4748e-06,  ...,  5.8450e-07,\n",
      "         -2.3449e-06, -1.9296e-06],\n",
      "        ...,\n",
      "        [ 2.4767e-06, -2.6750e-06, -2.5443e-06,  ...,  6.0091e-07,\n",
      "         -2.4112e-06, -1.9840e-06],\n",
      "        [ 2.1568e-06, -2.3295e-06, -2.2157e-06,  ...,  5.2330e-07,\n",
      "         -2.0998e-06, -1.7277e-06],\n",
      "        [-2.4195e-06,  2.6132e-06,  2.4855e-06,  ..., -5.8702e-07,\n",
      "          2.3555e-06,  1.9381e-06]])\n",
      "batch:  ['ddddd']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddd']\n",
      "per_prediction_loss:  tensor([34.9195], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-1.0608e-05,  1.1103e-05,  1.0535e-05,  ..., -5.9990e-06,\n",
      "          1.1745e-05,  8.0946e-06],\n",
      "        [ 1.1136e-05, -1.1659e-05, -1.1061e-05,  ...,  6.3039e-06,\n",
      "         -1.2327e-05, -8.4960e-06],\n",
      "        [ 1.0836e-05, -1.1344e-05, -1.0763e-05,  ...,  6.1310e-06,\n",
      "         -1.1997e-05, -8.2682e-06],\n",
      "        ...,\n",
      "        [ 1.1814e-05, -1.2347e-05, -1.1721e-05,  ...,  6.6557e-06,\n",
      "         -1.3090e-05, -9.0202e-06],\n",
      "        [ 1.0524e-05, -1.0999e-05, -1.0441e-05,  ...,  5.9297e-06,\n",
      "         -1.1661e-05, -8.0352e-06],\n",
      "        [-1.1400e-05,  1.1916e-05,  1.1311e-05,  ..., -6.4256e-06,\n",
      "          1.2630e-05,  8.7033e-06]])\n",
      "Epoch 0 iteration 10: loss = -6.025, tp = 24.55 lines/s, ETA 00h00m23s\n",
      "batch:  ['eeeeeeeeee']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eeee']\n",
      "per_prediction_loss:  tensor([56.1106], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-1.3219e-06,  1.5867e-06,  1.4599e-06,  ..., -5.3142e-07,\n",
      "          9.2074e-07,  9.7533e-07],\n",
      "        [ 1.3562e-06, -1.6250e-06, -1.4957e-06,  ...,  5.4702e-07,\n",
      "         -9.4828e-07, -1.0016e-06],\n",
      "        [ 1.3269e-06, -1.5921e-06, -1.4650e-06,  ...,  5.3385e-07,\n",
      "         -9.2508e-07, -9.7928e-07],\n",
      "        ...,\n",
      "        [ 1.2710e-06, -1.5370e-06, -1.4118e-06,  ...,  5.0397e-07,\n",
      "         -8.7134e-07, -9.3409e-07],\n",
      "        [ 1.2246e-06, -1.4762e-06, -1.3570e-06,  ...,  4.8848e-07,\n",
      "         -8.4533e-07, -9.0154e-07],\n",
      "        [-1.2713e-06,  1.5345e-06,  1.4102e-06,  ..., -5.0580e-07,\n",
      "          8.7496e-07,  9.3521e-07]])\n",
      "batch:  ['yyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yy']\n",
      "per_prediction_loss:  tensor([39.6355], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-8.1771e-06,  9.2777e-06,  8.5461e-06,  ..., -6.1378e-06,\n",
      "          6.3486e-06,  4.9979e-06],\n",
      "        [ 8.1194e-06, -9.2198e-06, -8.4900e-06,  ...,  6.0988e-06,\n",
      "         -6.2989e-06, -4.9601e-06],\n",
      "        [ 8.1859e-06, -9.2902e-06, -8.5567e-06,  ...,  6.1458e-06,\n",
      "         -6.3537e-06, -5.0024e-06],\n",
      "        ...,\n",
      "        [ 9.2728e-06, -1.0500e-05, -9.6793e-06,  ...,  6.9483e-06,\n",
      "         -7.2126e-06, -5.6745e-06],\n",
      "        [ 8.3330e-06, -9.4439e-06, -8.7030e-06,  ...,  6.2488e-06,\n",
      "         -6.4764e-06, -5.0967e-06],\n",
      "        [-8.8927e-06,  1.0075e-05,  9.2859e-06,  ..., -6.6670e-06,\n",
      "          6.9129e-06,  5.4398e-06]])\n",
      "Epoch 0 iteration 12: loss = -6.704, tp = 24.34 lines/s, ETA 00h00m24s\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['d']\n",
      "per_prediction_loss:  tensor([42.8584], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-3.6249e-06,  3.8284e-06,  3.5942e-06,  ..., -2.0640e-06,\n",
      "          3.1726e-06,  2.3666e-06],\n",
      "        [ 3.6006e-06, -3.8028e-06, -3.5701e-06,  ...,  2.0501e-06,\n",
      "         -3.1509e-06, -2.3505e-06],\n",
      "        [ 3.6423e-06, -3.8469e-06, -3.6115e-06,  ...,  2.0739e-06,\n",
      "         -3.1878e-06, -2.3780e-06],\n",
      "        ...,\n",
      "        [ 4.0634e-06, -4.2914e-06, -4.0289e-06,  ...,  2.3138e-06,\n",
      "         -3.5581e-06, -2.6541e-06],\n",
      "        [ 3.6603e-06, -3.8658e-06, -3.6293e-06,  ...,  2.0842e-06,\n",
      "         -3.2041e-06, -2.3901e-06],\n",
      "        [-3.8964e-06,  4.1151e-06,  3.8634e-06,  ..., -2.2187e-06,\n",
      "          3.4113e-06,  2.5447e-06]])\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dd']\n",
      "per_prediction_loss:  tensor([46.1751], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-4.6560e-08,  4.7600e-08,  4.4934e-08,  ..., -1.8976e-08,\n",
      "          2.6422e-08,  2.0239e-08],\n",
      "        [ 4.5922e-08, -4.6948e-08, -4.4318e-08,  ...,  1.8716e-08,\n",
      "         -2.6059e-08, -1.9961e-08],\n",
      "        [ 4.6614e-08, -4.7656e-08, -4.4986e-08,  ...,  1.8998e-08,\n",
      "         -2.6452e-08, -2.0262e-08],\n",
      "        ...,\n",
      "        [ 4.8577e-08, -4.9662e-08, -4.6880e-08,  ...,  1.9799e-08,\n",
      "         -2.7568e-08, -2.1117e-08],\n",
      "        [ 4.5125e-08, -4.6133e-08, -4.3549e-08,  ...,  1.8391e-08,\n",
      "         -2.5606e-08, -1.9615e-08],\n",
      "        [-4.7672e-08,  4.8736e-08,  4.6006e-08,  ..., -1.9430e-08,\n",
      "          2.7053e-08,  2.0723e-08]])\n",
      "Epoch 0 iteration 14: loss = -6.628, tp = 25.31 lines/s, ETA 00h00m23s\n",
      "batch:  ['eeeee']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ee']\n",
      "per_prediction_loss:  tensor([42.4361], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-2.8651e-10,  2.9517e-10,  2.7707e-10,  ..., -6.3007e-11,\n",
      "          9.0173e-11,  8.4622e-11],\n",
      "        [ 2.8470e-10, -2.9329e-10, -2.7531e-10,  ...,  6.2512e-11,\n",
      "         -8.9464e-11, -8.3973e-11],\n",
      "        [ 2.8660e-10, -2.9526e-10, -2.7716e-10,  ...,  6.3023e-11,\n",
      "         -9.0196e-11, -8.4644e-11],\n",
      "        ...,\n",
      "        [ 2.9122e-10, -3.0005e-10, -2.8164e-10,  ...,  6.4215e-11,\n",
      "         -9.1904e-11, -8.6216e-11],\n",
      "        [ 2.7944e-10, -2.8786e-10, -2.7022e-10,  ...,  6.1211e-11,\n",
      "         -8.7599e-11, -8.2248e-11],\n",
      "        [-2.8857e-10,  2.9730e-10,  2.7907e-10,  ..., -6.3541e-11,\n",
      "          9.0939e-11,  8.5326e-11]])\n",
      "batch:  ['aaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaa']\n",
      "per_prediction_loss:  tensor([34.9326], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 4.1494e-17, -4.0291e-17, -3.9436e-17,  ..., -1.1418e-17,\n",
      "          1.5495e-17,  1.1633e-17],\n",
      "        [-4.1512e-17,  4.0309e-17,  3.9453e-17,  ...,  1.1423e-17,\n",
      "         -1.5502e-17, -1.1638e-17],\n",
      "        [-4.1502e-17,  4.0299e-17,  3.9443e-17,  ...,  1.1420e-17,\n",
      "         -1.5498e-17, -1.1636e-17],\n",
      "        ...,\n",
      "        [-4.1517e-17,  4.0313e-17,  3.9457e-17,  ...,  1.1424e-17,\n",
      "         -1.5503e-17, -1.1640e-17],\n",
      "        [-4.1404e-17,  4.0204e-17,  3.9350e-17,  ...,  1.1393e-17,\n",
      "         -1.5461e-17, -1.1608e-17],\n",
      "        [ 4.1475e-17, -4.0273e-17, -3.9418e-17,  ..., -1.1413e-17,\n",
      "          1.5488e-17,  1.1628e-17]])\n",
      "Epoch 0 iteration 16: loss = -7.092, tp = 25.30 lines/s, ETA 00h00m23s\n",
      "batch:  ['jjjjjjjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975,\n",
      "         0.4975, 0.4975, 0.4975]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjjj']\n",
      "per_prediction_loss:  tensor([58.9111], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 1.0769e-13, -1.0649e-13, -1.0290e-13,  ..., -1.1271e-14,\n",
      "          1.5605e-14,  1.1477e-14],\n",
      "        [-1.0773e-13,  1.0653e-13,  1.0294e-13,  ...,  1.1275e-14,\n",
      "         -1.5611e-14, -1.1481e-14],\n",
      "        [-1.0770e-13,  1.0650e-13,  1.0292e-13,  ...,  1.1273e-14,\n",
      "         -1.5607e-14, -1.1478e-14],\n",
      "        ...,\n",
      "        [-1.0775e-13,  1.0655e-13,  1.0296e-13,  ...,  1.1277e-14,\n",
      "         -1.5614e-14, -1.1483e-14],\n",
      "        [-1.0749e-13,  1.0629e-13,  1.0271e-13,  ...,  1.1250e-14,\n",
      "         -1.5577e-14, -1.1456e-14],\n",
      "        [ 1.0769e-13, -1.0649e-13, -1.0290e-13,  ..., -1.1271e-14,\n",
      "          1.5605e-14,  1.1476e-14]])\n",
      "batch:  ['xxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xx']\n",
      "per_prediction_loss:  tensor([39.4852], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 1.7513e-22, -1.7169e-22, -1.5290e-22,  ..., -7.5180e-23,\n",
      "          9.9539e-23,  6.2727e-23],\n",
      "        [-1.7520e-22,  1.7175e-22,  1.5295e-22,  ...,  7.5208e-23,\n",
      "         -9.9575e-23, -6.2750e-23],\n",
      "        [-1.7519e-22,  1.7175e-22,  1.5295e-22,  ...,  7.5206e-23,\n",
      "         -9.9574e-23, -6.2749e-23],\n",
      "        ...,\n",
      "        [-1.7523e-22,  1.7178e-22,  1.5298e-22,  ...,  7.5221e-23,\n",
      "         -9.9593e-23, -6.2761e-23],\n",
      "        [-1.7488e-22,  1.7144e-22,  1.5268e-22,  ...,  7.5072e-23,\n",
      "         -9.9395e-23, -6.2637e-23],\n",
      "        [ 1.7518e-22, -1.7174e-22, -1.5294e-22,  ..., -7.5202e-23,\n",
      "          9.9567e-23,  6.2745e-23]])\n",
      "Epoch 0 iteration 18: loss = -6.906, tp = 25.57 lines/s, ETA 00h00m22s\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uu']\n",
      "per_prediction_loss:  tensor([36.4911], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 1.5342e-25, -1.4565e-25, -1.4260e-25,  ..., -5.7584e-26,\n",
      "          7.6854e-26,  5.8305e-26],\n",
      "        [-1.5346e-25,  1.4570e-25,  1.4264e-25,  ...,  5.7600e-26,\n",
      "         -7.6876e-26, -5.8321e-26],\n",
      "        [-1.5341e-25,  1.4565e-25,  1.4259e-25,  ...,  5.7580e-26,\n",
      "         -7.6849e-26, -5.8301e-26],\n",
      "        ...,\n",
      "        [-1.5343e-25,  1.4567e-25,  1.4261e-25,  ...,  5.7589e-26,\n",
      "         -7.6861e-26, -5.8310e-26],\n",
      "        [-1.5329e-25,  1.4553e-25,  1.4248e-25,  ...,  5.7536e-26,\n",
      "         -7.6790e-26, -5.8256e-26],\n",
      "        [ 1.5342e-25, -1.4565e-25, -1.4260e-25,  ..., -5.7584e-26,\n",
      "          7.6854e-26,  5.8304e-26]])\n",
      "batch:  ['sssss']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['s']\n",
      "per_prediction_loss:  tensor([38.2434], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 9.6951e-23, -9.3891e-23, -8.7635e-23,  ..., -7.7389e-23,\n",
      "          1.0185e-22,  7.9499e-23],\n",
      "        [-9.6969e-23,  9.3908e-23,  8.7650e-23,  ...,  7.7403e-23,\n",
      "         -1.0187e-22, -7.9513e-23],\n",
      "        [-9.6956e-23,  9.3896e-23,  8.7639e-23,  ...,  7.7393e-23,\n",
      "         -1.0186e-22, -7.9503e-23],\n",
      "        ...,\n",
      "        [-9.6974e-23,  9.3913e-23,  8.7655e-23,  ...,  7.7407e-23,\n",
      "         -1.0188e-22, -7.9518e-23],\n",
      "        [-9.6871e-23,  9.3814e-23,  8.7562e-23,  ...,  7.7325e-23,\n",
      "         -1.0177e-22, -7.9433e-23],\n",
      "        [ 9.6956e-23, -9.3896e-23, -8.7639e-23,  ..., -7.7393e-23,\n",
      "          1.0186e-22,  7.9503e-23]])\n",
      "Epoch 0 iteration 20: loss = -5.172, tp = 26.33 lines/s, ETA 00h00m21s\n",
      "batch:  ['vvvvv']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['vv']\n",
      "per_prediction_loss:  tensor([63.6978], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[ 9.2245e-33, -8.8695e-33, -8.4658e-33,  ..., -3.2483e-33,\n",
      "          4.2021e-33,  3.3127e-33],\n",
      "        [-9.2256e-33,  8.8706e-33,  8.4669e-33,  ...,  3.2487e-33,\n",
      "         -4.2027e-33, -3.3132e-33],\n",
      "        [-9.2247e-33,  8.8697e-33,  8.4661e-33,  ...,  3.2484e-33,\n",
      "         -4.2022e-33, -3.3128e-33],\n",
      "        ...,\n",
      "        [-9.2258e-33,  8.8708e-33,  8.4671e-33,  ...,  3.2488e-33,\n",
      "         -4.2027e-33, -3.3132e-33],\n",
      "        [-9.2200e-33,  8.8652e-33,  8.4617e-33,  ...,  3.2467e-33,\n",
      "         -4.2001e-33, -3.3111e-33],\n",
      "        [ 9.2245e-33, -8.8695e-33, -8.4659e-33,  ..., -3.2483e-33,\n",
      "          4.2021e-33,  3.3128e-33]])\n",
      "batch:  ['lllll']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ll']\n",
      "per_prediction_loss:  tensor([38.7027], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 8.0104e-36, -7.8036e-36, -7.3605e-36,  ..., -2.7983e-36,\n",
      "          3.5090e-36,  2.7237e-36],\n",
      "        [-8.0100e-36,  7.8032e-36,  7.3601e-36,  ...,  2.7982e-36,\n",
      "         -3.5088e-36, -2.7236e-36],\n",
      "        [-8.0102e-36,  7.8034e-36,  7.3603e-36,  ...,  2.7983e-36,\n",
      "         -3.5090e-36, -2.7237e-36],\n",
      "        ...,\n",
      "        [-8.0104e-36,  7.8036e-36,  7.3604e-36,  ...,  2.7983e-36,\n",
      "         -3.5090e-36, -2.7237e-36],\n",
      "        [-8.0049e-36,  7.7982e-36,  7.3554e-36,  ...,  2.7964e-36,\n",
      "         -3.5066e-36, -2.7218e-36],\n",
      "        [ 8.0094e-36, -7.8026e-36, -7.3595e-36,  ..., -2.7980e-36,\n",
      "          3.5086e-36,  2.7234e-36]])\n",
      "Epoch 0 iteration 22: loss = -6.619, tp = 26.75 lines/s, ETA 00h00m21s\n",
      "batch:  ['nnnnnnnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnnnnnn']\n",
      "per_prediction_loss:  tensor([97.8753], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 3.8779e-11, -3.8772e-11, -3.8484e-11,  ..., -1.7770e-12,\n",
      "          2.3710e-12,  1.6499e-12],\n",
      "        [-3.8781e-11,  3.8774e-11,  3.8486e-11,  ...,  1.7770e-12,\n",
      "         -2.3710e-12, -1.6500e-12],\n",
      "        [-3.8779e-11,  3.8772e-11,  3.8484e-11,  ...,  1.7769e-12,\n",
      "         -2.3709e-12, -1.6499e-12],\n",
      "        ...,\n",
      "        [-3.8781e-11,  3.8774e-11,  3.8485e-11,  ...,  1.7770e-12,\n",
      "         -2.3710e-12, -1.6500e-12],\n",
      "        [-3.8771e-11,  3.8764e-11,  3.8475e-11,  ...,  1.7766e-12,\n",
      "         -2.3704e-12, -1.6496e-12],\n",
      "        [ 3.8779e-11, -3.8772e-11, -3.8483e-11,  ..., -1.7769e-12,\n",
      "          2.3709e-12,  1.6499e-12]])\n",
      "batch:  ['nnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nn']\n",
      "per_prediction_loss:  tensor([59.7943], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 1.0769e-40, -1.1055e-40, -1.0599e-40,  ..., -3.6984e-41,\n",
      "          4.6443e-41,  3.7553e-41],\n",
      "        [-1.0770e-40,  1.1056e-40,  1.0600e-40,  ...,  3.6987e-41,\n",
      "         -4.6447e-41, -3.7556e-41],\n",
      "        [-1.0770e-40,  1.1056e-40,  1.0599e-40,  ...,  3.6986e-41,\n",
      "         -4.6445e-41, -3.7555e-41],\n",
      "        ...,\n",
      "        [-1.0770e-40,  1.1056e-40,  1.0599e-40,  ...,  3.6986e-41,\n",
      "         -4.6446e-41, -3.7555e-41],\n",
      "        [-1.0767e-40,  1.1053e-40,  1.0596e-40,  ...,  3.6976e-41,\n",
      "         -4.6433e-41, -3.7545e-41],\n",
      "        [ 1.0770e-40, -1.1055e-40, -1.0599e-40,  ..., -3.6984e-41,\n",
      "          4.6445e-41,  3.7553e-41]])\n",
      "Epoch 0 iteration 24: loss = -10.798, tp = 26.86 lines/s, ETA 00h00m21s\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977,\n",
      "         0.4977, 0.4977, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddddddd']\n",
      "per_prediction_loss:  tensor([145.8354], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-6.8345e-14,  6.8327e-14,  6.7922e-14,  ...,  1.0965e-14,\n",
      "         -1.3457e-14, -1.0784e-14],\n",
      "        [ 6.8346e-14, -6.8328e-14, -6.7923e-14,  ..., -1.0965e-14,\n",
      "          1.3457e-14,  1.0784e-14],\n",
      "        [ 6.8344e-14, -6.8326e-14, -6.7921e-14,  ..., -1.0965e-14,\n",
      "          1.3457e-14,  1.0784e-14],\n",
      "        ...,\n",
      "        [ 6.8346e-14, -6.8328e-14, -6.7923e-14,  ..., -1.0965e-14,\n",
      "          1.3457e-14,  1.0784e-14],\n",
      "        [ 6.8334e-14, -6.8316e-14, -6.7911e-14,  ..., -1.0963e-14,\n",
      "          1.3455e-14,  1.0782e-14],\n",
      "        [-6.8344e-14,  6.8326e-14,  6.7921e-14,  ...,  1.0965e-14,\n",
      "         -1.3457e-14, -1.0784e-14]])\n",
      "batch:  ['cccccccccc']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['cccccc']\n",
      "per_prediction_loss:  tensor([130.4290], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-3.7757e-14,  3.7749e-14,  3.7552e-14,  ...,  5.9825e-15,\n",
      "         -7.4004e-15, -6.0347e-15],\n",
      "        [ 3.7757e-14, -3.7749e-14, -3.7552e-14,  ..., -5.9826e-15,\n",
      "          7.4005e-15,  6.0348e-15],\n",
      "        [ 3.7756e-14, -3.7749e-14, -3.7552e-14,  ..., -5.9825e-15,\n",
      "          7.4004e-15,  6.0347e-15],\n",
      "        ...,\n",
      "        [ 3.7757e-14, -3.7749e-14, -3.7552e-14,  ..., -5.9826e-15,\n",
      "          7.4005e-15,  6.0348e-15],\n",
      "        [ 3.7751e-14, -3.7743e-14, -3.7546e-14,  ..., -5.9817e-15,\n",
      "          7.3994e-15,  6.0339e-15],\n",
      "        [-3.7756e-14,  3.7748e-14,  3.7551e-14,  ...,  5.9824e-15,\n",
      "         -7.4003e-15, -6.0346e-15]])\n",
      "Epoch 0 iteration 26: loss = -25.532, tp = 26.43 lines/s, ETA 00h00m21s\n",
      "batch:  ['cccccccccc']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ccccc']\n",
      "per_prediction_loss:  tensor([140.2650], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-4.0007e-15,  3.9996e-15,  3.9786e-15,  ...,  6.5955e-16,\n",
      "         -8.1185e-16, -6.6450e-16],\n",
      "        [ 4.0008e-15, -3.9996e-15, -3.9787e-15,  ..., -6.5956e-16,\n",
      "          8.1186e-16,  6.6452e-16],\n",
      "        [ 4.0007e-15, -3.9996e-15, -3.9786e-15,  ..., -6.5955e-16,\n",
      "          8.1185e-16,  6.6451e-16],\n",
      "        ...,\n",
      "        [ 4.0008e-15, -3.9997e-15, -3.9787e-15,  ..., -6.5957e-16,\n",
      "          8.1187e-16,  6.6452e-16],\n",
      "        [ 4.0002e-15, -3.9991e-15, -3.9781e-15,  ..., -6.5947e-16,\n",
      "          8.1176e-16,  6.6443e-16],\n",
      "        [-4.0007e-15,  3.9995e-15,  3.9786e-15,  ...,  6.5954e-16,\n",
      "         -8.1184e-16, -6.6450e-16]])\n",
      "batch:  ['kkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kk']\n",
      "per_prediction_loss:  tensor([41.7752], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[ 1.4013e-45, -1.4013e-45,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-1.4013e-45,  1.4013e-45,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-1.4013e-45,  1.4013e-45,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [-1.4013e-45,  1.4013e-45,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-1.4013e-45,  1.4013e-45,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 1.4013e-45, -1.4013e-45,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]])\n",
      "Epoch 0 iteration 28: loss = -9.375, tp = 25.98 lines/s, ETA 00h00m21s\n",
      "batch:  ['vvvvv']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['vv']\n",
      "per_prediction_loss:  tensor([86.6997], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "batch:  ['qqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['qqq']\n",
      "per_prediction_loss:  tensor([44.0707], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-3.1435e-25,  3.1340e-25,  3.0596e-25,  ...,  6.0948e-26,\n",
      "         -7.2656e-26, -5.9705e-26],\n",
      "        [ 3.1435e-25, -3.1340e-25, -3.0597e-25,  ..., -6.0950e-26,\n",
      "          7.2657e-26,  5.9707e-26],\n",
      "        [ 3.1435e-25, -3.1340e-25, -3.0597e-25,  ..., -6.0949e-26,\n",
      "          7.2657e-26,  5.9706e-26],\n",
      "        ...,\n",
      "        [ 3.1435e-25, -3.1340e-25, -3.0596e-25,  ..., -6.0948e-26,\n",
      "          7.2656e-26,  5.9706e-26],\n",
      "        [ 3.1432e-25, -3.1337e-25, -3.0594e-25,  ..., -6.0944e-26,\n",
      "          7.2651e-26,  5.9701e-26],\n",
      "        [-3.1435e-25,  3.1339e-25,  3.0596e-25,  ...,  6.0948e-26,\n",
      "         -7.2655e-26, -5.9705e-26]])\n",
      "Epoch 0 iteration 30: loss = -11.233, tp = 26.42 lines/s, ETA 00h00m21s\n",
      "batch:  ['ppppp']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['pp']\n",
      "per_prediction_loss:  tensor([43.7791], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "batch:  ['yyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['y']\n",
      "per_prediction_loss:  tensor([91.8932], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 0 iteration 32: loss = -18.179, tp = 26.83 lines/s, ETA 00h00m21s\n",
      "batch:  ['gggggggggg']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ggg']\n",
      "per_prediction_loss:  tensor([69.9678], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-2.5173e-25,  2.5089e-25,  2.4648e-25,  ...,  5.1123e-26,\n",
      "         -6.0787e-26, -4.9464e-26],\n",
      "        [ 2.5173e-25, -2.5089e-25, -2.4648e-25,  ..., -5.1123e-26,\n",
      "          6.0787e-26,  4.9464e-26],\n",
      "        [ 2.5173e-25, -2.5089e-25, -2.4648e-25,  ..., -5.1123e-26,\n",
      "          6.0787e-26,  4.9464e-26],\n",
      "        ...,\n",
      "        [ 2.5173e-25, -2.5089e-25, -2.4648e-25,  ..., -5.1123e-26,\n",
      "          6.0787e-26,  4.9464e-26],\n",
      "        [ 2.5172e-25, -2.5088e-25, -2.4647e-25,  ..., -5.1120e-26,\n",
      "          6.0784e-26,  4.9462e-26],\n",
      "        [-2.5173e-25,  2.5089e-25,  2.4648e-25,  ...,  5.1122e-26,\n",
      "         -6.0786e-26, -4.9464e-26]])\n",
      "batch:  ['ppppp']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['p']\n",
      "per_prediction_loss:  tensor([54.3241], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 0 iteration 34: loss = -13.123, tp = 26.94 lines/s, ETA 00h00m20s\n",
      "batch:  ['rrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rrr']\n",
      "per_prediction_loss:  tensor([107.4936], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-4.9615e-25,  4.9478e-25,  4.8790e-25,  ...,  1.0554e-25,\n",
      "         -1.2908e-25, -1.0793e-25],\n",
      "        [ 4.9615e-25, -4.9478e-25, -4.8790e-25,  ..., -1.0554e-25,\n",
      "          1.2908e-25,  1.0793e-25],\n",
      "        [ 4.9615e-25, -4.9478e-25, -4.8790e-25,  ..., -1.0554e-25,\n",
      "          1.2908e-25,  1.0793e-25],\n",
      "        ...,\n",
      "        [ 4.9615e-25, -4.9478e-25, -4.8790e-25,  ..., -1.0554e-25,\n",
      "          1.2908e-25,  1.0793e-25],\n",
      "        [ 4.9613e-25, -4.9477e-25, -4.8789e-25,  ..., -1.0554e-25,\n",
      "          1.2908e-25,  1.0793e-25],\n",
      "        [-4.9615e-25,  4.9478e-25,  4.8790e-25,  ...,  1.0554e-25,\n",
      "         -1.2908e-25, -1.0793e-25]])\n",
      "batch:  ['yyyyyyyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977,\n",
      "         0.4977, 0.4977, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yyyy']\n",
      "per_prediction_loss:  tensor([191.5334], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-5.5037e-21,  5.5009e-21,  5.4782e-21,  ...,  1.1794e-21,\n",
      "         -1.3365e-21, -1.1071e-21],\n",
      "        [ 5.5037e-21, -5.5010e-21, -5.4782e-21,  ..., -1.1794e-21,\n",
      "          1.3365e-21,  1.1071e-21],\n",
      "        [ 5.5037e-21, -5.5010e-21, -5.4782e-21,  ..., -1.1794e-21,\n",
      "          1.3365e-21,  1.1071e-21],\n",
      "        ...,\n",
      "        [ 5.5037e-21, -5.5010e-21, -5.4782e-21,  ..., -1.1794e-21,\n",
      "          1.3365e-21,  1.1071e-21],\n",
      "        [ 5.5034e-21, -5.5007e-21, -5.4780e-21,  ..., -1.1794e-21,\n",
      "          1.3364e-21,  1.1070e-21],\n",
      "        [-5.5036e-21,  5.5009e-21,  5.4782e-21,  ...,  1.1794e-21,\n",
      "         -1.3365e-21, -1.1071e-21]])\n",
      "Epoch 0 iteration 36: loss = -43.023, tp = 27.12 lines/s, ETA 00h00m20s\n",
      "batch:  ['xxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xx']\n",
      "per_prediction_loss:  tensor([97.1623], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "batch:  ['nnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nn']\n",
      "per_prediction_loss:  tensor([107.6307], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 0 iteration 38: loss = -26.592, tp = 27.47 lines/s, ETA 00h00m20s\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuu']\n",
      "per_prediction_loss:  tensor([163.0527], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-9.4549e-21,  9.4514e-21,  9.4190e-21,  ...,  1.8795e-21,\n",
      "         -2.2695e-21, -1.8710e-21],\n",
      "        [ 9.4549e-21, -9.4514e-21, -9.4190e-21,  ..., -1.8795e-21,\n",
      "          2.2695e-21,  1.8710e-21],\n",
      "        [ 9.4549e-21, -9.4514e-21, -9.4190e-21,  ..., -1.8795e-21,\n",
      "          2.2695e-21,  1.8710e-21],\n",
      "        ...,\n",
      "        [ 9.4549e-21, -9.4514e-21, -9.4190e-21,  ..., -1.8795e-21,\n",
      "          2.2695e-21,  1.8710e-21],\n",
      "        [ 9.4546e-21, -9.4511e-21, -9.4187e-21,  ..., -1.8795e-21,\n",
      "          2.2694e-21,  1.8709e-21],\n",
      "        [-9.4548e-21,  9.4513e-21,  9.4190e-21,  ...,  1.8795e-21,\n",
      "         -2.2695e-21, -1.8709e-21]])\n",
      "batch:  ['hhhhh']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['h']\n",
      "per_prediction_loss:  tensor([53.3507], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 0 iteration 40: loss = -14.790, tp = 27.03 lines/s, ETA 00h00m20s\n",
      "batch:  ['pppppppppp']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ppppp']\n",
      "per_prediction_loss:  tensor([127.1544], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-2.7668e-21,  2.7665e-21,  2.7599e-21,  ...,  5.6998e-22,\n",
      "         -6.8523e-22, -5.6964e-22],\n",
      "        [ 2.7668e-21, -2.7665e-21, -2.7599e-21,  ..., -5.6998e-22,\n",
      "          6.8523e-22,  5.6964e-22],\n",
      "        [ 2.7668e-21, -2.7665e-21, -2.7599e-21,  ..., -5.6998e-22,\n",
      "          6.8523e-22,  5.6964e-22],\n",
      "        ...,\n",
      "        [ 2.7668e-21, -2.7665e-21, -2.7599e-21,  ..., -5.6997e-22,\n",
      "          6.8523e-22,  5.6963e-22],\n",
      "        [ 2.7667e-21, -2.7664e-21, -2.7598e-21,  ..., -5.6995e-22,\n",
      "          6.8520e-22,  5.6961e-22],\n",
      "        [-2.7668e-21,  2.7665e-21,  2.7599e-21,  ...,  5.6997e-22,\n",
      "         -6.8523e-22, -5.6963e-22]])\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977,\n",
      "         0.4977, 0.4977, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dddd']\n",
      "per_prediction_loss:  tensor([242.0430], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-2.2514e-23,  2.2508e-23,  2.2440e-23,  ...,  5.1434e-24,\n",
      "         -6.0484e-24, -5.0506e-24],\n",
      "        [ 2.2514e-23, -2.2508e-23, -2.2440e-23,  ..., -5.1434e-24,\n",
      "          6.0484e-24,  5.0506e-24],\n",
      "        [ 2.2514e-23, -2.2508e-23, -2.2440e-23,  ..., -5.1434e-24,\n",
      "          6.0484e-24,  5.0506e-24],\n",
      "        ...,\n",
      "        [ 2.2514e-23, -2.2508e-23, -2.2440e-23,  ..., -5.1434e-24,\n",
      "          6.0484e-24,  5.0506e-24],\n",
      "        [ 2.2513e-23, -2.2508e-23, -2.2439e-23,  ..., -5.1433e-24,\n",
      "          6.0482e-24,  5.0505e-24],\n",
      "        [-2.2514e-23,  2.2508e-23,  2.2440e-23,  ...,  5.1434e-24,\n",
      "         -6.0483e-24, -5.0506e-24]])\n",
      "Epoch 0 iteration 42: loss = -62.795, tp = 26.84 lines/s, ETA 00h00m20s\n",
      "batch:  ['qqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['qqq']\n",
      "per_prediction_loss:  tensor([86.9616], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[-6.6879e-26,  6.6780e-26,  6.5986e-26,  ...,  1.4828e-26,\n",
      "         -1.7331e-26, -1.4515e-26],\n",
      "        [ 6.6879e-26, -6.6780e-26, -6.5986e-26,  ..., -1.4828e-26,\n",
      "          1.7331e-26,  1.4515e-26],\n",
      "        [ 6.6879e-26, -6.6780e-26, -6.5986e-26,  ..., -1.4828e-26,\n",
      "          1.7331e-26,  1.4515e-26],\n",
      "        ...,\n",
      "        [ 6.6879e-26, -6.6780e-26, -6.5986e-26,  ..., -1.4828e-26,\n",
      "          1.7331e-26,  1.4515e-26],\n",
      "        [ 6.6878e-26, -6.6779e-26, -6.5985e-26,  ..., -1.4827e-26,\n",
      "          1.7331e-26,  1.4515e-26],\n",
      "        [-6.6879e-26,  6.6780e-26,  6.5986e-26,  ...,  1.4828e-26,\n",
      "         -1.7331e-26, -1.4515e-26]])\n",
      "batch:  ['ttttt']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['tt']\n",
      "per_prediction_loss:  tensor([57.1271], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 0 iteration 44: loss = -17.033, tp = 26.91 lines/s, ETA 00h00m20s\n",
      "batch:  ['zzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zzzz']\n",
      "per_prediction_loss:  tensor([58.1894], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-3.0183e-24,  3.0175e-24,  3.0079e-24,  ...,  6.4125e-25,\n",
      "         -7.6164e-25, -6.4668e-25],\n",
      "        [ 3.0183e-24, -3.0176e-24, -3.0080e-24,  ..., -6.4126e-25,\n",
      "          7.6165e-25,  6.4669e-25],\n",
      "        [ 3.0183e-24, -3.0175e-24, -3.0079e-24,  ..., -6.4125e-25,\n",
      "          7.6164e-25,  6.4668e-25],\n",
      "        ...,\n",
      "        [ 3.0183e-24, -3.0175e-24, -3.0079e-24,  ..., -6.4125e-25,\n",
      "          7.6164e-25,  6.4668e-25],\n",
      "        [ 3.0183e-24, -3.0175e-24, -3.0079e-24,  ..., -6.4125e-25,\n",
      "          7.6164e-25,  6.4668e-25],\n",
      "        [-3.0183e-24,  3.0175e-24,  3.0079e-24,  ...,  6.4125e-25,\n",
      "         -7.6164e-25, -6.4668e-25]])\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jj']\n",
      "per_prediction_loss:  tensor([119.8304], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 0 iteration 46: loss = -34.977, tp = 27.18 lines/s, ETA 00h00m20s\n",
      "batch:  ['oooooooooo']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975,\n",
      "         0.4975, 0.4975, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ooooo']\n",
      "per_prediction_loss:  tensor([86.4390], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-6.6263e-23,  6.6258e-23,  6.6117e-23,  ...,  1.4420e-23,\n",
      "         -1.6994e-23, -1.4003e-23],\n",
      "        [ 6.6263e-23, -6.6258e-23, -6.6117e-23,  ..., -1.4420e-23,\n",
      "          1.6994e-23,  1.4003e-23],\n",
      "        [ 6.6262e-23, -6.6258e-23, -6.6117e-23,  ..., -1.4420e-23,\n",
      "          1.6994e-23,  1.4003e-23],\n",
      "        ...,\n",
      "        [ 6.6262e-23, -6.6258e-23, -6.6117e-23,  ..., -1.4420e-23,\n",
      "          1.6994e-23,  1.4003e-23],\n",
      "        [ 6.6261e-23, -6.6256e-23, -6.6115e-23,  ..., -1.4420e-23,\n",
      "          1.6994e-23,  1.4003e-23],\n",
      "        [-6.6262e-23,  6.6258e-23,  6.6117e-23,  ...,  1.4420e-23,\n",
      "         -1.6994e-23, -1.4003e-23]])\n",
      "batch:  ['cccccccccc']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ccccc']\n",
      "per_prediction_loss:  tensor([256.0384], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-2.6570e-23,  2.6569e-23,  2.6518e-23,  ...,  5.9760e-24,\n",
      "         -7.0781e-24, -5.9718e-24],\n",
      "        [ 2.6570e-23, -2.6569e-23, -2.6518e-23,  ..., -5.9760e-24,\n",
      "          7.0781e-24,  5.9718e-24],\n",
      "        [ 2.6570e-23, -2.6569e-23, -2.6518e-23,  ..., -5.9760e-24,\n",
      "          7.0781e-24,  5.9718e-24],\n",
      "        ...,\n",
      "        [ 2.6570e-23, -2.6569e-23, -2.6518e-23,  ..., -5.9760e-24,\n",
      "          7.0781e-24,  5.9718e-24],\n",
      "        [ 2.6569e-23, -2.6568e-23, -2.6517e-23,  ..., -5.9759e-24,\n",
      "          7.0779e-24,  5.9716e-24],\n",
      "        [-2.6570e-23,  2.6569e-23,  2.6517e-23,  ...,  5.9760e-24,\n",
      "         -7.0781e-24, -5.9718e-24]])\n",
      "Epoch 0 iteration 48: loss = -75.303, tp = 27.11 lines/s, ETA 00h00m20s\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977,\n",
      "         0.4977, 0.4977, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddd']\n",
      "per_prediction_loss:  tensor([283.8948], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-3.3632e-27,  3.3596e-27,  3.3308e-27,  ...,  8.2106e-28,\n",
      "         -9.6109e-28, -8.0621e-28],\n",
      "        [ 3.3632e-27, -3.3596e-27, -3.3308e-27,  ..., -8.2106e-28,\n",
      "          9.6109e-28,  8.0621e-28],\n",
      "        [ 3.3632e-27, -3.3596e-27, -3.3308e-27,  ..., -8.2106e-28,\n",
      "          9.6109e-28,  8.0621e-28],\n",
      "        ...,\n",
      "        [ 3.3632e-27, -3.3596e-27, -3.3308e-27,  ..., -8.2106e-28,\n",
      "          9.6109e-28,  8.0621e-28],\n",
      "        [ 3.3631e-27, -3.3595e-27, -3.3307e-27,  ..., -8.2104e-28,\n",
      "          9.6107e-28,  8.0619e-28],\n",
      "        [-3.3632e-27,  3.3596e-27,  3.3308e-27,  ...,  8.2106e-28,\n",
      "         -9.6109e-28, -8.0621e-28]])\n",
      "batch:  ['bbbbb']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['bb']\n",
      "per_prediction_loss:  tensor([138.5107], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 0 iteration 50: loss = -43.191, tp = 26.92 lines/s, ETA 00h00m20s\n",
      "batch:  ['rrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['r']\n",
      "per_prediction_loss:  tensor([155.8537], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "batch:  ['eeeee']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['e']\n",
      "per_prediction_loss:  tensor([153.7648], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 0 iteration 52: loss = -48.533, tp = 27.15 lines/s, ETA 00h00m20s\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuuuu']\n",
      "per_prediction_loss:  tensor([248.2440], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-2.1151e-22,  2.1150e-22,  2.1113e-22,  ...,  4.5455e-23,\n",
      "         -5.4322e-23, -4.5187e-23],\n",
      "        [ 2.1151e-22, -2.1150e-22, -2.1113e-22,  ..., -4.5455e-23,\n",
      "          5.4322e-23,  4.5187e-23],\n",
      "        [ 2.1151e-22, -2.1150e-22, -2.1113e-22,  ..., -4.5455e-23,\n",
      "          5.4321e-23,  4.5187e-23],\n",
      "        ...,\n",
      "        [ 2.1151e-22, -2.1150e-22, -2.1113e-22,  ..., -4.5455e-23,\n",
      "          5.4321e-23,  4.5187e-23],\n",
      "        [ 2.1150e-22, -2.1150e-22, -2.1113e-22,  ..., -4.5454e-23,\n",
      "          5.4320e-23,  4.5186e-23],\n",
      "        [-2.1151e-22,  2.1150e-22,  2.1113e-22,  ...,  4.5455e-23,\n",
      "         -5.4321e-23, -4.5187e-23]])\n",
      "batch:  ['yyyyyyyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977,\n",
      "         0.4977, 0.4977, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yyyyy']\n",
      "per_prediction_loss:  tensor([301.4966], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-5.0374e-24,  5.0370e-24,  5.0275e-24,  ...,  1.2067e-24,\n",
      "         -1.3570e-24, -1.1383e-24],\n",
      "        [ 5.0374e-24, -5.0370e-24, -5.0275e-24,  ..., -1.2067e-24,\n",
      "          1.3570e-24,  1.1383e-24],\n",
      "        [ 5.0374e-24, -5.0370e-24, -5.0275e-24,  ..., -1.2067e-24,\n",
      "          1.3570e-24,  1.1383e-24],\n",
      "        ...,\n",
      "        [ 5.0374e-24, -5.0370e-24, -5.0275e-24,  ..., -1.2067e-24,\n",
      "          1.3570e-24,  1.1383e-24],\n",
      "        [ 5.0373e-24, -5.0369e-24, -5.0274e-24,  ..., -1.2067e-24,\n",
      "          1.3570e-24,  1.1383e-24],\n",
      "        [-5.0374e-24,  5.0370e-24,  5.0275e-24,  ...,  1.2067e-24,\n",
      "         -1.3570e-24, -1.1383e-24]])\n",
      "Epoch 0 iteration 54: loss = -99.461, tp = 27.04 lines/s, ETA 00h00m20s\n",
      "batch:  ['aaaaaaaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaaaaa']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_prediction_loss:  tensor([263.9647], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-4.0370e-22,  4.0369e-22,  4.0300e-22,  ...,  8.7497e-23,\n",
      "         -1.0304e-22, -8.6303e-23],\n",
      "        [ 4.0370e-22, -4.0369e-22, -4.0300e-22,  ..., -8.7497e-23,\n",
      "          1.0304e-22,  8.6303e-23],\n",
      "        [ 4.0370e-22, -4.0369e-22, -4.0300e-22,  ..., -8.7497e-23,\n",
      "          1.0304e-22,  8.6302e-23],\n",
      "        ...,\n",
      "        [ 4.0370e-22, -4.0369e-22, -4.0300e-22,  ..., -8.7497e-23,\n",
      "          1.0304e-22,  8.6302e-23],\n",
      "        [ 4.0369e-22, -4.0368e-22, -4.0299e-22,  ..., -8.7495e-23,\n",
      "          1.0303e-22,  8.6301e-23],\n",
      "        [-4.0370e-22,  4.0369e-22,  4.0299e-22,  ...,  8.7497e-23,\n",
      "         -1.0304e-22, -8.6302e-23]])\n",
      "batch:  ['xxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xxxx']\n",
      "per_prediction_loss:  tensor([158.0676], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-2.6048e-24,  2.6043e-24,  2.5976e-24,  ...,  6.1154e-25,\n",
      "         -7.2747e-25, -5.5851e-25],\n",
      "        [ 2.6048e-24, -2.6043e-24, -2.5976e-24,  ..., -6.1154e-25,\n",
      "          7.2747e-25,  5.5851e-25],\n",
      "        [ 2.6048e-24, -2.6043e-24, -2.5976e-24,  ..., -6.1154e-25,\n",
      "          7.2747e-25,  5.5851e-25],\n",
      "        ...,\n",
      "        [ 2.6048e-24, -2.6043e-24, -2.5976e-24,  ..., -6.1154e-25,\n",
      "          7.2747e-25,  5.5851e-25],\n",
      "        [ 2.6048e-24, -2.6043e-24, -2.5975e-24,  ..., -6.1153e-25,\n",
      "          7.2745e-25,  5.5850e-25],\n",
      "        [-2.6048e-24,  2.6043e-24,  2.5976e-24,  ...,  6.1154e-25,\n",
      "         -7.2747e-25, -5.5851e-25]])\n",
      "Epoch 0 iteration 56: loss = -56.190, tp = 27.04 lines/s, ETA 00h00m20s\n",
      "batch:  ['cccccccccc']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['c']\n",
      "per_prediction_loss:  tensor([315.3569], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "batch:  ['ppppp']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['p']\n",
      "per_prediction_loss:  tensor([144.5035], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 0 iteration 58: loss = -51.876, tp = 27.17 lines/s, ETA 00h00m19s\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977,\n",
      "         0.4977, 0.4977, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddddd']\n",
      "per_prediction_loss:  tensor([368.2910], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-5.2629e-25,  5.2626e-25,  5.2534e-25,  ...,  1.2779e-25,\n",
      "         -1.4914e-25, -1.2547e-25],\n",
      "        [ 5.2629e-25, -5.2626e-25, -5.2534e-25,  ..., -1.2779e-25,\n",
      "          1.4914e-25,  1.2547e-25],\n",
      "        [ 5.2629e-25, -5.2626e-25, -5.2534e-25,  ..., -1.2779e-25,\n",
      "          1.4914e-25,  1.2547e-25],\n",
      "        ...,\n",
      "        [ 5.2629e-25, -5.2626e-25, -5.2534e-25,  ..., -1.2779e-25,\n",
      "          1.4914e-25,  1.2547e-25],\n",
      "        [ 5.2628e-25, -5.2625e-25, -5.2533e-25,  ..., -1.2778e-25,\n",
      "          1.4914e-25,  1.2547e-25],\n",
      "        [-5.2629e-25,  5.2626e-25,  5.2534e-25,  ...,  1.2779e-25,\n",
      "         -1.4914e-25, -1.2547e-25]])\n",
      "batch:  ['aaaaaaaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaaaaaaa']\n",
      "per_prediction_loss:  tensor([297.0992], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-2.2960e-22,  2.2959e-22,  2.2922e-22,  ...,  5.0206e-23,\n",
      "         -5.9064e-23, -4.9514e-23],\n",
      "        [ 2.2960e-22, -2.2959e-22, -2.2922e-22,  ..., -5.0206e-23,\n",
      "          5.9064e-23,  4.9514e-23],\n",
      "        [ 2.2960e-22, -2.2959e-22, -2.2922e-22,  ..., -5.0206e-23,\n",
      "          5.9064e-23,  4.9514e-23],\n",
      "        ...,\n",
      "        [ 2.2960e-22, -2.2959e-22, -2.2922e-22,  ..., -5.0206e-23,\n",
      "          5.9064e-23,  4.9514e-23],\n",
      "        [ 2.2959e-22, -2.2959e-22, -2.2921e-22,  ..., -5.0205e-23,\n",
      "          5.9063e-23,  4.9513e-23],\n",
      "        [-2.2960e-22,  2.2959e-22,  2.2922e-22,  ...,  5.0206e-23,\n",
      "         -5.9064e-23, -4.9514e-23]])\n",
      "Epoch 0 iteration 60: loss = -113.015, tp = 27.06 lines/s, ETA 00h00m19s\n",
      "batch:  ['eeeeeeeeee']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eeeeee']\n",
      "per_prediction_loss:  tensor([325.6517], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-2.2212e-23,  2.2211e-23,  2.2175e-23,  ...,  5.0171e-24,\n",
      "         -5.8709e-24, -5.0334e-24],\n",
      "        [ 2.2212e-23, -2.2211e-23, -2.2175e-23,  ..., -5.0171e-24,\n",
      "          5.8708e-24,  5.0334e-24],\n",
      "        [ 2.2212e-23, -2.2211e-23, -2.2175e-23,  ..., -5.0171e-24,\n",
      "          5.8708e-24,  5.0334e-24],\n",
      "        ...,\n",
      "        [ 2.2212e-23, -2.2211e-23, -2.2175e-23,  ..., -5.0170e-24,\n",
      "          5.8708e-24,  5.0334e-24],\n",
      "        [ 2.2212e-23, -2.2211e-23, -2.2175e-23,  ..., -5.0170e-24,\n",
      "          5.8707e-24,  5.0333e-24],\n",
      "        [-2.2212e-23,  2.2211e-23,  2.2175e-23,  ...,  5.0170e-24,\n",
      "         -5.8708e-24, -5.0334e-24]])\n",
      "batch:  ['cccccccccc']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['cccc']\n",
      "per_prediction_loss:  tensor([361.8035], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-1.7083e-24,  1.7081e-24,  1.7041e-24,  ...,  3.9877e-25,\n",
      "         -4.7041e-25, -3.9820e-25],\n",
      "        [ 1.7083e-24, -1.7081e-24, -1.7041e-24,  ..., -3.9877e-25,\n",
      "          4.7041e-25,  3.9820e-25],\n",
      "        [ 1.7083e-24, -1.7081e-24, -1.7041e-24,  ..., -3.9877e-25,\n",
      "          4.7041e-25,  3.9820e-25],\n",
      "        ...,\n",
      "        [ 1.7083e-24, -1.7081e-24, -1.7041e-24,  ..., -3.9877e-25,\n",
      "          4.7041e-25,  3.9820e-25],\n",
      "        [ 1.7083e-24, -1.7081e-24, -1.7040e-24,  ..., -3.9876e-25,\n",
      "          4.7040e-25,  3.9820e-25],\n",
      "        [-1.7083e-24,  1.7081e-24,  1.7041e-24,  ...,  3.9877e-25,\n",
      "         -4.7041e-25, -3.9820e-25]])\n",
      "Epoch 0 iteration 62: loss = -139.449, tp = 26.72 lines/s, ETA 00h00m20s\n",
      "batch:  ['jjjjjjjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975,\n",
      "         0.4975, 0.4975, 0.4975]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjj']\n",
      "per_prediction_loss:  tensor([313.2457], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-3.2643e-24,  3.2635e-24,  3.2558e-24,  ...,  7.5030e-25,\n",
      "         -8.9848e-25, -7.5308e-25],\n",
      "        [ 3.2643e-24, -3.2635e-24, -3.2558e-24,  ..., -7.5030e-25,\n",
      "          8.9847e-25,  7.5308e-25],\n",
      "        [ 3.2643e-24, -3.2635e-24, -3.2558e-24,  ..., -7.5030e-25,\n",
      "          8.9847e-25,  7.5308e-25],\n",
      "        ...,\n",
      "        [ 3.2643e-24, -3.2635e-24, -3.2558e-24,  ..., -7.5030e-25,\n",
      "          8.9847e-25,  7.5308e-25],\n",
      "        [ 3.2642e-24, -3.2635e-24, -3.2557e-24,  ..., -7.5028e-25,\n",
      "          8.9845e-25,  7.5306e-25],\n",
      "        [-3.2643e-24,  3.2635e-24,  3.2558e-24,  ...,  7.5029e-25,\n",
      "         -8.9847e-25, -7.5308e-25]])\n",
      "batch:  ['vvvvv']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['vvv']\n",
      "per_prediction_loss:  tensor([199.6674], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-2.8235e-27,  2.8192e-27,  2.7967e-27,  ...,  6.8321e-28,\n",
      "         -8.0683e-28, -6.8058e-28],\n",
      "        [ 2.8235e-27, -2.8192e-27, -2.7967e-27,  ..., -6.8321e-28,\n",
      "          8.0683e-28,  6.8058e-28],\n",
      "        [ 2.8235e-27, -2.8192e-27, -2.7967e-27,  ..., -6.8321e-28,\n",
      "          8.0683e-28,  6.8058e-28],\n",
      "        ...,\n",
      "        [ 2.8235e-27, -2.8192e-27, -2.7967e-27,  ..., -6.8321e-28,\n",
      "          8.0683e-28,  6.8058e-28],\n",
      "        [ 2.8235e-27, -2.8192e-27, -2.7966e-27,  ..., -6.8320e-28,\n",
      "          8.0682e-28,  6.8057e-28],\n",
      "        [-2.8235e-27,  2.8192e-27,  2.7967e-27,  ...,  6.8321e-28,\n",
      "         -8.0683e-28, -6.8058e-28]])\n",
      "Epoch 0 iteration 64: loss = -81.705, tp = 26.82 lines/s, ETA 00h00m19s\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuuu']\n",
      "per_prediction_loss:  tensor([333.6073], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[-6.7180e-23,  6.7176e-23,  6.7062e-23,  ...,  1.4734e-23,\n",
      "         -1.7564e-23, -1.4642e-23],\n",
      "        [ 6.7180e-23, -6.7176e-23, -6.7062e-23,  ..., -1.4734e-23,\n",
      "          1.7564e-23,  1.4642e-23],\n",
      "        [ 6.7180e-23, -6.7176e-23, -6.7062e-23,  ..., -1.4734e-23,\n",
      "          1.7564e-23,  1.4642e-23],\n",
      "        ...,\n",
      "        [ 6.7180e-23, -6.7176e-23, -6.7062e-23,  ..., -1.4734e-23,\n",
      "          1.7564e-23,  1.4642e-23],\n",
      "        [ 6.7178e-23, -6.7175e-23, -6.7060e-23,  ..., -1.4733e-23,\n",
      "          1.7564e-23,  1.4642e-23],\n",
      "        [-6.7179e-23,  6.7176e-23,  6.7062e-23,  ...,  1.4733e-23,\n",
      "         -1.7564e-23, -1.4642e-23]])\n",
      "batch:  ['eeeee']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eeee']\n",
      "per_prediction_loss:  tensor([216.7759], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-5.7214e-24,  5.7201e-24,  5.7069e-24,  ...,  1.3035e-24,\n",
      "         -1.5244e-24, -1.3075e-24],\n",
      "        [ 5.7214e-24, -5.7200e-24, -5.7069e-24,  ..., -1.3035e-24,\n",
      "          1.5244e-24,  1.3075e-24],\n",
      "        [ 5.7214e-24, -5.7201e-24, -5.7069e-24,  ..., -1.3035e-24,\n",
      "          1.5244e-24,  1.3075e-24],\n",
      "        ...,\n",
      "        [ 5.7214e-24, -5.7200e-24, -5.7069e-24,  ..., -1.3035e-24,\n",
      "          1.5243e-24,  1.3075e-24],\n",
      "        [ 5.7213e-24, -5.7200e-24, -5.7068e-24,  ..., -1.3034e-24,\n",
      "          1.5243e-24,  1.3075e-24],\n",
      "        [-5.7213e-24,  5.7200e-24,  5.7069e-24,  ...,  1.3034e-24,\n",
      "         -1.5243e-24, -1.3075e-24]])\n",
      "Epoch 0 iteration 66: loss = -91.655, tp = 26.81 lines/s, ETA 00h00m19s\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmm']\n",
      "per_prediction_loss:  tensor([104.1615], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-2.4455e-24,  2.4450e-24,  2.4389e-24,  ...,  5.5599e-25,\n",
      "         -6.5312e-25, -5.4640e-25],\n",
      "        [ 2.4455e-24, -2.4449e-24, -2.4389e-24,  ..., -5.5599e-25,\n",
      "          6.5311e-25,  5.4640e-25],\n",
      "        [ 2.4455e-24, -2.4449e-24, -2.4389e-24,  ..., -5.5599e-25,\n",
      "          6.5311e-25,  5.4640e-25],\n",
      "        ...,\n",
      "        [ 2.4455e-24, -2.4449e-24, -2.4389e-24,  ..., -5.5599e-25,\n",
      "          6.5311e-25,  5.4640e-25],\n",
      "        [ 2.4454e-24, -2.4449e-24, -2.4389e-24,  ..., -5.5598e-25,\n",
      "          6.5310e-25,  5.4639e-25],\n",
      "        [-2.4455e-24,  2.4449e-24,  2.4389e-24,  ...,  5.5599e-25,\n",
      "         -6.5311e-25, -5.4639e-25]])\n",
      "batch:  ['cccccccccc']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['cccc']\n",
      "per_prediction_loss:  tensor([410.4682], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-1.5978e-24,  1.5975e-24,  1.5938e-24,  ...,  3.7488e-25,\n",
      "         -4.4195e-25, -3.7431e-25],\n",
      "        [ 1.5978e-24, -1.5975e-24, -1.5938e-24,  ..., -3.7488e-25,\n",
      "          4.4195e-25,  3.7431e-25],\n",
      "        [ 1.5978e-24, -1.5975e-24, -1.5938e-24,  ..., -3.7488e-25,\n",
      "          4.4195e-25,  3.7431e-25],\n",
      "        ...,\n",
      "        [ 1.5978e-24, -1.5975e-24, -1.5938e-24,  ..., -3.7488e-25,\n",
      "          4.4195e-25,  3.7431e-25],\n",
      "        [ 1.5977e-24, -1.5975e-24, -1.5938e-24,  ..., -3.7488e-25,\n",
      "          4.4195e-25,  3.7430e-25],\n",
      "        [-1.5978e-24,  1.5975e-24,  1.5938e-24,  ...,  3.7488e-25,\n",
      "         -4.4195e-25, -3.7431e-25]])\n",
      "Epoch 0 iteration 68: loss = -174.375, tp = 26.78 lines/s, ETA 00h00m19s\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iii']\n",
      "per_prediction_loss:  tensor([105.9546], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-5.3035e-27,  5.2980e-27,  5.2529e-27,  ...,  1.2489e-27,\n",
      "         -1.4644e-27, -1.2246e-27],\n",
      "        [ 5.3035e-27, -5.2980e-27, -5.2529e-27,  ..., -1.2489e-27,\n",
      "          1.4644e-27,  1.2246e-27],\n",
      "        [ 5.3034e-27, -5.2980e-27, -5.2528e-27,  ..., -1.2489e-27,\n",
      "          1.4644e-27,  1.2246e-27],\n",
      "        ...,\n",
      "        [ 5.3034e-27, -5.2980e-27, -5.2528e-27,  ..., -1.2489e-27,\n",
      "          1.4644e-27,  1.2246e-27],\n",
      "        [ 5.3034e-27, -5.2979e-27, -5.2528e-27,  ..., -1.2489e-27,\n",
      "          1.4643e-27,  1.2246e-27],\n",
      "        [-5.3034e-27,  5.2980e-27,  5.2528e-27,  ...,  1.2489e-27,\n",
      "         -1.4644e-27, -1.2246e-27]])\n",
      "batch:  ['qqqqqqqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['qqqqq']\n",
      "per_prediction_loss:  tensor([307.5430], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-2.6471e-23,  2.6470e-23,  2.6423e-23,  ...,  6.0318e-24,\n",
      "         -7.0010e-24, -5.9042e-24],\n",
      "        [ 2.6471e-23, -2.6470e-23, -2.6423e-23,  ..., -6.0317e-24,\n",
      "          7.0010e-24,  5.9041e-24],\n",
      "        [ 2.6471e-23, -2.6470e-23, -2.6423e-23,  ..., -6.0317e-24,\n",
      "          7.0009e-24,  5.9041e-24],\n",
      "        ...,\n",
      "        [ 2.6471e-23, -2.6470e-23, -2.6423e-23,  ..., -6.0317e-24,\n",
      "          7.0009e-24,  5.9041e-24],\n",
      "        [ 2.6471e-23, -2.6470e-23, -2.6423e-23,  ..., -6.0316e-24,\n",
      "          7.0008e-24,  5.9040e-24],\n",
      "        [-2.6471e-23,  2.6470e-23,  2.6423e-23,  ...,  6.0317e-24,\n",
      "         -7.0009e-24, -5.9041e-24]])\n",
      "Epoch 0 iteration 70: loss = -138.086, tp = 26.75 lines/s, ETA 00h00m19s\n",
      "batch:  ['vvvvv']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['vv']\n",
      "per_prediction_loss:  tensor([225.6367], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "batch:  ['yyyyyyyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977,\n",
      "         0.4977, 0.4977, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yyyyy']\n",
      "per_prediction_loss:  tensor([427.3340], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-1.9611e-24,  1.9610e-24,  1.9576e-24,  ...,  4.7941e-25,\n",
      "         -5.3839e-25, -4.5263e-25],\n",
      "        [ 1.9611e-24, -1.9610e-24, -1.9576e-24,  ..., -4.7941e-25,\n",
      "          5.3839e-25,  4.5263e-25],\n",
      "        [ 1.9611e-24, -1.9610e-24, -1.9576e-24,  ..., -4.7941e-25,\n",
      "          5.3839e-25,  4.5263e-25],\n",
      "        ...,\n",
      "        [ 1.9611e-24, -1.9610e-24, -1.9576e-24,  ..., -4.7941e-25,\n",
      "          5.3839e-25,  4.5263e-25],\n",
      "        [ 1.9611e-24, -1.9609e-24, -1.9575e-24,  ..., -4.7940e-25,\n",
      "          5.3838e-25,  4.5262e-25],\n",
      "        [-1.9611e-24,  1.9610e-24,  1.9576e-24,  ...,  4.7941e-25,\n",
      "         -5.3839e-25, -4.5263e-25]])\n",
      "Epoch 0 iteration 72: loss = -196.722, tp = 26.54 lines/s, ETA 00h00m19s\n",
      "batch:  ['lllll']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['lll']\n",
      "per_prediction_loss:  tensor([204.1260], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-2.3424e-26,  2.3394e-26,  2.3207e-26,  ...,  5.4801e-27,\n",
      "         -6.4238e-27, -5.3463e-27],\n",
      "        [ 2.3424e-26, -2.3394e-26, -2.3207e-26,  ..., -5.4801e-27,\n",
      "          6.4238e-27,  5.3463e-27],\n",
      "        [ 2.3424e-26, -2.3394e-26, -2.3207e-26,  ..., -5.4801e-27,\n",
      "          6.4237e-27,  5.3463e-27],\n",
      "        ...,\n",
      "        [ 2.3424e-26, -2.3394e-26, -2.3207e-26,  ..., -5.4801e-27,\n",
      "          6.4237e-27,  5.3463e-27],\n",
      "        [ 2.3423e-26, -2.3394e-26, -2.3207e-26,  ..., -5.4800e-27,\n",
      "          6.4237e-27,  5.3462e-27],\n",
      "        [-2.3424e-26,  2.3394e-26,  2.3207e-26,  ...,  5.4801e-27,\n",
      "         -6.4237e-27, -5.3463e-27]])\n",
      "batch:  ['vvvvv']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['vvvv']\n",
      "per_prediction_loss:  tensor([244.8369], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-7.8469e-25,  7.8450e-25,  7.8268e-25,  ...,  1.8613e-25,\n",
      "         -2.1964e-25, -1.8539e-25],\n",
      "        [ 7.8469e-25, -7.8450e-25, -7.8268e-25,  ..., -1.8613e-25,\n",
      "          2.1964e-25,  1.8539e-25],\n",
      "        [ 7.8469e-25, -7.8450e-25, -7.8268e-25,  ..., -1.8613e-25,\n",
      "          2.1964e-25,  1.8539e-25],\n",
      "        ...,\n",
      "        [ 7.8469e-25, -7.8450e-25, -7.8268e-25,  ..., -1.8613e-25,\n",
      "          2.1964e-25,  1.8539e-25],\n",
      "        [ 7.8468e-25, -7.8448e-25, -7.8266e-25,  ..., -1.8613e-25,\n",
      "          2.1963e-25,  1.8538e-25],\n",
      "        [-7.8469e-25,  7.8449e-25,  7.8268e-25,  ...,  1.8613e-25,\n",
      "         -2.1964e-25, -1.8539e-25]])\n",
      "Epoch 0 iteration 74: loss = -116.169, tp = 26.67 lines/s, ETA 00h00m19s\n",
      "batch:  ['eeeee']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ee']\n",
      "per_prediction_loss:  tensor([244.3857], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "batch:  ['jjjjjjjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975,\n",
      "         0.4975, 0.4975, 0.4975]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjjjjj']\n",
      "per_prediction_loss:  tensor([405.0050], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[-6.0383e-24,  6.0383e-24,  6.0285e-24,  ...,  1.3936e-24,\n",
      "         -1.6672e-24, -1.3986e-24],\n",
      "        [ 6.0383e-24, -6.0383e-24, -6.0285e-24,  ..., -1.3936e-24,\n",
      "          1.6672e-24,  1.3986e-24],\n",
      "        [ 6.0383e-24, -6.0383e-24, -6.0285e-24,  ..., -1.3936e-24,\n",
      "          1.6672e-24,  1.3986e-24],\n",
      "        ...,\n",
      "        [ 6.0383e-24, -6.0383e-24, -6.0285e-24,  ..., -1.3936e-24,\n",
      "          1.6672e-24,  1.3986e-24],\n",
      "        [ 6.0382e-24, -6.0382e-24, -6.0283e-24,  ..., -1.3935e-24,\n",
      "          1.6671e-24,  1.3986e-24],\n",
      "        [-6.0383e-24,  6.0383e-24,  6.0285e-24,  ...,  1.3936e-24,\n",
      "         -1.6672e-24, -1.3986e-24]])\n",
      "Epoch 0 iteration 76: loss = -198.443, tp = 26.74 lines/s, ETA 00h00m19s\n",
      "batch:  ['zzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zzzz']\n",
      "per_prediction_loss:  tensor([165.8772], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-5.1286e-24,  5.1277e-24,  5.1148e-24,  ...,  1.1554e-24,\n",
      "         -1.3629e-24, -1.1629e-24],\n",
      "        [ 5.1286e-24, -5.1277e-24, -5.1148e-24,  ..., -1.1554e-24,\n",
      "          1.3629e-24,  1.1629e-24],\n",
      "        [ 5.1286e-24, -5.1276e-24, -5.1148e-24,  ..., -1.1554e-24,\n",
      "          1.3629e-24,  1.1629e-24],\n",
      "        ...,\n",
      "        [ 5.1286e-24, -5.1276e-24, -5.1148e-24,  ..., -1.1554e-24,\n",
      "          1.3629e-24,  1.1629e-24],\n",
      "        [ 5.1285e-24, -5.1275e-24, -5.1147e-24,  ..., -1.1554e-24,\n",
      "          1.3629e-24,  1.1629e-24],\n",
      "        [-5.1286e-24,  5.1276e-24,  5.1147e-24,  ...,  1.1554e-24,\n",
      "         -1.3629e-24, -1.1629e-24]])\n",
      "batch:  ['vvvvvvvvvv']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['vvvvvvv']\n",
      "per_prediction_loss:  tensor([485.4776], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-3.3178e-24,  3.3176e-24,  3.3124e-24,  ...,  7.8516e-25,\n",
      "         -9.2636e-25, -7.8200e-25],\n",
      "        [ 3.3178e-24, -3.3176e-24, -3.3124e-24,  ..., -7.8516e-25,\n",
      "          9.2637e-25,  7.8200e-25],\n",
      "        [ 3.3178e-24, -3.3176e-24, -3.3124e-24,  ..., -7.8516e-25,\n",
      "          9.2636e-25,  7.8200e-25],\n",
      "        ...,\n",
      "        [ 3.3178e-24, -3.3176e-24, -3.3124e-24,  ..., -7.8516e-25,\n",
      "          9.2636e-25,  7.8200e-25],\n",
      "        [ 3.3177e-24, -3.3176e-24, -3.3124e-24,  ..., -7.8515e-25,\n",
      "          9.2635e-25,  7.8199e-25],\n",
      "        [-3.3178e-24,  3.3176e-24,  3.3124e-24,  ...,  7.8516e-25,\n",
      "         -9.2636e-25, -7.8200e-25]])\n",
      "Epoch 0 iteration 78: loss = -245.474, tp = 26.77 lines/s, ETA 00h00m19s\n",
      "batch:  ['yyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yy']\n",
      "per_prediction_loss:  tensor([270.5891], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "batch:  ['ffffffffff']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['fffffff']\n",
      "per_prediction_loss:  tensor([113.6855], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-5.1542e-24,  5.1541e-24,  5.1458e-24,  ...,  1.1636e-24,\n",
      "         -1.3791e-24, -1.1696e-24],\n",
      "        [ 5.1542e-24, -5.1541e-24, -5.1458e-24,  ..., -1.1636e-24,\n",
      "          1.3791e-24,  1.1696e-24],\n",
      "        [ 5.1541e-24, -5.1541e-24, -5.1458e-24,  ..., -1.1636e-24,\n",
      "          1.3791e-24,  1.1696e-24],\n",
      "        ...,\n",
      "        [ 5.1541e-24, -5.1541e-24, -5.1458e-24,  ..., -1.1636e-24,\n",
      "          1.3791e-24,  1.1696e-24],\n",
      "        [ 5.1541e-24, -5.1540e-24, -5.1457e-24,  ..., -1.1636e-24,\n",
      "          1.3791e-24,  1.1695e-24],\n",
      "        [-5.1541e-24,  5.1541e-24,  5.1458e-24,  ...,  1.1636e-24,\n",
      "         -1.3791e-24, -1.1696e-24]])\n",
      "Epoch 0 iteration 80: loss = -65.848, tp = 26.82 lines/s, ETA 00h00m19s\n",
      "batch:  ['sssss']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ssss']\n",
      "per_prediction_loss:  tensor([237.5480], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-2.0188e-24,  2.0184e-24,  2.0135e-24,  ...,  4.6613e-25,\n",
      "         -5.4909e-25, -4.6361e-25],\n",
      "        [ 2.0188e-24, -2.0184e-24, -2.0135e-24,  ..., -4.6613e-25,\n",
      "          5.4909e-25,  4.6361e-25],\n",
      "        [ 2.0188e-24, -2.0184e-24, -2.0135e-24,  ..., -4.6613e-25,\n",
      "          5.4909e-25,  4.6361e-25],\n",
      "        ...,\n",
      "        [ 2.0188e-24, -2.0184e-24, -2.0135e-24,  ..., -4.6613e-25,\n",
      "          5.4909e-25,  4.6361e-25],\n",
      "        [ 2.0187e-24, -2.0183e-24, -2.0135e-24,  ..., -4.6612e-25,\n",
      "          5.4909e-25,  4.6360e-25],\n",
      "        [-2.0188e-24,  2.0184e-24,  2.0135e-24,  ...,  4.6613e-25,\n",
      "         -5.4909e-25, -4.6361e-25]])\n",
      "batch:  ['ooooo']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['o']\n",
      "per_prediction_loss:  tensor([179.8046], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 0 iteration 82: loss = -95.510, tp = 26.96 lines/s, ETA 00h00m19s\n",
      "batch:  ['ooooo']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['oooo']\n",
      "per_prediction_loss:  tensor([183.4889], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-6.1778e-24,  6.1767e-24,  6.1615e-24,  ...,  1.4163e-24,\n",
      "         -1.6598e-24, -1.3761e-24],\n",
      "        [ 6.1777e-24, -6.1766e-24, -6.1615e-24,  ..., -1.4163e-24,\n",
      "          1.6598e-24,  1.3761e-24],\n",
      "        [ 6.1778e-24, -6.1767e-24, -6.1615e-24,  ..., -1.4163e-24,\n",
      "          1.6598e-24,  1.3761e-24],\n",
      "        ...,\n",
      "        [ 6.1777e-24, -6.1766e-24, -6.1615e-24,  ..., -1.4163e-24,\n",
      "          1.6598e-24,  1.3761e-24],\n",
      "        [ 6.1776e-24, -6.1765e-24, -6.1614e-24,  ..., -1.4163e-24,\n",
      "          1.6598e-24,  1.3761e-24],\n",
      "        [-6.1777e-24,  6.1767e-24,  6.1615e-24,  ...,  1.4163e-24,\n",
      "         -1.6598e-24, -1.3761e-24]])\n",
      "batch:  ['pppppppppp']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['pppppp']\n",
      "per_prediction_loss:  tensor([411.0798], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-3.0664e-23,  3.0663e-23,  3.0615e-23,  ...,  6.9135e-24,\n",
      "         -8.2120e-24, -6.8920e-24],\n",
      "        [ 3.0665e-23, -3.0663e-23, -3.0615e-23,  ..., -6.9135e-24,\n",
      "          8.2121e-24,  6.8920e-24],\n",
      "        [ 3.0664e-23, -3.0663e-23, -3.0615e-23,  ..., -6.9135e-24,\n",
      "          8.2120e-24,  6.8920e-24],\n",
      "        ...,\n",
      "        [ 3.0664e-23, -3.0663e-23, -3.0615e-23,  ..., -6.9135e-24,\n",
      "          8.2120e-24,  6.8919e-24],\n",
      "        [ 3.0664e-23, -3.0662e-23, -3.0614e-23,  ..., -6.9134e-24,\n",
      "          8.2119e-24,  6.8918e-24],\n",
      "        [-3.0664e-23,  3.0663e-23,  3.0615e-23,  ...,  6.9135e-24,\n",
      "         -8.2120e-24, -6.8920e-24]])\n",
      "Epoch 0 iteration 84: loss = -224.174, tp = 26.81 lines/s, ETA 00h00m19s\n",
      "batch:  ['eeeee']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eee']\n",
      "per_prediction_loss:  tensor([287.6128], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-2.4670e-26,  2.4637e-26,  2.4456e-26,  ...,  5.7994e-27,\n",
      "         -6.7765e-27, -5.8159e-27],\n",
      "        [ 2.4670e-26, -2.4637e-26, -2.4456e-26,  ..., -5.7993e-27,\n",
      "          6.7765e-27,  5.8159e-27],\n",
      "        [ 2.4670e-26, -2.4637e-26, -2.4456e-26,  ..., -5.7993e-27,\n",
      "          6.7765e-27,  5.8159e-27],\n",
      "        ...,\n",
      "        [ 2.4670e-26, -2.4637e-26, -2.4456e-26,  ..., -5.7993e-27,\n",
      "          6.7765e-27,  5.8159e-27],\n",
      "        [ 2.4669e-26, -2.4637e-26, -2.4455e-26,  ..., -5.7993e-27,\n",
      "          6.7764e-27,  5.8159e-27],\n",
      "        [-2.4670e-26,  2.4637e-26,  2.4456e-26,  ...,  5.7993e-27,\n",
      "         -6.7765e-27, -5.8159e-27]])\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmmmmm']\n",
      "per_prediction_loss:  tensor([228.7159], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-1.2363e-23,  1.2363e-23,  1.2343e-23,  ...,  2.8174e-24,\n",
      "         -3.3071e-24, -2.7687e-24],\n",
      "        [ 1.2363e-23, -1.2363e-23, -1.2343e-23,  ..., -2.8174e-24,\n",
      "          3.3071e-24,  2.7687e-24],\n",
      "        [ 1.2363e-23, -1.2363e-23, -1.2343e-23,  ..., -2.8174e-24,\n",
      "          3.3071e-24,  2.7687e-24],\n",
      "        ...,\n",
      "        [ 1.2363e-23, -1.2363e-23, -1.2343e-23,  ..., -2.8174e-24,\n",
      "          3.3071e-24,  2.7687e-24],\n",
      "        [ 1.2363e-23, -1.2363e-23, -1.2343e-23,  ..., -2.8173e-24,\n",
      "          3.3070e-24,  2.7687e-24],\n",
      "        [-1.2363e-23,  1.2363e-23,  1.2343e-23,  ...,  2.8174e-24,\n",
      "         -3.3071e-24, -2.7687e-24]])\n",
      "Epoch 0 iteration 86: loss = -132.778, tp = 26.83 lines/s, ETA 00h00m19s\n",
      "batch:  ['ssssssssss']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ss']\n",
      "per_prediction_loss:  tensor([440.8954], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "batch:  ['dddddddddd']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977,\n",
      "         0.4977, 0.4977, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dddddd']\n",
      "per_prediction_loss:  tensor([530.8807], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-3.3814e-25,  3.3813e-25,  3.3760e-25,  ...,  8.3275e-26,\n",
      "         -9.7025e-26, -8.1766e-26],\n",
      "        [ 3.3814e-25, -3.3813e-25, -3.3760e-25,  ..., -8.3275e-26,\n",
      "          9.7024e-26,  8.1766e-26],\n",
      "        [ 3.3814e-25, -3.3813e-25, -3.3760e-25,  ..., -8.3275e-26,\n",
      "          9.7024e-26,  8.1765e-26],\n",
      "        ...,\n",
      "        [ 3.3813e-25, -3.3812e-25, -3.3760e-25,  ..., -8.3275e-26,\n",
      "          9.7024e-26,  8.1765e-26],\n",
      "        [ 3.3813e-25, -3.3812e-25, -3.3759e-25,  ..., -8.3273e-26,\n",
      "          9.7022e-26,  8.1764e-26],\n",
      "        [-3.3813e-25,  3.3812e-25,  3.3760e-25,  ...,  8.3275e-26,\n",
      "         -9.7024e-26, -8.1765e-26]])\n",
      "Epoch 0 iteration 88: loss = -301.839, tp = 26.76 lines/s, ETA 00h00m19s\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ttt']\n",
      "per_prediction_loss:  tensor([353.5027], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-2.5992e-26,  2.5957e-26,  2.5740e-26,  ...,  6.0143e-27,\n",
      "         -7.1613e-27, -6.0978e-27],\n",
      "        [ 2.5992e-26, -2.5957e-26, -2.5740e-26,  ..., -6.0143e-27,\n",
      "          7.1613e-27,  6.0978e-27],\n",
      "        [ 2.5992e-26, -2.5957e-26, -2.5740e-26,  ..., -6.0143e-27,\n",
      "          7.1613e-27,  6.0978e-27],\n",
      "        ...,\n",
      "        [ 2.5992e-26, -2.5957e-26, -2.5740e-26,  ..., -6.0143e-27,\n",
      "          7.1613e-27,  6.0978e-27],\n",
      "        [ 2.5992e-26, -2.5957e-26, -2.5739e-26,  ..., -6.0142e-27,\n",
      "          7.1612e-27,  6.0977e-27],\n",
      "        [-2.5992e-26,  2.5957e-26,  2.5740e-26,  ...,  6.0143e-27,\n",
      "         -7.1613e-27, -6.0977e-27]])\n",
      "batch:  ['oooooooooo']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975,\n",
      "         0.4975, 0.4975, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ooooo']\n",
      "per_prediction_loss:  tensor([364.6268], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-2.6923e-23,  2.6922e-23,  2.6875e-23,  ...,  6.1569e-24,\n",
      "         -7.2146e-24, -5.9820e-24],\n",
      "        [ 2.6923e-23, -2.6922e-23, -2.6875e-23,  ..., -6.1569e-24,\n",
      "          7.2146e-24,  5.9820e-24],\n",
      "        [ 2.6923e-23, -2.6922e-23, -2.6875e-23,  ..., -6.1569e-24,\n",
      "          7.2146e-24,  5.9820e-24],\n",
      "        ...,\n",
      "        [ 2.6923e-23, -2.6922e-23, -2.6875e-23,  ..., -6.1569e-24,\n",
      "          7.2146e-24,  5.9819e-24],\n",
      "        [ 2.6922e-23, -2.6921e-23, -2.6875e-23,  ..., -6.1568e-24,\n",
      "          7.2145e-24,  5.9818e-24],\n",
      "        [-2.6923e-23,  2.6922e-23,  2.6875e-23,  ...,  6.1569e-24,\n",
      "         -7.2146e-24, -5.9819e-24]])\n",
      "Epoch 0 iteration 90: loss = -213.741, tp = 26.74 lines/s, ETA 00h00m19s\n",
      "batch:  ['kkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kk']\n",
      "per_prediction_loss:  tensor([239.4188], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ttt']\n",
      "per_prediction_loss:  tensor([384.0198], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-3.0329e-26,  3.0288e-26,  3.0035e-26,  ...,  7.0191e-27,\n",
      "         -8.3574e-27, -7.1165e-27],\n",
      "        [ 3.0329e-26, -3.0288e-26, -3.0035e-26,  ..., -7.0191e-27,\n",
      "          8.3574e-27,  7.1165e-27],\n",
      "        [ 3.0329e-26, -3.0288e-26, -3.0035e-26,  ..., -7.0191e-27,\n",
      "          8.3574e-27,  7.1164e-27],\n",
      "        ...,\n",
      "        [ 3.0329e-26, -3.0288e-26, -3.0035e-26,  ..., -7.0191e-27,\n",
      "          8.3574e-27,  7.1164e-27],\n",
      "        [ 3.0328e-26, -3.0288e-26, -3.0034e-26,  ..., -7.0190e-27,\n",
      "          8.3573e-27,  7.1163e-27],\n",
      "        [-3.0329e-26,  3.0288e-26,  3.0035e-26,  ...,  7.0191e-27,\n",
      "         -8.3574e-27, -7.1164e-27]])\n",
      "Epoch 0 iteration 92: loss = -228.391, tp = 26.82 lines/s, ETA 00h00m18s\n",
      "batch:  ['ppppp']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ppp']\n",
      "per_prediction_loss:  tensor([286.4982], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-5.7909e-26,  5.7823e-26,  5.7400e-26,  ...,  1.3430e-26,\n",
      "         -1.5950e-26, -1.3388e-26],\n",
      "        [ 5.7909e-26, -5.7823e-26, -5.7400e-26,  ..., -1.3430e-26,\n",
      "          1.5950e-26,  1.3388e-26],\n",
      "        [ 5.7909e-26, -5.7823e-26, -5.7400e-26,  ..., -1.3430e-26,\n",
      "          1.5950e-26,  1.3388e-26],\n",
      "        ...,\n",
      "        [ 5.7909e-26, -5.7823e-26, -5.7400e-26,  ..., -1.3430e-26,\n",
      "          1.5950e-26,  1.3387e-26],\n",
      "        [ 5.7908e-26, -5.7822e-26, -5.7399e-26,  ..., -1.3430e-26,\n",
      "          1.5950e-26,  1.3387e-26],\n",
      "        [-5.7909e-26,  5.7823e-26,  5.7400e-26,  ...,  1.3430e-26,\n",
      "         -1.5950e-26, -1.3387e-26]])\n",
      "batch:  ['ccccc']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['cc']\n",
      "per_prediction_loss:  tensor([320.7622], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Epoch 0 iteration 94: loss = -195.012, tp = 26.83 lines/s, ETA 00h00m18s\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjj']\n",
      "per_prediction_loss:  tensor([312.5421], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-8.8175e-27,  8.8045e-27,  8.7398e-27,  ...,  2.1000e-27,\n",
      "         -2.5113e-27, -2.1075e-27],\n",
      "        [ 8.8175e-27, -8.8045e-27, -8.7398e-27,  ..., -2.1000e-27,\n",
      "          2.5113e-27,  2.1075e-27],\n",
      "        [ 8.8175e-27, -8.8045e-27, -8.7398e-27,  ..., -2.1000e-27,\n",
      "          2.5113e-27,  2.1075e-27],\n",
      "        ...,\n",
      "        [ 8.8175e-27, -8.8044e-27, -8.7398e-27,  ..., -2.0999e-27,\n",
      "          2.5113e-27,  2.1074e-27],\n",
      "        [ 8.8173e-27, -8.8042e-27, -8.7396e-27,  ..., -2.0999e-27,\n",
      "          2.5112e-27,  2.1074e-27],\n",
      "        [-8.8175e-27,  8.8044e-27,  8.7398e-27,  ...,  2.0999e-27,\n",
      "         -2.5113e-27, -2.1074e-27]])\n",
      "batch:  ['yyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yyy']\n",
      "per_prediction_loss:  tensor([333.6788], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-2.8981e-27,  2.8937e-27,  2.8710e-27,  ...,  7.3120e-28,\n",
      "         -8.2089e-28, -6.9046e-28],\n",
      "        [ 2.8981e-27, -2.8937e-27, -2.8710e-27,  ..., -7.3120e-28,\n",
      "          8.2089e-28,  6.9046e-28],\n",
      "        [ 2.8981e-27, -2.8937e-27, -2.8710e-27,  ..., -7.3120e-28,\n",
      "          8.2089e-28,  6.9046e-28],\n",
      "        ...,\n",
      "        [ 2.8981e-27, -2.8937e-27, -2.8710e-27,  ..., -7.3120e-28,\n",
      "          8.2089e-28,  6.9046e-28],\n",
      "        [ 2.8980e-27, -2.8936e-27, -2.8709e-27,  ..., -7.3118e-28,\n",
      "          8.2088e-28,  6.9045e-28],\n",
      "        [-2.8981e-27,  2.8937e-27,  2.8710e-27,  ...,  7.3120e-28,\n",
      "         -8.2089e-28, -6.9046e-28]])\n",
      "Epoch 0 iteration 96: loss = -207.512, tp = 26.89 lines/s, ETA 00h00m18s\n",
      "batch:  ['pppppppppp']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ppppppp']\n",
      "per_prediction_loss:  tensor([512.1363], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-3.9016e-23,  3.9014e-23,  3.8954e-23,  ...,  8.8063e-24,\n",
      "         -1.0458e-23, -8.7785e-24],\n",
      "        [ 3.9016e-23, -3.9014e-23, -3.8954e-23,  ..., -8.8063e-24,\n",
      "          1.0458e-23,  8.7785e-24],\n",
      "        [ 3.9015e-23, -3.9014e-23, -3.8954e-23,  ..., -8.8063e-24,\n",
      "          1.0458e-23,  8.7785e-24],\n",
      "        ...,\n",
      "        [ 3.9015e-23, -3.9014e-23, -3.8953e-23,  ..., -8.8063e-24,\n",
      "          1.0458e-23,  8.7785e-24],\n",
      "        [ 3.9015e-23, -3.9013e-23, -3.8953e-23,  ..., -8.8062e-24,\n",
      "          1.0458e-23,  8.7784e-24],\n",
      "        [-3.9015e-23,  3.9014e-23,  3.8953e-23,  ...,  8.8063e-24,\n",
      "         -1.0458e-23, -8.7785e-24]])\n",
      "batch:  ['kkkkkkkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kkkkk']\n",
      "per_prediction_loss:  tensor([482.5391], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[-3.7984e-23,  3.7983e-23,  3.7919e-23,  ...,  8.7113e-24,\n",
      "         -1.0172e-23, -8.5649e-24],\n",
      "        [ 3.7984e-23, -3.7983e-23, -3.7919e-23,  ..., -8.7113e-24,\n",
      "          1.0172e-23,  8.5649e-24],\n",
      "        [ 3.7984e-23, -3.7983e-23, -3.7919e-23,  ..., -8.7113e-24,\n",
      "          1.0172e-23,  8.5649e-24],\n",
      "        ...,\n",
      "        [ 3.7984e-23, -3.7983e-23, -3.7919e-23,  ..., -8.7113e-24,\n",
      "          1.0172e-23,  8.5649e-24],\n",
      "        [ 3.7984e-23, -3.7982e-23, -3.7919e-23,  ..., -8.7111e-24,\n",
      "          1.0172e-23,  8.5647e-24],\n",
      "        [-3.7984e-23,  3.7983e-23,  3.7919e-23,  ...,  8.7113e-24,\n",
      "         -1.0172e-23, -8.5649e-24]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iteration 98: loss = -308.877, tp = 26.78 lines/s, ETA 00h00m18s\n",
      "batch:  ['kkkkkkkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kkkkkk']\n",
      "per_prediction_loss:  tensor([496.6616], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['nnnnnnnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnnnn']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 100: loss = nan, tp = 26.69 lines/s, ETA 00h00m18s\n",
      "batch:  ['qqqqqqqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['qqqqq']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjjjjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975,\n",
      "         0.4975, 0.4975, 0.4975]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 102: loss = nan, tp = 26.65 lines/s, ETA 00h00m18s\n",
      "batch:  ['ooooo']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['oo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ccccc']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['cccc']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 104: loss = nan, tp = 26.72 lines/s, ETA 00h00m18s\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ggggg']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['gggg']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 106: loss = nan, tp = 26.68 lines/s, ETA 00h00m18s\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmmmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ttttttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 108: loss = nan, tp = 26.64 lines/s, ETA 00h00m18s\n",
      "batch:  ['xxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xxx']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['bbbbb']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['bbb']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 110: loss = nan, tp = 26.68 lines/s, ETA 00h00m18s\n",
      "batch:  ['ttttt']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['tt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['aaaaaaaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 112: loss = nan, tp = 26.73 lines/s, ETA 00h00m18s\n",
      "batch:  ['aaaaaaaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaaa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 114: loss = nan, tp = 26.77 lines/s, ETA 00h00m18s\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuuuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 116: loss = nan, tp = 26.74 lines/s, ETA 00h00m18s\n",
      "batch:  ['kkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kkk']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['qqqqqqqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['qqqqqq']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 118: loss = nan, tp = 26.77 lines/s, ETA 00h00m17s\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 120: loss = nan, tp = 26.81 lines/s, ETA 00h00m17s\n",
      "batch:  ['jjjjjjjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975,\n",
      "         0.4975, 0.4975, 0.4975]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['llllllllll']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['lllll']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 122: loss = nan, tp = 26.66 lines/s, ETA 00h00m17s\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977,\n",
      "         0.4977, 0.4977, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dddddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['kkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['k']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 124: loss = nan, tp = 26.69 lines/s, ETA 00h00m17s\n",
      "batch:  ['oooooooooo']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975,\n",
      "         0.4975, 0.4975, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['oooooo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iiiiiii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 126: loss = nan, tp = 26.60 lines/s, ETA 00h00m17s\n",
      "batch:  ['rrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rrr']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['aaaaaaaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 128: loss = nan, tp = 26.65 lines/s, ETA 00h00m17s\n",
      "batch:  ['aaaaaaaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaaaa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iiii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 130: loss = nan, tp = 26.64 lines/s, ETA 00h00m17s\n",
      "batch:  ['hhhhhhhhhh']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977,\n",
      "         0.4977, 0.4977, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['hhhhh']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['tttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 132: loss = nan, tp = 26.63 lines/s, ETA 00h00m17s\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iiii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iiiiii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 134: loss = nan, tp = 26.63 lines/s, ETA 00h00m17s\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['zzzzzzzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zzzzzz']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 136: loss = nan, tp = 26.66 lines/s, ETA 00h00m17s\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ttttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 138: loss = nan, tp = 26.55 lines/s, ETA 00h00m17s\n",
      "batch:  ['eeeee']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eeee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 140: loss = nan, tp = 26.63 lines/s, ETA 00h00m17s\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 142: loss = nan, tp = 26.62 lines/s, ETA 00h00m17s\n",
      "batch:  ['ooooo']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['oo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['llllllllll']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['llll']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 144: loss = nan, tp = 26.50 lines/s, ETA 00h00m17s\n",
      "batch:  ['sssss']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['s']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['vvvvvvvvvv']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['vvvvv']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 146: loss = nan, tp = 26.39 lines/s, ETA 00h00m17s\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ggggg']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['g']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 148: loss = nan, tp = 26.40 lines/s, ETA 00h00m17s\n",
      "batch:  ['eeeee']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['cccccccccc']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['cccc']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 150: loss = nan, tp = 26.44 lines/s, ETA 00h00m16s\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['eeeeeeeeee']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eeeee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 152: loss = nan, tp = 26.38 lines/s, ETA 00h00m16s\n",
      "batch:  ['oooooooooo']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975,\n",
      "         0.4975, 0.4975, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['oooo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['zzzzzzzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zzzzz']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 154: loss = nan, tp = 26.32 lines/s, ETA 00h00m16s\n",
      "batch:  ['ccccc']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['c']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['xxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xx']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 156: loss = nan, tp = 26.40 lines/s, ETA 00h00m16s\n",
      "batch:  ['bbbbb']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['bbb']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['vvvvvvvvvv']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['vvvvv']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 158: loss = nan, tp = 26.37 lines/s, ETA 00h00m16s\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['yyyyyyyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977,\n",
      "         0.4977, 0.4977, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yyy']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 160: loss = nan, tp = 26.42 lines/s, ETA 00h00m16s\n",
      "batch:  ['zzzzzzzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zzzzzzz']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['eeeee']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 162: loss = nan, tp = 26.43 lines/s, ETA 00h00m16s\n",
      "batch:  ['vvvvvvvvvv']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['vvvvvv']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['vvvvvvvvvv']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['vvvvv']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 164: loss = nan, tp = 26.39 lines/s, ETA 00h00m16s\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iiiiii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['zzzzzzzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zzzzz']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 166: loss = nan, tp = 26.37 lines/s, ETA 00h00m16s\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['m']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977,\n",
      "         0.4977, 0.4977, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['d']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 168: loss = nan, tp = 26.33 lines/s, ETA 00h00m16s\n",
      "batch:  ['eeeee']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['e']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuuuuuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 170: loss = nan, tp = 26.13 lines/s, ETA 00h00m16s\n",
      "batch:  ['eeeee']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['e']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['oooooooooo']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975,\n",
      "         0.4975, 0.4975, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ooooo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 172: loss = nan, tp = 26.03 lines/s, ETA 00h00m16s\n",
      "batch:  ['ssssssssss']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['sssssss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['aaaaaaaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaaaaa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 174: loss = nan, tp = 25.92 lines/s, ETA 00h00m16s\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iiii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 176: loss = nan, tp = 25.89 lines/s, ETA 00h00m16s\n",
      "batch:  ['sssss']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['sss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['kkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kk']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 178: loss = nan, tp = 25.95 lines/s, ETA 00h00m16s\n",
      "batch:  ['eeeee']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuuuuuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 180: loss = nan, tp = 25.84 lines/s, ETA 00h00m16s\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ttttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['zzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['z']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 182: loss = nan, tp = 25.74 lines/s, ETA 00h00m16s\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iiiii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['bbbbb']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['bbb']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 184: loss = nan, tp = 25.62 lines/s, ETA 00h00m16s\n",
      "batch:  ['kkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kkk']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 186: loss = nan, tp = 25.57 lines/s, ETA 00h00m16s\n",
      "batch:  ['eeeee']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['pppppppppp']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ppppp']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 188: loss = nan, tp = 25.59 lines/s, ETA 00h00m16s\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ttttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 190: loss = nan, tp = 25.61 lines/s, ETA 00h00m15s\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['cccccccccc']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ccccc']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 192: loss = nan, tp = 25.66 lines/s, ETA 00h00m15s\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['pppppppppp']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['pppppp']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 194: loss = nan, tp = 25.68 lines/s, ETA 00h00m15s\n",
      "batch:  ['kkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kkk']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['eeeee']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 196: loss = nan, tp = 25.72 lines/s, ETA 00h00m15s\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['sssss']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ssss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 198: loss = nan, tp = 25.75 lines/s, ETA 00h00m15s\n",
      "batch:  ['cccccccccc']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['cccc']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['eeeeeeeeee']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eeeee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 0 iteration 200: loss = nan, tp = 25.74 lines/s, ETA 00h00m15s\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['rrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['r']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 1: loss = nan, tp = 25.65 lines/s, ETA 00h00m15s\n",
      "batch:  ['qqqqqqqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['qqqqq']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['rrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rrr']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 3: loss = nan, tp = 25.54 lines/s, ETA 00h00m15s\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuuuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['cccccccccc']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['cccc']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 5: loss = nan, tp = 25.39 lines/s, ETA 00h00m15s\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977,\n",
      "         0.4977, 0.4977, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddddddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['oooooooooo']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975,\n",
      "         0.4975, 0.4975, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ooo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 7: loss = nan, tp = 25.39 lines/s, ETA 00h00m15s\n",
      "batch:  ['bbbbb']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['bbb']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ooooo']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ooo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 9: loss = nan, tp = 25.42 lines/s, ETA 00h00m15s\n",
      "batch:  ['vvvvv']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['vvv']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['zzzzzzzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zzzzz']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 11: loss = nan, tp = 25.46 lines/s, ETA 00h00m15s\n",
      "batch:  ['vvvvvvvvvv']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['vvvv']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['bbbbb']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['bbb']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 iteration 13: loss = nan, tp = 25.47 lines/s, ETA 00h00m15s\n",
      "batch:  ['gggggggggg']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ggg']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['zzzzzzzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['zzz']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 15: loss = nan, tp = 25.34 lines/s, ETA 00h00m15s\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['sssss']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 17: loss = nan, tp = 25.32 lines/s, ETA 00h00m15s\n",
      "batch:  ['vvvvvvvvvv']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['vvvvv']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ttttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 19: loss = nan, tp = 25.17 lines/s, ETA 00h00m15s\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['wwwwwwwwww']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['wwww']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 21: loss = nan, tp = 25.20 lines/s, ETA 00h00m14s\n",
      "batch:  ['cccccccccc']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['cccccc']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['oooooooooo']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975,\n",
      "         0.4975, 0.4975, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ooooooo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 23: loss = nan, tp = 25.17 lines/s, ETA 00h00m14s\n",
      "batch:  ['ppppp']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['pppp']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 25: loss = nan, tp = 25.22 lines/s, ETA 00h00m14s\n",
      "batch:  ['jjjjjjjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975,\n",
      "         0.4975, 0.4975, 0.4975]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjjjjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ssssssssss']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['sssss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 27: loss = nan, tp = 25.20 lines/s, ETA 00h00m14s\n",
      "batch:  ['lllll']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['llll']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ssssssssss']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['sssss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 29: loss = nan, tp = 25.23 lines/s, ETA 00h00m14s\n",
      "batch:  ['rrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['r']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['gggggggggg']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['gggg']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 31: loss = nan, tp = 25.27 lines/s, ETA 00h00m14s\n",
      "batch:  ['xxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xxxx']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['zzzzzzzzzz']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_batch_strings: ['zzzzz']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 33: loss = nan, tp = 25.27 lines/s, ETA 00h00m14s\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['lllll']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['llll']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 35: loss = nan, tp = 25.29 lines/s, ETA 00h00m14s\n",
      "batch:  ['bbbbb']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['b']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['eeeee']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eeee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 37: loss = nan, tp = 25.35 lines/s, ETA 00h00m14s\n",
      "batch:  ['bbbbb']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['bbbbb']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['eeeeeeeeee']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eeeeee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 39: loss = nan, tp = 25.31 lines/s, ETA 00h00m14s\n",
      "batch:  ['ttttt']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['tt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['bbbbb']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['bbbb']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 41: loss = nan, tp = 25.34 lines/s, ETA 00h00m14s\n",
      "batch:  ['nnnnnnnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnnn']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 43: loss = nan, tp = 25.33 lines/s, ETA 00h00m14s\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iiii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['lllll']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ll']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 45: loss = nan, tp = 25.34 lines/s, ETA 00h00m13s\n",
      "batch:  ['rrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rr']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 47: loss = nan, tp = 25.39 lines/s, ETA 00h00m13s\n",
      "batch:  ['yyyyyyyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977,\n",
      "         0.4977, 0.4977, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yyyyyy']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuu']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 49: loss = nan, tp = 25.37 lines/s, ETA 00h00m13s\n",
      "batch:  ['oooooooooo']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975,\n",
      "         0.4975, 0.4975, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ooo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ttttt']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 51: loss = nan, tp = 25.41 lines/s, ETA 00h00m13s\n",
      "batch:  ['cccccccccc']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ccccccc']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['vvvvv']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['vv']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 53: loss = nan, tp = 25.44 lines/s, ETA 00h00m13s\n",
      "batch:  ['rrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['r']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuuuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 55: loss = nan, tp = 25.44 lines/s, ETA 00h00m13s\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['xxxxxxxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xxxxx']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 57: loss = nan, tp = 25.40 lines/s, ETA 00h00m13s\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['kkkkk']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['kk']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 59: loss = nan, tp = 25.25 lines/s, ETA 00h00m13s\n",
      "batch:  ['ffffffffff']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ff']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmmmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 61: loss = nan, tp = 25.10 lines/s, ETA 00h00m13s\n",
      "batch:  ['ppppp']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['pppp']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['vvvvvvvvvv']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['vvv']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 63: loss = nan, tp = 24.97 lines/s, ETA 00h00m13s\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['rrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rr']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 65: loss = nan, tp = 25.01 lines/s, ETA 00h00m13s\n",
      "batch:  ['tttttttttt']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_batch_strings: ['tttttt']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['bbbbb']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 67: loss = nan, tp = 25.02 lines/s, ETA 00h00m13s\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['wwwww']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['www']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 69: loss = nan, tp = 25.05 lines/s, ETA 00h00m13s\n",
      "batch:  ['rrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['rrr']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['pppppppppp']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['pppppp']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 71: loss = nan, tp = 24.96 lines/s, ETA 00h00m13s\n",
      "batch:  ['eeeeeeeeee']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['eeee']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 73: loss = nan, tp = 24.76 lines/s, ETA 00h00m13s\n",
      "batch:  ['nnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nn']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 75: loss = nan, tp = 24.74 lines/s, ETA 00h00m13s\n",
      "batch:  ['nnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnnnn']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['rrrrr']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['r']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 77: loss = nan, tp = 24.78 lines/s, ETA 00h00m12s\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuuuuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['oooooooooo']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975,\n",
      "         0.4975, 0.4975, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['oooooo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 79: loss = nan, tp = 24.78 lines/s, ETA 00h00m12s\n",
      "batch:  ['oooooooooo']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975,\n",
      "         0.4975, 0.4975, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ooooo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['jjjjjjjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975,\n",
      "         0.4975, 0.4975, 0.4975]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['jjjjjjj']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 81: loss = nan, tp = 24.77 lines/s, ETA 00h00m12s\n",
      "batch:  ['uuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977,\n",
      "         0.4977, 0.4977, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['dddddddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 83: loss = nan, tp = 24.77 lines/s, ETA 00h00m12s\n",
      "batch:  ['mmmmmmmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['oooooooooo']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975,\n",
      "         0.4975, 0.4975, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ooo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 85: loss = nan, tp = 24.62 lines/s, ETA 00h00m12s\n",
      "batch:  ['ccccc']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ccc']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ooooo']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['oo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 87: loss = nan, tp = 24.57 lines/s, ETA 00h00m12s\n",
      "batch:  ['aaaaaaaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaaaa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['cccccccccc']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ccccc']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 89: loss = nan, tp = 24.43 lines/s, ETA 00h00m12s\n",
      "batch:  ['vvvvv']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['vv']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iiiiiii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 91: loss = nan, tp = 24.44 lines/s, ETA 00h00m12s\n",
      "batch:  ['ccccc']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['mmmmm']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['mmm']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 93: loss = nan, tp = 24.49 lines/s, ETA 00h00m12s\n",
      "batch:  ['eeeee']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['e']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['dddddddddd']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977,\n",
      "         0.4977, 0.4977, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 95: loss = nan, tp = 24.52 lines/s, ETA 00h00m12s\n",
      "batch:  ['aaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['a']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['aaaaaaaaaa']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['aaaaa']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 97: loss = nan, tp = 24.53 lines/s, ETA 00h00m12s\n",
      "batch:  ['lllll']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['l']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ooooo']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['oo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 99: loss = nan, tp = 24.57 lines/s, ETA 00h00m12s\n",
      "batch:  ['cccccccccc']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ccccc']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ssssssssss']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ssss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 101: loss = nan, tp = 24.55 lines/s, ETA 00h00m12s\n",
      "batch:  ['yyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yy']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['nnnnnnnnnn']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['nnnnnn']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 103: loss = nan, tp = 24.57 lines/s, ETA 00h00m12s\n",
      "batch:  ['oooooooooo']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975,\n",
      "         0.4975, 0.4975, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['oooo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ggggg']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['gggg']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 105: loss = nan, tp = 24.60 lines/s, ETA 00h00m11s\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['j']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['xxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xxx']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 107: loss = nan, tp = 24.64 lines/s, ETA 00h00m11s\n",
      "batch:  ['jjjjj']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['j']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['uuuuuuuuuu']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['uuuu']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 109: loss = nan, tp = 24.63 lines/s, ETA 00h00m11s\n",
      "batch:  ['xxxxx']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['xx']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['hhhhhhhhhh']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977,\n",
      "         0.4977, 0.4977, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['hhhhh']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 111: loss = nan, tp = 24.65 lines/s, ETA 00h00m11s\n",
      "batch:  ['yyyyyyyyyy']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4977,\n",
      "         0.4977, 0.4977, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['yy']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['qqqqq']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['qqq']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 113: loss = nan, tp = 24.68 lines/s, ETA 00h00m11s\n",
      "batch:  ['oooooooooo']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975, 0.4975,\n",
      "         0.4975, 0.4975, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['oooooo']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['bbbbb']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['bbb']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 115: loss = nan, tp = 24.69 lines/s, ETA 00h00m11s\n",
      "batch:  ['sssss']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ssss']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['iiiiiiiiii']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976,\n",
      "         0.4976, 0.4976, 0.4976]], grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['iii']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 117: loss = nan, tp = 24.70 lines/s, ETA 00h00m11s\n",
      "batch:  ['eeeee']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['e']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['lllll']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['lll']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "Epoch 1 iteration 119: loss = nan, tp = 24.73 lines/s, ETA 00h00m11s\n",
      "batch:  ['ddddd']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4977, 0.4977, 0.4977, 0.4977, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['ddd']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "batch:  ['ppppp']\n",
      "encoded_batch_probs:  tensor([[0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976, 0.4976]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "encoded_batch_strings: ['pp']\n",
      "per_prediction_loss:  tensor([nan], grad_fn=<SumBackward1>)\n",
      "grad:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-0b2c672d17a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_loss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdumb_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyBrain/BodySelf/FormalEducation/Stanford/CS224n/FinalProject/cs224n-project/cs224n-project/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, dataset, parameters, device)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'grad: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_proj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'learning_rate': 5e-3,\n",
    "    'verbose': True,\n",
    "    'batch_size': 1,\n",
    "    'init_scale': 0.01,\n",
    "    'epochs': 3,\n",
    "    'log_every':2\n",
    "}\n",
    "\n",
    "train_loss_history = train(nencoder, decoder, dumb_dataset, parameters, device)\n",
    "plt.plot(train_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., grad_fn=<SumBackward0>)\n",
      "tensor([1., 1.], grad_fn=<BernoulliBackward0>)\n",
      "None\n",
      "tensor(-0.3185, grad_fn=<SqueezeBackward1>)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ps = torch.tensor([0.3, 0.8], requires_grad=True)\n",
    "bs = torch.bernoulli(ps)\n",
    "s = bs.sum()\n",
    "print(s)\n",
    "s.backward()\n",
    "print(bs)\n",
    "print(bs.grad)\n",
    "\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "b = Categorical(ps)\n",
    "mask = b.sample()\n",
    "# next_state, reward = env.step(action)\n",
    "# loss = -m.log_prob(action) * reward\n",
    "loss = b.log_prob(mask)\n",
    "loss.backward()\n",
    "print(loss)\n",
    "print(mask.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs._grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String: 'ffffffffff'\n",
      "Encoded: 'ffffffffff'\n",
      "Decoded: 'd}{}\\x13d}}|d}\\x13\\x13d})dddA}}\\x13})}7d}}}|}}\\x13d}\\x13d}A|\\x13}}Z(AA|\\x13}}{}}}K}}AP}d}}\\x13}d}}}}\\x13d}j}}\\x13Z\\x13A}j}}}\\x13dd})}\\x13}Z\\x13}|d\\x13\\x13}}}t})}\\x13d}}dZ}}|}}d}Z}}dZAA}\\x13})}dd}}dt}\\x04d}dA}}}}A})d}}d}d}A}}\\x04}A}A\\x13d|A}}}}\\x13A}\\x13d}}}dA}}})d}d}{)\\x13|A'\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "SPLIT = 'train'\n",
    "\n",
    "import copy\n",
    "\n",
    "s = random.choice(dumb_dataset[SPLIT])\n",
    "compressed = encoder.encode(s)\n",
    "decompressed = decoder([compressed])\n",
    "\n",
    "print('String:', repr(s))\n",
    "print('Encoded:', repr(compressed))\n",
    "print('Decoded:', repr(decompressed[0]))\n",
    "print(len(decompressed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    }
   ],
   "source": [
    "def top1accuracy(dataset):\n",
    "    return len(list(filter(lambda s: s == decoder([encoder.encode(s)])[0],\n",
    "                         dataset)))/len(dataset)\n",
    "print(top1accuracy(dumb_dataset[SPLIT]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
