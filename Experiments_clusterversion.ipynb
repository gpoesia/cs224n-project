{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import baseline\n",
    "from baseline import *\n",
    "from decoder import *\n",
    "from alphabet import *\n",
    "from train import *\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602762 training examples, 80964 validation examples, 80789 test exampless\n"
     ]
    }
   ],
   "source": [
    "LANGUAGE = 'Python'\n",
    "\n",
    "def filter_ascii(strings):\n",
    "    'Returns only the strings that can be encoded in ASCII.'\n",
    "    l = []\n",
    "    for s in strings:\n",
    "        try:\n",
    "            s.encode('ascii')\n",
    "            if 10 <= len(s) <= 80:\n",
    "                l.append(s)\n",
    "        except UnicodeEncodeError:\n",
    "            pass\n",
    "        \n",
    "    return list(set(l))\n",
    "\n",
    "with open('dataset/large.json') as f:\n",
    "    multilang_dataset = json.load(f)\n",
    "    dataset = multilang_dataset[LANGUAGE]\n",
    "    \n",
    "    dataset['train'] = filter_ascii(dataset['train'])\n",
    "    dataset['dev'] = filter_ascii(dataset['dev'])\n",
    "    dataset['test'] = filter_ascii(dataset['test'])\n",
    "    \n",
    "#     tiny_dataset = {\n",
    "#         'train': dataset['train'][:5],\n",
    "#         'dev': dataset['train'][:5],\n",
    "#         'test': dataset['train'][:5],\n",
    "#     }\n",
    "    \n",
    "    print('{} training examples, {} validation examples, {} test exampless'.format(\n",
    "        len(dataset['train']), \n",
    "        len(dataset['dev']),\n",
    "        len(dataset['test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iteration 0: loss = 4.852, tp = 10.29 lines/s, ETA 16h16m18s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-3103aa9bfdfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m'epochs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m }\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphabet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/home/BingBong/cs224n-project/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, dataset, parameters, alphabet, device)\u001b[0m\n\u001b[1;32m    160\u001b[0m                     \u001b[0mtrain_reconstruction_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstruction_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0moptimizer_dec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'learning_rate': 1e-4,\n",
    "    'verbose': True,\n",
    "    'batch_size': 64,\n",
    "    'init_scale': 0.01,\n",
    "    'epochs': 1,\n",
    "}\n",
    "train(encoder, decoder, dataset, parameters, alphabet, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.91654085692197"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(data['losses'])*128)/ len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0caf65f400>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "json_data = open('RulesBasedEncoderPython(whitespace=False)_0.001_python.json')\n",
    "data = json.load(json_data)\n",
    "data.keys()\n",
    "plt.plot(np.arange(len(data['losses'])), data['losses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AutoCompleteDecoderModel(\n",
       "  (encoder_lstm): LSTM(50, 512, batch_first=True, bidirectional=True)\n",
       "  (decoder_lstm): LSTMCell(562, 512)\n",
       "  (h_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "  (c_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "  (attention_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "  (output_proj): Linear(in_features=1536, out_features=512, bias=False)\n",
       "  (vocab_proj): Linear(in_features=512, out_features=128, bias=False)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPLIT = 'test'\n",
    "# load end-to-end models\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cpu')\n",
    "\n",
    "\n",
    "#get alphabet\n",
    "alph_pth = 'RulesBasedEncoderPython(whitespace=False)_0.001_pythonalphabet.model'\n",
    "alphabet = AsciiEmbeddedEncoding(device)\n",
    "# alphabet = AsciiEmbeddedEncoding(*args, **kwargs)\n",
    "alph_state_dict = torch.load(alph_pth, map_location=device)\n",
    "alphabet.load_state_dict(alph_state_dict)\n",
    "alphabet.to(device)\n",
    "# encoder\n",
    "encoder = UniformEncoder(.8)\n",
    "\n",
    "# decoder\n",
    "# def __init__(self, alphabet, hidden_size=100, max_test_length=200, dropout_rate=0.2,\n",
    "#                  copy=None):?\n",
    "decode_pth = 'RulesBasedEncoderPython(whitespace=False)_0.001_pythondecoder.model'\n",
    "decoder = AutoCompleteDecoderModel(alphabet, hidden_size=512)\n",
    "decoder.load_state_dict(state_dict=torch.load(decode_pth, map_location=device))\n",
    "decoder.to(device)\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# for i in range(10):\n",
    "#     s = random.choice(dataset[SPLIT])\n",
    "#     compressed = encoder.encode(s)\n",
    "# #     import pdb; pdb.set_trace()\n",
    "#     decompressed = decoder([compressed], alphabet)\n",
    "\n",
    "#     print('String:', repr(s))\n",
    "#     print('Encoded:', repr(compressed))\n",
    "#     print('Decoded:', repr(decompressed[0]))\n",
    "#     print(decompressed[0] == s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-11d9d2734aee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     return len(list(filter(lambda s: s == decoder([encoder.encode(s)], alphabet)[0],\n\u001b[1;32m      3\u001b[0m                          dataset)))/len(dataset)\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model without copy train accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop1accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-11d9d2734aee>\u001b[0m in \u001b[0;36mtop1accuracy\u001b[0;34m(dataset, decoder)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtop1accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     return len(list(filter(lambda s: s == decoder([encoder.encode(s)], alphabet)[0],\n\u001b[0;32m----> 3\u001b[0;31m                          dataset)))/len(dataset)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model without copy train accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop1accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-11d9d2734aee>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtop1accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     return len(list(filter(lambda s: s == decoder([encoder.encode(s)], alphabet)[0],\n\u001b[0m\u001b[1;32m      3\u001b[0m                          dataset)))/len(dataset)\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model without copy train accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop1accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/BingBong/cs224n-project/decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, compressed, alphabet, expected)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mC_padding_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompressed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0menc_hn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_cn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         decoder_state = (self.h_proj(enc_hn.transpose(0, 1).reshape(B, -1)),\n\u001b[1;32m     64\u001b[0m                          self.c_proj(enc_cn.transpose(0, 1).reshape(B, -1)))\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 559\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def top1accuracy(dataset, decoder):\n",
    "    return len(list(filter(lambda s: s == decoder([encoder.encode(s)], alphabet)[0],\n",
    "                         dataset)))/len(dataset)\n",
    "print('Model without copy train accuracy:', top1accuracy(dataset['train'], decoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dumb_dataset():\n",
    "    'Returns a dataset where all examples are the same string, which consists of 10 times the same letter.'\n",
    "\n",
    "    SIZE = 200\n",
    "    l = []\n",
    "\n",
    "    for i in range(SIZE):\n",
    "        l.append(random.choice('abcdefghijklmnopqrstuvwxyz') * random.choice([5, 10]))\n",
    "        \n",
    "    return {'train': l, 'dev': l, 'test': l}\n",
    "\n",
    "dumb_dataset = generate_dumb_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(0) if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "alphabet = AsciiOneHotEncoding(device)\n",
    "encoder = baseline.UniformEncoder(0.9)\n",
    "decoder_copy = AutoCompleteDecoderModel(alphabet, hidden_size=512, copy=COPY_CLASSIC)\n",
    "decoder_nocopy = AutoCompleteDecoderModel(alphabet, hidden_size=512, copy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /data/home/BingBong/cs224n-project/train.py(53)train()\n",
      "-> if save_model_every_epoch:\n",
      "(Pdb) total_examples\n",
      "7104\n",
      "(Pdb) len(training_set)\n",
      "7050\n",
      "(Pdb) 7104 / batch_size\n",
      "111.0\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8411b0024c70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_loss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_copy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/BingBong/cs224n-project/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, dataset, parameters, device)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mtotal_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0msave_model_every_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mintermediate_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Saving model after every epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/BingBong/cs224n-project/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, dataset, parameters, device)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mtotal_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0msave_model_every_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mintermediate_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Saving model after every epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'learning_rate': 1e-4,\n",
    "    'verbose': True,\n",
    "    'batch_size': 64,\n",
    "    'init_scale': 0.01,\n",
    "    'epochs': 1,\n",
    "}\n",
    "\n",
    "train_loss_history = train(encoder, decoder_copy, dataset, parameters, device)\n",
    "plt.plot(train_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iteration 0: loss = 2.258, tp = 199.14 lines/s, ETA 04h57m16s\n",
      "Epoch 0 iteration 100: loss = 1.694, tp = 197.61 lines/s, ETA 04h59m01s\n",
      "Epoch 1 iteration 89: loss = 1.647, tp = 198.04 lines/s, ETA 04h57m50s\n",
      "Epoch 2 iteration 78: loss = 1.766, tp = 198.01 lines/s, ETA 04h57m21s\n",
      "Epoch 3 iteration 67: loss = 1.699, tp = 198.36 lines/s, ETA 04h56m17s\n",
      "Epoch 4 iteration 56: loss = 1.762, tp = 198.26 lines/s, ETA 04h55m53s\n",
      "Epoch 5 iteration 45: loss = 1.674, tp = 198.10 lines/s, ETA 04h55m36s\n",
      "Epoch 6 iteration 34: loss = 1.673, tp = 198.26 lines/s, ETA 04h54m49s\n",
      "Epoch 7 iteration 23: loss = 1.489, tp = 198.30 lines/s, ETA 04h54m14s\n",
      "Epoch 8 iteration 12: loss = 1.544, tp = 198.32 lines/s, ETA 04h53m39s\n",
      "Epoch 9 iteration 1: loss = 1.573, tp = 198.44 lines/s, ETA 04h52m56s\n",
      "Epoch 9 iteration 101: loss = 1.486, tp = 198.56 lines/s, ETA 04h52m13s\n",
      "Epoch 10 iteration 90: loss = 1.725, tp = 198.61 lines/s, ETA 04h51m37s\n",
      "Epoch 11 iteration 79: loss = 1.619, tp = 198.65 lines/s, ETA 04h51m01s\n",
      "Epoch 12 iteration 68: loss = 1.528, tp = 198.62 lines/s, ETA 04h50m32s\n",
      "Epoch 13 iteration 57: loss = 1.636, tp = 198.77 lines/s, ETA 04h49m46s\n",
      "Epoch 14 iteration 46: loss = 1.540, tp = 198.75 lines/s, ETA 04h49m16s\n",
      "Epoch 15 iteration 35: loss = 1.556, tp = 198.90 lines/s, ETA 04h48m31s\n",
      "Epoch 16 iteration 24: loss = 1.740, tp = 198.99 lines/s, ETA 04h47m51s\n",
      "Epoch 17 iteration 13: loss = 1.444, tp = 199.00 lines/s, ETA 04h47m17s\n",
      "Epoch 18 iteration 2: loss = 1.526, tp = 199.03 lines/s, ETA 04h46m43s\n",
      "Epoch 18 iteration 102: loss = 1.509, tp = 199.06 lines/s, ETA 04h46m08s\n",
      "Epoch 19 iteration 91: loss = 1.360, tp = 199.14 lines/s, ETA 04h45m28s\n",
      "Epoch 20 iteration 80: loss = 1.290, tp = 199.22 lines/s, ETA 04h44m50s\n",
      "Epoch 21 iteration 69: loss = 1.497, tp = 199.25 lines/s, ETA 04h44m16s\n",
      "Epoch 22 iteration 58: loss = 1.328, tp = 199.24 lines/s, ETA 04h43m44s\n",
      "Epoch 23 iteration 47: loss = 1.355, tp = 199.30 lines/s, ETA 04h43m06s\n",
      "Epoch 24 iteration 36: loss = 1.380, tp = 199.33 lines/s, ETA 04h42m32s\n",
      "Epoch 25 iteration 25: loss = 1.581, tp = 199.38 lines/s, ETA 04h41m56s\n",
      "Epoch 26 iteration 14: loss = 1.356, tp = 199.41 lines/s, ETA 04h41m21s\n",
      "Epoch 27 iteration 3: loss = 1.551, tp = 199.44 lines/s, ETA 04h40m46s\n",
      "Epoch 27 iteration 103: loss = 1.439, tp = 199.44 lines/s, ETA 04h40m15s\n",
      "Epoch 28 iteration 92: loss = 1.347, tp = 199.41 lines/s, ETA 04h39m44s\n",
      "Epoch 29 iteration 81: loss = 1.221, tp = 199.36 lines/s, ETA 04h39m17s\n",
      "Epoch 30 iteration 70: loss = 1.365, tp = 199.29 lines/s, ETA 04h38m50s\n",
      "Epoch 31 iteration 59: loss = 1.525, tp = 199.26 lines/s, ETA 04h38m21s\n",
      "Epoch 32 iteration 48: loss = 1.280, tp = 199.22 lines/s, ETA 04h37m52s\n",
      "Epoch 33 iteration 37: loss = 1.193, tp = 199.19 lines/s, ETA 04h37m22s\n",
      "Epoch 34 iteration 26: loss = 1.294, tp = 199.19 lines/s, ETA 04h36m50s\n",
      "Epoch 35 iteration 15: loss = 1.240, tp = 199.17 lines/s, ETA 04h36m20s\n",
      "Epoch 36 iteration 4: loss = 1.318, tp = 199.17 lines/s, ETA 04h35m48s\n",
      "Epoch 36 iteration 104: loss = 1.289, tp = 199.19 lines/s, ETA 04h35m14s\n",
      "Epoch 37 iteration 93: loss = 1.313, tp = 199.23 lines/s, ETA 04h34m39s\n",
      "Epoch 38 iteration 82: loss = 1.327, tp = 199.26 lines/s, ETA 04h34m04s\n",
      "Epoch 39 iteration 71: loss = 1.351, tp = 199.27 lines/s, ETA 04h33m31s\n",
      "Epoch 40 iteration 60: loss = 1.390, tp = 199.32 lines/s, ETA 04h32m55s\n",
      "Epoch 41 iteration 49: loss = 1.189, tp = 199.35 lines/s, ETA 04h32m21s\n",
      "Epoch 42 iteration 38: loss = 1.310, tp = 199.36 lines/s, ETA 04h31m47s\n",
      "Epoch 43 iteration 27: loss = 1.150, tp = 199.37 lines/s, ETA 04h31m14s\n",
      "Epoch 44 iteration 16: loss = 1.306, tp = 199.38 lines/s, ETA 04h30m42s\n",
      "Epoch 45 iteration 5: loss = 1.161, tp = 199.37 lines/s, ETA 04h30m10s\n",
      "Epoch 45 iteration 105: loss = 1.293, tp = 199.40 lines/s, ETA 04h29m36s\n",
      "Epoch 46 iteration 94: loss = 1.139, tp = 199.39 lines/s, ETA 04h29m05s\n",
      "Epoch 47 iteration 83: loss = 1.047, tp = 199.34 lines/s, ETA 04h28m37s\n",
      "Epoch 48 iteration 72: loss = 1.186, tp = 199.32 lines/s, ETA 04h28m06s\n",
      "Epoch 49 iteration 61: loss = 1.052, tp = 199.28 lines/s, ETA 04h27m37s\n",
      "Epoch 50 iteration 50: loss = 1.162, tp = 199.25 lines/s, ETA 04h27m08s\n",
      "Epoch 51 iteration 39: loss = 1.211, tp = 199.20 lines/s, ETA 04h26m39s\n",
      "Epoch 52 iteration 28: loss = 1.246, tp = 199.17 lines/s, ETA 04h26m09s\n",
      "Epoch 53 iteration 17: loss = 1.118, tp = 199.14 lines/s, ETA 04h25m40s\n",
      "Epoch 54 iteration 6: loss = 1.016, tp = 199.12 lines/s, ETA 04h25m10s\n",
      "Epoch 54 iteration 106: loss = 1.170, tp = 199.11 lines/s, ETA 04h24m37s\n",
      "Epoch 55 iteration 95: loss = 1.192, tp = 199.12 lines/s, ETA 04h24m05s\n",
      "Epoch 56 iteration 84: loss = 1.325, tp = 199.13 lines/s, ETA 04h23m32s\n",
      "Epoch 57 iteration 73: loss = 1.149, tp = 199.15 lines/s, ETA 04h22m58s\n",
      "Epoch 58 iteration 62: loss = 1.093, tp = 199.15 lines/s, ETA 04h22m26s\n",
      "Epoch 59 iteration 51: loss = 1.096, tp = 199.17 lines/s, ETA 04h21m52s\n",
      "Epoch 60 iteration 40: loss = 1.064, tp = 199.16 lines/s, ETA 04h21m21s\n",
      "Epoch 61 iteration 29: loss = 1.069, tp = 199.14 lines/s, ETA 04h20m50s\n",
      "Epoch 62 iteration 18: loss = 0.982, tp = 199.13 lines/s, ETA 04h20m19s\n",
      "Epoch 63 iteration 7: loss = 1.055, tp = 199.11 lines/s, ETA 04h19m49s\n",
      "Epoch 63 iteration 107: loss = 1.051, tp = 199.11 lines/s, ETA 04h19m16s\n",
      "Epoch 64 iteration 96: loss = 1.143, tp = 199.08 lines/s, ETA 04h18m47s\n",
      "Epoch 65 iteration 85: loss = 1.020, tp = 199.08 lines/s, ETA 04h18m15s\n",
      "Epoch 66 iteration 74: loss = 1.119, tp = 199.10 lines/s, ETA 04h17m41s\n",
      "Epoch 67 iteration 63: loss = 1.072, tp = 199.10 lines/s, ETA 04h17m08s\n",
      "Epoch 68 iteration 52: loss = 0.918, tp = 199.12 lines/s, ETA 04h16m35s\n",
      "Epoch 69 iteration 41: loss = 0.861, tp = 199.10 lines/s, ETA 04h16m04s\n",
      "Epoch 70 iteration 30: loss = 0.800, tp = 199.06 lines/s, ETA 04h15m35s\n",
      "Epoch 71 iteration 19: loss = 0.643, tp = 199.03 lines/s, ETA 04h15m05s\n",
      "Epoch 72 iteration 8: loss = 0.538, tp = 199.00 lines/s, ETA 04h14m35s\n",
      "Epoch 72 iteration 108: loss = 0.541, tp = 198.99 lines/s, ETA 04h14m04s\n",
      "Epoch 73 iteration 97: loss = 0.548, tp = 198.98 lines/s, ETA 04h13m33s\n",
      "Epoch 74 iteration 86: loss = 0.494, tp = 198.96 lines/s, ETA 04h13m02s\n",
      "Epoch 75 iteration 75: loss = 0.515, tp = 198.94 lines/s, ETA 04h12m31s\n",
      "Epoch 76 iteration 64: loss = 0.444, tp = 198.92 lines/s, ETA 04h12m01s\n",
      "Epoch 77 iteration 53: loss = 0.379, tp = 198.91 lines/s, ETA 04h11m29s\n",
      "Epoch 78 iteration 42: loss = 0.380, tp = 198.92 lines/s, ETA 04h10m56s\n",
      "Epoch 79 iteration 31: loss = 0.359, tp = 198.92 lines/s, ETA 04h10m24s\n",
      "Epoch 80 iteration 20: loss = 0.355, tp = 198.93 lines/s, ETA 04h09m51s\n",
      "Epoch 81 iteration 9: loss = 0.259, tp = 198.95 lines/s, ETA 04h09m18s\n",
      "Epoch 81 iteration 109: loss = 0.265, tp = 198.95 lines/s, ETA 04h08m45s\n",
      "Epoch 82 iteration 98: loss = 0.346, tp = 198.96 lines/s, ETA 04h08m13s\n",
      "Epoch 83 iteration 87: loss = 0.360, tp = 198.96 lines/s, ETA 04h07m41s\n",
      "Epoch 84 iteration 76: loss = 0.305, tp = 198.94 lines/s, ETA 04h07m10s\n",
      "Epoch 85 iteration 65: loss = 0.369, tp = 198.91 lines/s, ETA 04h06m40s\n",
      "Epoch 86 iteration 54: loss = 0.318, tp = 198.89 lines/s, ETA 04h06m09s\n",
      "Epoch 87 iteration 43: loss = 0.319, tp = 198.90 lines/s, ETA 04h05m36s\n",
      "Epoch 88 iteration 32: loss = 0.310, tp = 198.89 lines/s, ETA 04h05m05s\n",
      "Epoch 89 iteration 21: loss = 0.274, tp = 198.89 lines/s, ETA 04h04m33s\n",
      "Epoch 90 iteration 10: loss = 0.304, tp = 198.89 lines/s, ETA 04h04m01s\n",
      "Epoch 90 iteration 110: loss = 0.294, tp = 198.90 lines/s, ETA 04h03m27s\n",
      "Epoch 91 iteration 99: loss = 0.253, tp = 198.91 lines/s, ETA 04h02m55s\n",
      "Epoch 92 iteration 88: loss = 0.322, tp = 198.90 lines/s, ETA 04h02m23s\n",
      "Epoch 93 iteration 77: loss = 0.278, tp = 198.90 lines/s, ETA 04h01m51s\n",
      "Epoch 94 iteration 66: loss = 0.259, tp = 198.89 lines/s, ETA 04h01m20s\n",
      "Epoch 95 iteration 55: loss = 0.283, tp = 198.87 lines/s, ETA 04h00m49s\n",
      "Epoch 96 iteration 44: loss = 0.260, tp = 198.85 lines/s, ETA 04h00m18s\n",
      "Epoch 97 iteration 33: loss = 0.719, tp = 198.85 lines/s, ETA 03h59m46s\n",
      "Epoch 98 iteration 22: loss = 0.260, tp = 198.86 lines/s, ETA 03h59m13s\n",
      "Epoch 99 iteration 11: loss = 0.311, tp = 198.87 lines/s, ETA 03h58m40s\n",
      "Epoch 100 iteration 0: loss = 0.323, tp = 198.86 lines/s, ETA 03h58m09s\n",
      "Epoch 100 iteration 100: loss = 0.318, tp = 198.84 lines/s, ETA 03h57m38s\n",
      "Epoch 101 iteration 89: loss = 0.258, tp = 198.86 lines/s, ETA 03h57m04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102 iteration 78: loss = 0.267, tp = 198.88 lines/s, ETA 03h56m31s\n",
      "Epoch 103 iteration 67: loss = 0.212, tp = 198.89 lines/s, ETA 03h55m58s\n",
      "Epoch 104 iteration 56: loss = 0.280, tp = 198.91 lines/s, ETA 03h55m24s\n",
      "Epoch 105 iteration 45: loss = 0.276, tp = 198.92 lines/s, ETA 03h54m51s\n",
      "Epoch 106 iteration 34: loss = 0.286, tp = 198.91 lines/s, ETA 03h54m20s\n",
      "Epoch 107 iteration 23: loss = 0.222, tp = 198.90 lines/s, ETA 03h53m49s\n",
      "Epoch 108 iteration 12: loss = 0.285, tp = 198.88 lines/s, ETA 03h53m17s\n",
      "Epoch 109 iteration 1: loss = 0.217, tp = 198.87 lines/s, ETA 03h52m46s\n",
      "Epoch 109 iteration 101: loss = 0.259, tp = 198.86 lines/s, ETA 03h52m15s\n",
      "Epoch 110 iteration 90: loss = 0.273, tp = 198.85 lines/s, ETA 03h51m43s\n",
      "Epoch 111 iteration 79: loss = 0.261, tp = 198.84 lines/s, ETA 03h51m12s\n",
      "Epoch 112 iteration 68: loss = 0.302, tp = 198.83 lines/s, ETA 03h50m40s\n",
      "Epoch 113 iteration 57: loss = 0.260, tp = 198.82 lines/s, ETA 03h50m09s\n",
      "Epoch 114 iteration 46: loss = 0.273, tp = 198.81 lines/s, ETA 03h49m37s\n",
      "Epoch 115 iteration 35: loss = 0.209, tp = 198.80 lines/s, ETA 03h49m06s\n",
      "Epoch 116 iteration 24: loss = 0.198, tp = 198.77 lines/s, ETA 03h48m35s\n",
      "Epoch 117 iteration 13: loss = 0.234, tp = 198.77 lines/s, ETA 03h48m04s\n",
      "Epoch 118 iteration 2: loss = 0.243, tp = 198.75 lines/s, ETA 03h47m32s\n",
      "Epoch 118 iteration 102: loss = 0.363, tp = 198.75 lines/s, ETA 03h47m01s\n",
      "Epoch 119 iteration 91: loss = 0.245, tp = 198.74 lines/s, ETA 03h46m29s\n",
      "Epoch 120 iteration 80: loss = 0.265, tp = 198.73 lines/s, ETA 03h45m57s\n",
      "Epoch 121 iteration 69: loss = 0.217, tp = 198.72 lines/s, ETA 03h45m25s\n",
      "Epoch 122 iteration 58: loss = 0.209, tp = 198.72 lines/s, ETA 03h44m54s\n",
      "Epoch 123 iteration 47: loss = 0.251, tp = 198.71 lines/s, ETA 03h44m22s\n",
      "Epoch 124 iteration 36: loss = 0.234, tp = 198.70 lines/s, ETA 03h43m50s\n",
      "Epoch 125 iteration 25: loss = 0.263, tp = 198.70 lines/s, ETA 03h43m18s\n",
      "Epoch 126 iteration 14: loss = 0.270, tp = 198.68 lines/s, ETA 03h42m47s\n",
      "Epoch 127 iteration 3: loss = 0.228, tp = 198.68 lines/s, ETA 03h42m15s\n",
      "Epoch 127 iteration 103: loss = 0.270, tp = 198.68 lines/s, ETA 03h41m43s\n",
      "Epoch 128 iteration 92: loss = 0.197, tp = 198.68 lines/s, ETA 03h41m11s\n",
      "Epoch 129 iteration 81: loss = 0.270, tp = 198.68 lines/s, ETA 03h40m39s\n",
      "Epoch 130 iteration 70: loss = 0.193, tp = 198.67 lines/s, ETA 03h40m07s\n",
      "Epoch 131 iteration 59: loss = 0.195, tp = 198.66 lines/s, ETA 03h39m35s\n",
      "Epoch 132 iteration 48: loss = 0.215, tp = 198.66 lines/s, ETA 03h39m03s\n",
      "Epoch 133 iteration 37: loss = 0.196, tp = 198.65 lines/s, ETA 03h38m31s\n",
      "Epoch 134 iteration 26: loss = 0.260, tp = 198.64 lines/s, ETA 03h38m00s\n",
      "Epoch 135 iteration 15: loss = 0.216, tp = 198.64 lines/s, ETA 03h37m28s\n",
      "Epoch 136 iteration 4: loss = 0.273, tp = 198.66 lines/s, ETA 03h36m54s\n",
      "Epoch 136 iteration 104: loss = 0.195, tp = 198.66 lines/s, ETA 03h36m22s\n",
      "Epoch 137 iteration 93: loss = 0.243, tp = 198.68 lines/s, ETA 03h35m49s\n",
      "Epoch 138 iteration 82: loss = 0.248, tp = 198.68 lines/s, ETA 03h35m16s\n",
      "Epoch 139 iteration 71: loss = 0.235, tp = 198.69 lines/s, ETA 03h34m43s\n",
      "Epoch 140 iteration 60: loss = 0.217, tp = 198.69 lines/s, ETA 03h34m11s\n",
      "Epoch 141 iteration 49: loss = 0.272, tp = 198.70 lines/s, ETA 03h33m39s\n",
      "Epoch 142 iteration 38: loss = 0.235, tp = 198.71 lines/s, ETA 03h33m06s\n",
      "Epoch 143 iteration 27: loss = 0.259, tp = 198.71 lines/s, ETA 03h32m33s\n",
      "Epoch 144 iteration 16: loss = 0.207, tp = 198.71 lines/s, ETA 03h32m01s\n",
      "Epoch 145 iteration 5: loss = 0.199, tp = 198.71 lines/s, ETA 03h31m29s\n",
      "Epoch 145 iteration 105: loss = 0.193, tp = 198.71 lines/s, ETA 03h30m57s\n",
      "Epoch 146 iteration 94: loss = 0.216, tp = 198.71 lines/s, ETA 03h30m25s\n",
      "Epoch 147 iteration 83: loss = 0.240, tp = 198.71 lines/s, ETA 03h29m53s\n",
      "Epoch 148 iteration 72: loss = 0.311, tp = 198.71 lines/s, ETA 03h29m20s\n",
      "Epoch 149 iteration 61: loss = 0.279, tp = 198.72 lines/s, ETA 03h28m47s\n",
      "Epoch 150 iteration 50: loss = 0.188, tp = 198.73 lines/s, ETA 03h28m14s\n",
      "Epoch 151 iteration 39: loss = 0.232, tp = 198.75 lines/s, ETA 03h27m41s\n",
      "Epoch 152 iteration 28: loss = 0.214, tp = 198.75 lines/s, ETA 03h27m09s\n",
      "Epoch 153 iteration 17: loss = 0.228, tp = 198.75 lines/s, ETA 03h26m37s\n",
      "Epoch 154 iteration 6: loss = 0.203, tp = 198.74 lines/s, ETA 03h26m05s\n",
      "Epoch 154 iteration 106: loss = 0.183, tp = 198.74 lines/s, ETA 03h25m33s\n",
      "Epoch 155 iteration 95: loss = 0.238, tp = 198.75 lines/s, ETA 03h25m00s\n",
      "Epoch 156 iteration 84: loss = 0.208, tp = 198.76 lines/s, ETA 03h24m27s\n",
      "Epoch 157 iteration 73: loss = 0.222, tp = 198.76 lines/s, ETA 03h23m55s\n",
      "Epoch 158 iteration 62: loss = 0.256, tp = 198.77 lines/s, ETA 03h23m22s\n",
      "Epoch 159 iteration 51: loss = 0.225, tp = 198.77 lines/s, ETA 03h22m50s\n",
      "Epoch 160 iteration 40: loss = 0.239, tp = 198.76 lines/s, ETA 03h22m18s\n",
      "Epoch 161 iteration 29: loss = 0.236, tp = 198.76 lines/s, ETA 03h21m47s\n",
      "Epoch 162 iteration 18: loss = 0.186, tp = 198.75 lines/s, ETA 03h21m15s\n",
      "Epoch 163 iteration 7: loss = 0.219, tp = 198.75 lines/s, ETA 03h20m43s\n",
      "Epoch 163 iteration 107: loss = 0.206, tp = 198.75 lines/s, ETA 03h20m10s\n",
      "Epoch 164 iteration 96: loss = 0.186, tp = 198.74 lines/s, ETA 03h19m39s\n",
      "Epoch 165 iteration 85: loss = 0.180, tp = 198.73 lines/s, ETA 03h19m07s\n",
      "Epoch 166 iteration 74: loss = 0.199, tp = 198.74 lines/s, ETA 03h18m34s\n",
      "Epoch 167 iteration 63: loss = 0.194, tp = 198.74 lines/s, ETA 03h18m02s\n",
      "Epoch 168 iteration 52: loss = 0.209, tp = 198.74 lines/s, ETA 03h17m30s\n",
      "Epoch 169 iteration 41: loss = 0.194, tp = 198.73 lines/s, ETA 03h16m58s\n",
      "Epoch 170 iteration 30: loss = 0.175, tp = 198.73 lines/s, ETA 03h16m26s\n",
      "Epoch 171 iteration 19: loss = 0.202, tp = 198.72 lines/s, ETA 03h15m54s\n",
      "Epoch 172 iteration 8: loss = 0.216, tp = 198.73 lines/s, ETA 03h15m22s\n",
      "Epoch 172 iteration 108: loss = 0.195, tp = 198.73 lines/s, ETA 03h14m49s\n",
      "Epoch 173 iteration 97: loss = 0.197, tp = 198.73 lines/s, ETA 03h14m17s\n",
      "Epoch 174 iteration 86: loss = 0.207, tp = 198.72 lines/s, ETA 03h13m46s\n",
      "Epoch 175 iteration 75: loss = 0.188, tp = 198.72 lines/s, ETA 03h13m14s\n",
      "Epoch 176 iteration 64: loss = 0.212, tp = 198.72 lines/s, ETA 03h12m41s\n",
      "Epoch 177 iteration 53: loss = 0.186, tp = 198.72 lines/s, ETA 03h12m09s\n",
      "Epoch 178 iteration 42: loss = 0.215, tp = 198.71 lines/s, ETA 03h11m37s\n",
      "Epoch 179 iteration 31: loss = 0.201, tp = 198.70 lines/s, ETA 03h11m05s\n",
      "Epoch 180 iteration 20: loss = 0.198, tp = 198.70 lines/s, ETA 03h10m34s\n",
      "Epoch 181 iteration 9: loss = 0.178, tp = 198.70 lines/s, ETA 03h10m01s\n",
      "Epoch 181 iteration 109: loss = 0.194, tp = 198.70 lines/s, ETA 03h09m29s\n",
      "Epoch 182 iteration 98: loss = 0.220, tp = 198.71 lines/s, ETA 03h08m56s\n",
      "Epoch 183 iteration 87: loss = 0.162, tp = 198.71 lines/s, ETA 03h08m24s\n",
      "Epoch 184 iteration 76: loss = 0.186, tp = 198.70 lines/s, ETA 03h07m52s\n",
      "Epoch 185 iteration 65: loss = 0.222, tp = 198.70 lines/s, ETA 03h07m20s\n",
      "Epoch 186 iteration 54: loss = 0.165, tp = 198.69 lines/s, ETA 03h06m48s\n",
      "Epoch 187 iteration 43: loss = 0.170, tp = 198.68 lines/s, ETA 03h06m17s\n",
      "Epoch 188 iteration 32: loss = 0.168, tp = 198.68 lines/s, ETA 03h05m45s\n",
      "Epoch 189 iteration 21: loss = 0.186, tp = 198.67 lines/s, ETA 03h05m13s\n",
      "Epoch 190 iteration 10: loss = 0.176, tp = 198.67 lines/s, ETA 03h04m41s\n",
      "Epoch 190 iteration 110: loss = 0.178, tp = 198.66 lines/s, ETA 03h04m09s\n",
      "Epoch 191 iteration 99: loss = 0.192, tp = 198.66 lines/s, ETA 03h03m37s\n",
      "Epoch 192 iteration 88: loss = 0.254, tp = 198.66 lines/s, ETA 03h03m05s\n",
      "Epoch 193 iteration 77: loss = 0.157, tp = 198.64 lines/s, ETA 03h02m33s\n",
      "Epoch 194 iteration 66: loss = 0.183, tp = 198.63 lines/s, ETA 03h02m02s\n",
      "Epoch 195 iteration 55: loss = 0.177, tp = 198.64 lines/s, ETA 03h01m29s\n",
      "Epoch 196 iteration 44: loss = 0.204, tp = 198.64 lines/s, ETA 03h00m57s\n",
      "Epoch 197 iteration 33: loss = 0.187, tp = 198.65 lines/s, ETA 03h00m24s\n",
      "Epoch 198 iteration 22: loss = 0.159, tp = 198.65 lines/s, ETA 02h59m52s\n",
      "Epoch 199 iteration 11: loss = 0.240, tp = 198.65 lines/s, ETA 02h59m20s\n",
      "Epoch 200 iteration 0: loss = 0.219, tp = 198.64 lines/s, ETA 02h58m48s\n",
      "Epoch 200 iteration 100: loss = 0.147, tp = 198.65 lines/s, ETA 02h58m16s\n",
      "Epoch 201 iteration 89: loss = 0.162, tp = 198.64 lines/s, ETA 02h57m44s\n",
      "Epoch 202 iteration 78: loss = 0.185, tp = 198.64 lines/s, ETA 02h57m12s\n",
      "Epoch 203 iteration 67: loss = 0.174, tp = 198.63 lines/s, ETA 02h56m40s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204 iteration 56: loss = 0.173, tp = 198.63 lines/s, ETA 02h56m08s\n",
      "Epoch 205 iteration 45: loss = 0.175, tp = 198.63 lines/s, ETA 02h55m36s\n",
      "Epoch 206 iteration 34: loss = 0.194, tp = 198.62 lines/s, ETA 02h55m04s\n",
      "Epoch 207 iteration 23: loss = 0.177, tp = 198.62 lines/s, ETA 02h54m32s\n",
      "Epoch 208 iteration 12: loss = 0.178, tp = 198.62 lines/s, ETA 02h53m59s\n",
      "Epoch 209 iteration 1: loss = 0.200, tp = 198.63 lines/s, ETA 02h53m27s\n",
      "Epoch 209 iteration 101: loss = 0.152, tp = 198.63 lines/s, ETA 02h52m54s\n",
      "Epoch 210 iteration 90: loss = 0.176, tp = 198.63 lines/s, ETA 02h52m22s\n",
      "Epoch 211 iteration 79: loss = 0.186, tp = 198.63 lines/s, ETA 02h51m50s\n",
      "Epoch 212 iteration 68: loss = 0.171, tp = 198.62 lines/s, ETA 02h51m18s\n",
      "Epoch 213 iteration 57: loss = 0.184, tp = 198.62 lines/s, ETA 02h50m46s\n",
      "Epoch 214 iteration 46: loss = 0.192, tp = 198.62 lines/s, ETA 02h50m14s\n",
      "Epoch 215 iteration 35: loss = 0.156, tp = 198.62 lines/s, ETA 02h49m41s\n",
      "Epoch 216 iteration 24: loss = 0.173, tp = 198.62 lines/s, ETA 02h49m09s\n",
      "Epoch 217 iteration 13: loss = 0.188, tp = 198.63 lines/s, ETA 02h48m37s\n",
      "Epoch 218 iteration 2: loss = 0.172, tp = 198.63 lines/s, ETA 02h48m04s\n",
      "Epoch 218 iteration 102: loss = 0.161, tp = 198.63 lines/s, ETA 02h47m32s\n",
      "Epoch 219 iteration 91: loss = 0.145, tp = 198.63 lines/s, ETA 02h47m00s\n",
      "Epoch 220 iteration 80: loss = 0.146, tp = 198.65 lines/s, ETA 02h46m27s\n",
      "Epoch 221 iteration 69: loss = 0.183, tp = 198.65 lines/s, ETA 02h45m54s\n",
      "Epoch 222 iteration 58: loss = 0.175, tp = 198.65 lines/s, ETA 02h45m22s\n",
      "Epoch 223 iteration 47: loss = 0.168, tp = 198.65 lines/s, ETA 02h44m50s\n",
      "Epoch 224 iteration 36: loss = 0.164, tp = 198.64 lines/s, ETA 02h44m18s\n",
      "Epoch 225 iteration 25: loss = 0.162, tp = 198.64 lines/s, ETA 02h43m46s\n",
      "Epoch 226 iteration 14: loss = 0.131, tp = 198.63 lines/s, ETA 02h43m14s\n",
      "Epoch 227 iteration 3: loss = 0.184, tp = 198.62 lines/s, ETA 02h42m42s\n",
      "Epoch 227 iteration 103: loss = 0.178, tp = 198.62 lines/s, ETA 02h42m10s\n",
      "Epoch 228 iteration 92: loss = 0.129, tp = 198.62 lines/s, ETA 02h41m38s\n",
      "Epoch 229 iteration 81: loss = 0.178, tp = 198.61 lines/s, ETA 02h41m06s\n",
      "Epoch 230 iteration 70: loss = 0.163, tp = 198.62 lines/s, ETA 02h40m34s\n",
      "Epoch 231 iteration 59: loss = 0.157, tp = 198.62 lines/s, ETA 02h40m01s\n",
      "Epoch 232 iteration 48: loss = 0.163, tp = 198.63 lines/s, ETA 02h39m29s\n",
      "Epoch 233 iteration 37: loss = 0.193, tp = 198.63 lines/s, ETA 02h38m56s\n",
      "Epoch 234 iteration 26: loss = 0.181, tp = 198.63 lines/s, ETA 02h38m24s\n",
      "Epoch 235 iteration 15: loss = 0.139, tp = 198.63 lines/s, ETA 02h37m52s\n",
      "Epoch 236 iteration 4: loss = 0.153, tp = 198.64 lines/s, ETA 02h37m19s\n",
      "Epoch 236 iteration 104: loss = 0.154, tp = 198.65 lines/s, ETA 02h36m47s\n",
      "Epoch 237 iteration 93: loss = 0.133, tp = 198.65 lines/s, ETA 02h36m15s\n",
      "Epoch 238 iteration 82: loss = 0.149, tp = 198.65 lines/s, ETA 02h35m42s\n",
      "Epoch 239 iteration 71: loss = 0.177, tp = 198.66 lines/s, ETA 02h35m10s\n",
      "Epoch 240 iteration 60: loss = 0.168, tp = 198.65 lines/s, ETA 02h34m38s\n",
      "Epoch 241 iteration 49: loss = 0.175, tp = 198.66 lines/s, ETA 02h34m05s\n",
      "Epoch 242 iteration 38: loss = 0.147, tp = 198.67 lines/s, ETA 02h33m32s\n",
      "Epoch 243 iteration 27: loss = 0.167, tp = 198.68 lines/s, ETA 02h33m00s\n",
      "Epoch 244 iteration 16: loss = 0.169, tp = 198.68 lines/s, ETA 02h32m28s\n",
      "Epoch 245 iteration 5: loss = 0.138, tp = 198.68 lines/s, ETA 02h31m55s\n",
      "Epoch 245 iteration 105: loss = 0.168, tp = 198.68 lines/s, ETA 02h31m23s\n",
      "Epoch 246 iteration 94: loss = 0.160, tp = 198.67 lines/s, ETA 02h30m51s\n",
      "Epoch 247 iteration 83: loss = 0.134, tp = 198.67 lines/s, ETA 02h30m19s\n",
      "Epoch 248 iteration 72: loss = 0.187, tp = 198.67 lines/s, ETA 02h29m47s\n",
      "Epoch 249 iteration 61: loss = 0.181, tp = 198.68 lines/s, ETA 02h29m14s\n",
      "Epoch 250 iteration 50: loss = 0.124, tp = 198.68 lines/s, ETA 02h28m42s\n",
      "Epoch 251 iteration 39: loss = 0.147, tp = 198.68 lines/s, ETA 02h28m10s\n",
      "Epoch 252 iteration 28: loss = 0.170, tp = 198.69 lines/s, ETA 02h27m37s\n",
      "Epoch 253 iteration 17: loss = 0.123, tp = 198.68 lines/s, ETA 02h27m05s\n",
      "Epoch 254 iteration 6: loss = 0.137, tp = 198.68 lines/s, ETA 02h26m33s\n",
      "Epoch 254 iteration 106: loss = 0.112, tp = 198.67 lines/s, ETA 02h26m02s\n",
      "Epoch 255 iteration 95: loss = 0.148, tp = 198.66 lines/s, ETA 02h25m30s\n",
      "Epoch 256 iteration 84: loss = 0.152, tp = 198.66 lines/s, ETA 02h24m57s\n",
      "Epoch 257 iteration 73: loss = 0.133, tp = 198.66 lines/s, ETA 02h24m25s\n",
      "Epoch 258 iteration 62: loss = 0.125, tp = 198.66 lines/s, ETA 02h23m53s\n",
      "Epoch 259 iteration 51: loss = 0.114, tp = 198.66 lines/s, ETA 02h23m21s\n",
      "Epoch 260 iteration 40: loss = 0.178, tp = 198.66 lines/s, ETA 02h22m49s\n",
      "Epoch 261 iteration 29: loss = 0.110, tp = 198.65 lines/s, ETA 02h22m17s\n",
      "Epoch 262 iteration 18: loss = 0.153, tp = 198.65 lines/s, ETA 02h21m45s\n",
      "Epoch 263 iteration 7: loss = 0.139, tp = 198.65 lines/s, ETA 02h21m12s\n",
      "Epoch 263 iteration 107: loss = 0.140, tp = 198.65 lines/s, ETA 02h20m40s\n",
      "Epoch 264 iteration 96: loss = 0.140, tp = 198.65 lines/s, ETA 02h20m08s\n",
      "Epoch 265 iteration 85: loss = 0.148, tp = 198.65 lines/s, ETA 02h19m36s\n",
      "Epoch 266 iteration 74: loss = 0.143, tp = 198.64 lines/s, ETA 02h19m04s\n",
      "Epoch 267 iteration 63: loss = 0.152, tp = 198.64 lines/s, ETA 02h18m32s\n",
      "Epoch 268 iteration 52: loss = 0.111, tp = 198.64 lines/s, ETA 02h18m00s\n",
      "Epoch 269 iteration 41: loss = 0.109, tp = 198.64 lines/s, ETA 02h17m27s\n",
      "Epoch 270 iteration 30: loss = 0.140, tp = 198.63 lines/s, ETA 02h16m55s\n",
      "Epoch 271 iteration 19: loss = 0.138, tp = 198.63 lines/s, ETA 02h16m23s\n",
      "Epoch 272 iteration 8: loss = 0.178, tp = 198.63 lines/s, ETA 02h15m51s\n",
      "Epoch 272 iteration 108: loss = 0.184, tp = 198.62 lines/s, ETA 02h15m19s\n",
      "Epoch 273 iteration 97: loss = 0.120, tp = 198.62 lines/s, ETA 02h14m47s\n",
      "Epoch 274 iteration 86: loss = 0.137, tp = 198.62 lines/s, ETA 02h14m15s\n",
      "Epoch 275 iteration 75: loss = 0.141, tp = 198.62 lines/s, ETA 02h13m42s\n",
      "Epoch 276 iteration 64: loss = 0.127, tp = 198.63 lines/s, ETA 02h13m10s\n",
      "Epoch 277 iteration 53: loss = 0.158, tp = 198.63 lines/s, ETA 02h12m38s\n",
      "Epoch 278 iteration 42: loss = 0.130, tp = 198.63 lines/s, ETA 02h12m05s\n",
      "Epoch 279 iteration 31: loss = 0.137, tp = 198.64 lines/s, ETA 02h11m33s\n",
      "Epoch 280 iteration 20: loss = 0.120, tp = 198.64 lines/s, ETA 02h11m01s\n",
      "Epoch 281 iteration 9: loss = 0.155, tp = 198.64 lines/s, ETA 02h10m28s\n",
      "Epoch 281 iteration 109: loss = 0.183, tp = 198.64 lines/s, ETA 02h09m56s\n",
      "Epoch 282 iteration 98: loss = 0.162, tp = 198.64 lines/s, ETA 02h09m24s\n",
      "Epoch 283 iteration 87: loss = 0.147, tp = 198.63 lines/s, ETA 02h08m52s\n",
      "Epoch 284 iteration 76: loss = 0.130, tp = 198.63 lines/s, ETA 02h08m20s\n",
      "Epoch 285 iteration 65: loss = 0.124, tp = 198.63 lines/s, ETA 02h07m48s\n",
      "Epoch 286 iteration 54: loss = 0.152, tp = 198.63 lines/s, ETA 02h07m16s\n",
      "Epoch 287 iteration 43: loss = 0.145, tp = 198.63 lines/s, ETA 02h06m43s\n",
      "Epoch 288 iteration 32: loss = 0.144, tp = 198.62 lines/s, ETA 02h06m11s\n",
      "Epoch 289 iteration 21: loss = 0.136, tp = 198.62 lines/s, ETA 02h05m39s\n",
      "Epoch 290 iteration 10: loss = 0.132, tp = 198.63 lines/s, ETA 02h05m07s\n",
      "Epoch 290 iteration 110: loss = 0.141, tp = 198.64 lines/s, ETA 02h04m34s\n",
      "Epoch 291 iteration 99: loss = 0.149, tp = 198.64 lines/s, ETA 02h04m02s\n",
      "Epoch 292 iteration 88: loss = 0.120, tp = 198.64 lines/s, ETA 02h03m29s\n",
      "Epoch 293 iteration 77: loss = 0.174, tp = 198.65 lines/s, ETA 02h02m57s\n",
      "Epoch 294 iteration 66: loss = 0.127, tp = 198.65 lines/s, ETA 02h02m25s\n",
      "Epoch 295 iteration 55: loss = 0.122, tp = 198.66 lines/s, ETA 02h01m52s\n",
      "Epoch 296 iteration 44: loss = 0.105, tp = 198.65 lines/s, ETA 02h01m20s\n",
      "Epoch 297 iteration 33: loss = 0.131, tp = 198.65 lines/s, ETA 02h00m48s\n",
      "Epoch 298 iteration 22: loss = 0.147, tp = 198.65 lines/s, ETA 02h00m16s\n",
      "Epoch 299 iteration 11: loss = 0.152, tp = 198.65 lines/s, ETA 01h59m44s\n",
      "Epoch 300 iteration 0: loss = 0.147, tp = 198.65 lines/s, ETA 01h59m12s\n",
      "Epoch 300 iteration 100: loss = 0.110, tp = 198.64 lines/s, ETA 01h58m39s\n",
      "Epoch 301 iteration 89: loss = 0.168, tp = 198.64 lines/s, ETA 01h58m07s\n",
      "Epoch 302 iteration 78: loss = 0.167, tp = 198.64 lines/s, ETA 01h57m35s\n",
      "Epoch 303 iteration 67: loss = 0.142, tp = 198.64 lines/s, ETA 01h57m03s\n",
      "Epoch 304 iteration 56: loss = 0.140, tp = 198.64 lines/s, ETA 01h56m31s\n",
      "Epoch 305 iteration 45: loss = 0.142, tp = 198.64 lines/s, ETA 01h55m58s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 306 iteration 34: loss = 0.152, tp = 198.65 lines/s, ETA 01h55m26s\n",
      "Epoch 307 iteration 23: loss = 0.132, tp = 198.65 lines/s, ETA 01h54m54s\n",
      "Epoch 308 iteration 12: loss = 0.111, tp = 198.65 lines/s, ETA 01h54m22s\n",
      "Epoch 309 iteration 1: loss = 0.134, tp = 198.64 lines/s, ETA 01h53m49s\n",
      "Epoch 309 iteration 101: loss = 0.148, tp = 198.64 lines/s, ETA 01h53m17s\n",
      "Epoch 310 iteration 90: loss = 0.138, tp = 198.64 lines/s, ETA 01h52m45s\n",
      "Epoch 311 iteration 79: loss = 0.142, tp = 198.64 lines/s, ETA 01h52m13s\n",
      "Epoch 312 iteration 68: loss = 0.140, tp = 198.63 lines/s, ETA 01h51m41s\n",
      "Epoch 313 iteration 57: loss = 0.138, tp = 198.63 lines/s, ETA 01h51m09s\n",
      "Epoch 314 iteration 46: loss = 0.120, tp = 198.63 lines/s, ETA 01h50m37s\n",
      "Epoch 315 iteration 35: loss = 0.109, tp = 198.63 lines/s, ETA 01h50m04s\n",
      "Epoch 316 iteration 24: loss = 0.110, tp = 198.63 lines/s, ETA 01h49m32s\n",
      "Epoch 317 iteration 13: loss = 0.115, tp = 198.63 lines/s, ETA 01h49m00s\n",
      "Epoch 318 iteration 2: loss = 0.145, tp = 198.63 lines/s, ETA 01h48m28s\n",
      "Epoch 318 iteration 102: loss = 0.108, tp = 198.63 lines/s, ETA 01h47m56s\n",
      "Epoch 319 iteration 91: loss = 0.127, tp = 198.63 lines/s, ETA 01h47m23s\n",
      "Epoch 320 iteration 80: loss = 0.116, tp = 198.64 lines/s, ETA 01h46m51s\n",
      "Epoch 321 iteration 69: loss = 0.142, tp = 198.64 lines/s, ETA 01h46m19s\n",
      "Epoch 322 iteration 58: loss = 0.121, tp = 198.64 lines/s, ETA 01h45m46s\n",
      "Epoch 323 iteration 47: loss = 0.109, tp = 198.65 lines/s, ETA 01h45m14s\n",
      "Epoch 324 iteration 36: loss = 0.145, tp = 198.65 lines/s, ETA 01h44m42s\n",
      "Epoch 325 iteration 25: loss = 0.138, tp = 198.64 lines/s, ETA 01h44m10s\n",
      "Epoch 326 iteration 14: loss = 0.139, tp = 198.64 lines/s, ETA 01h43m37s\n",
      "Epoch 327 iteration 3: loss = 0.131, tp = 198.64 lines/s, ETA 01h43m05s\n",
      "Epoch 327 iteration 103: loss = 0.124, tp = 198.64 lines/s, ETA 01h42m33s\n",
      "Epoch 328 iteration 92: loss = 0.157, tp = 198.63 lines/s, ETA 01h42m01s\n",
      "Epoch 329 iteration 81: loss = 0.131, tp = 198.63 lines/s, ETA 01h41m29s\n",
      "Epoch 330 iteration 70: loss = 0.113, tp = 198.63 lines/s, ETA 01h40m57s\n",
      "Epoch 331 iteration 59: loss = 0.103, tp = 198.62 lines/s, ETA 01h40m25s\n",
      "Epoch 332 iteration 48: loss = 0.143, tp = 198.63 lines/s, ETA 01h39m52s\n",
      "Epoch 333 iteration 37: loss = 0.104, tp = 198.63 lines/s, ETA 01h39m20s\n",
      "Epoch 334 iteration 26: loss = 0.105, tp = 198.63 lines/s, ETA 01h38m48s\n",
      "Epoch 335 iteration 15: loss = 0.109, tp = 198.63 lines/s, ETA 01h38m16s\n",
      "Epoch 336 iteration 4: loss = 0.109, tp = 198.62 lines/s, ETA 01h37m44s\n",
      "Epoch 336 iteration 104: loss = 0.098, tp = 198.62 lines/s, ETA 01h37m11s\n",
      "Epoch 337 iteration 93: loss = 0.115, tp = 198.62 lines/s, ETA 01h36m39s\n",
      "Epoch 338 iteration 82: loss = 0.129, tp = 198.62 lines/s, ETA 01h36m07s\n",
      "Epoch 339 iteration 71: loss = 0.110, tp = 198.62 lines/s, ETA 01h35m35s\n",
      "Epoch 340 iteration 60: loss = 0.089, tp = 198.62 lines/s, ETA 01h35m03s\n",
      "Epoch 341 iteration 49: loss = 0.103, tp = 198.62 lines/s, ETA 01h34m30s\n",
      "Epoch 342 iteration 38: loss = 0.083, tp = 198.61 lines/s, ETA 01h33m58s\n",
      "Epoch 343 iteration 27: loss = 0.115, tp = 198.62 lines/s, ETA 01h33m26s\n",
      "Epoch 344 iteration 16: loss = 0.152, tp = 198.62 lines/s, ETA 01h32m54s\n",
      "Epoch 345 iteration 5: loss = 0.126, tp = 198.63 lines/s, ETA 01h32m21s\n",
      "Epoch 345 iteration 105: loss = 0.145, tp = 198.63 lines/s, ETA 01h31m49s\n",
      "Epoch 346 iteration 94: loss = 0.116, tp = 198.63 lines/s, ETA 01h31m17s\n",
      "Epoch 347 iteration 83: loss = 0.101, tp = 198.64 lines/s, ETA 01h30m44s\n",
      "Epoch 348 iteration 72: loss = 0.098, tp = 198.64 lines/s, ETA 01h30m12s\n",
      "Epoch 349 iteration 61: loss = 0.140, tp = 198.65 lines/s, ETA 01h29m40s\n",
      "Epoch 350 iteration 50: loss = 0.105, tp = 198.65 lines/s, ETA 01h29m07s\n",
      "Epoch 351 iteration 39: loss = 0.100, tp = 198.66 lines/s, ETA 01h28m35s\n",
      "Epoch 352 iteration 28: loss = 0.109, tp = 198.66 lines/s, ETA 01h28m03s\n",
      "Epoch 353 iteration 17: loss = 0.100, tp = 198.66 lines/s, ETA 01h27m30s\n",
      "Epoch 354 iteration 6: loss = 0.117, tp = 198.67 lines/s, ETA 01h26m58s\n",
      "Epoch 354 iteration 106: loss = 0.116, tp = 198.67 lines/s, ETA 01h26m26s\n",
      "Epoch 355 iteration 95: loss = 0.092, tp = 198.67 lines/s, ETA 01h25m53s\n",
      "Epoch 356 iteration 84: loss = 0.113, tp = 198.68 lines/s, ETA 01h25m21s\n",
      "Epoch 357 iteration 73: loss = 0.110, tp = 198.68 lines/s, ETA 01h24m49s\n",
      "Epoch 358 iteration 62: loss = 0.121, tp = 198.68 lines/s, ETA 01h24m17s\n",
      "Epoch 359 iteration 51: loss = 0.085, tp = 198.68 lines/s, ETA 01h23m44s\n",
      "Epoch 360 iteration 40: loss = 0.120, tp = 198.68 lines/s, ETA 01h23m12s\n",
      "Epoch 361 iteration 29: loss = 0.120, tp = 198.68 lines/s, ETA 01h22m40s\n",
      "Epoch 362 iteration 18: loss = 0.071, tp = 198.67 lines/s, ETA 01h22m08s\n",
      "Epoch 363 iteration 7: loss = 0.117, tp = 198.67 lines/s, ETA 01h21m36s\n",
      "Epoch 363 iteration 107: loss = 0.103, tp = 198.67 lines/s, ETA 01h21m03s\n",
      "Epoch 364 iteration 96: loss = 0.126, tp = 198.67 lines/s, ETA 01h20m31s\n",
      "Epoch 365 iteration 85: loss = 0.096, tp = 198.68 lines/s, ETA 01h19m59s\n",
      "Epoch 366 iteration 74: loss = 0.086, tp = 198.67 lines/s, ETA 01h19m27s\n",
      "Epoch 367 iteration 63: loss = 0.128, tp = 198.67 lines/s, ETA 01h18m55s\n",
      "Epoch 368 iteration 52: loss = 0.112, tp = 198.67 lines/s, ETA 01h18m23s\n",
      "Epoch 369 iteration 41: loss = 0.080, tp = 198.67 lines/s, ETA 01h17m50s\n",
      "Epoch 370 iteration 30: loss = 0.094, tp = 198.67 lines/s, ETA 01h17m18s\n",
      "Epoch 371 iteration 19: loss = 0.111, tp = 198.67 lines/s, ETA 01h16m46s\n",
      "Epoch 372 iteration 8: loss = 0.105, tp = 198.67 lines/s, ETA 01h16m14s\n",
      "Epoch 372 iteration 108: loss = 0.106, tp = 198.67 lines/s, ETA 01h15m41s\n",
      "Epoch 373 iteration 97: loss = 0.095, tp = 198.68 lines/s, ETA 01h15m09s\n",
      "Epoch 374 iteration 86: loss = 0.120, tp = 198.67 lines/s, ETA 01h14m37s\n",
      "Epoch 375 iteration 75: loss = 0.107, tp = 198.68 lines/s, ETA 01h14m05s\n",
      "Epoch 376 iteration 64: loss = 0.130, tp = 198.68 lines/s, ETA 01h13m32s\n",
      "Epoch 377 iteration 53: loss = 0.083, tp = 198.69 lines/s, ETA 01h13m00s\n",
      "Epoch 378 iteration 42: loss = 0.101, tp = 198.69 lines/s, ETA 01h12m28s\n",
      "Epoch 379 iteration 31: loss = 0.103, tp = 198.69 lines/s, ETA 01h11m55s\n",
      "Epoch 380 iteration 20: loss = 0.120, tp = 198.69 lines/s, ETA 01h11m23s\n",
      "Epoch 381 iteration 9: loss = 0.107, tp = 198.69 lines/s, ETA 01h10m51s\n",
      "Epoch 381 iteration 109: loss = 0.088, tp = 198.69 lines/s, ETA 01h10m19s\n",
      "Epoch 382 iteration 98: loss = 0.132, tp = 198.69 lines/s, ETA 01h09m47s\n",
      "Epoch 383 iteration 87: loss = 0.099, tp = 198.70 lines/s, ETA 01h09m14s\n",
      "Epoch 384 iteration 76: loss = 0.098, tp = 198.70 lines/s, ETA 01h08m42s\n",
      "Epoch 385 iteration 65: loss = 0.109, tp = 198.70 lines/s, ETA 01h08m10s\n",
      "Epoch 386 iteration 54: loss = 0.107, tp = 198.70 lines/s, ETA 01h07m37s\n",
      "Epoch 387 iteration 43: loss = 0.106, tp = 198.70 lines/s, ETA 01h07m05s\n",
      "Epoch 388 iteration 32: loss = 0.108, tp = 198.70 lines/s, ETA 01h06m33s\n",
      "Epoch 389 iteration 21: loss = 0.109, tp = 198.70 lines/s, ETA 01h06m01s\n",
      "Epoch 390 iteration 10: loss = 0.100, tp = 198.69 lines/s, ETA 01h05m29s\n",
      "Epoch 390 iteration 110: loss = 0.071, tp = 198.69 lines/s, ETA 01h04m57s\n",
      "Epoch 391 iteration 99: loss = 0.132, tp = 198.69 lines/s, ETA 01h04m25s\n",
      "Epoch 392 iteration 88: loss = 0.108, tp = 198.68 lines/s, ETA 01h03m52s\n",
      "Epoch 393 iteration 77: loss = 0.098, tp = 198.69 lines/s, ETA 01h03m20s\n",
      "Epoch 394 iteration 66: loss = 0.130, tp = 198.69 lines/s, ETA 01h02m48s\n",
      "Epoch 395 iteration 55: loss = 0.083, tp = 198.69 lines/s, ETA 01h02m16s\n",
      "Epoch 396 iteration 44: loss = 0.104, tp = 198.70 lines/s, ETA 01h01m43s\n",
      "Epoch 397 iteration 33: loss = 0.099, tp = 198.69 lines/s, ETA 01h01m11s\n",
      "Epoch 398 iteration 22: loss = 0.108, tp = 198.69 lines/s, ETA 01h00m39s\n",
      "Epoch 399 iteration 11: loss = 0.097, tp = 198.70 lines/s, ETA 01h00m07s\n",
      "Epoch 400 iteration 0: loss = 0.087, tp = 198.70 lines/s, ETA 00h59m35s\n",
      "Epoch 400 iteration 100: loss = 0.107, tp = 198.69 lines/s, ETA 00h59m02s\n",
      "Epoch 401 iteration 89: loss = 0.105, tp = 198.69 lines/s, ETA 00h58m30s\n",
      "Epoch 402 iteration 78: loss = 0.072, tp = 198.69 lines/s, ETA 00h57m58s\n",
      "Epoch 403 iteration 67: loss = 0.063, tp = 198.69 lines/s, ETA 00h57m26s\n",
      "Epoch 404 iteration 56: loss = 0.090, tp = 198.68 lines/s, ETA 00h56m54s\n",
      "Epoch 405 iteration 45: loss = 0.108, tp = 198.68 lines/s, ETA 00h56m22s\n",
      "Epoch 406 iteration 34: loss = 0.094, tp = 198.67 lines/s, ETA 00h55m49s\n",
      "Epoch 407 iteration 23: loss = 0.109, tp = 198.67 lines/s, ETA 00h55m17s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 408 iteration 12: loss = 0.111, tp = 198.67 lines/s, ETA 00h54m45s\n",
      "Epoch 409 iteration 1: loss = 0.095, tp = 198.66 lines/s, ETA 00h54m13s\n",
      "Epoch 409 iteration 101: loss = 0.106, tp = 198.66 lines/s, ETA 00h53m41s\n",
      "Epoch 410 iteration 90: loss = 0.119, tp = 198.66 lines/s, ETA 00h53m09s\n",
      "Epoch 411 iteration 79: loss = 0.119, tp = 198.65 lines/s, ETA 00h52m36s\n",
      "Epoch 412 iteration 68: loss = 0.102, tp = 198.65 lines/s, ETA 00h52m04s\n",
      "Epoch 413 iteration 57: loss = 0.075, tp = 198.66 lines/s, ETA 00h51m32s\n",
      "Epoch 414 iteration 46: loss = 0.104, tp = 198.66 lines/s, ETA 00h51m00s\n",
      "Epoch 415 iteration 35: loss = 0.083, tp = 198.66 lines/s, ETA 00h50m27s\n",
      "Epoch 416 iteration 24: loss = 0.077, tp = 198.67 lines/s, ETA 00h49m55s\n",
      "Epoch 417 iteration 13: loss = 0.116, tp = 198.67 lines/s, ETA 00h49m23s\n",
      "Epoch 418 iteration 2: loss = 0.109, tp = 198.67 lines/s, ETA 00h48m51s\n",
      "Epoch 418 iteration 102: loss = 0.095, tp = 198.67 lines/s, ETA 00h48m18s\n",
      "Epoch 419 iteration 91: loss = 0.093, tp = 198.67 lines/s, ETA 00h47m46s\n",
      "Epoch 420 iteration 80: loss = 0.104, tp = 198.67 lines/s, ETA 00h47m14s\n",
      "Epoch 421 iteration 69: loss = 0.077, tp = 198.67 lines/s, ETA 00h46m42s\n",
      "Epoch 422 iteration 58: loss = 0.099, tp = 198.67 lines/s, ETA 00h46m10s\n",
      "Epoch 423 iteration 47: loss = 0.092, tp = 198.66 lines/s, ETA 00h45m37s\n",
      "Epoch 424 iteration 36: loss = 0.090, tp = 198.65 lines/s, ETA 00h45m05s\n",
      "Epoch 425 iteration 25: loss = 0.099, tp = 198.65 lines/s, ETA 00h44m33s\n",
      "Epoch 426 iteration 14: loss = 0.130, tp = 198.65 lines/s, ETA 00h44m01s\n",
      "Epoch 427 iteration 3: loss = 0.084, tp = 198.64 lines/s, ETA 00h43m29s\n",
      "Epoch 427 iteration 103: loss = 0.066, tp = 198.64 lines/s, ETA 00h42m57s\n",
      "Epoch 428 iteration 92: loss = 0.085, tp = 198.64 lines/s, ETA 00h42m24s\n",
      "Epoch 429 iteration 81: loss = 0.085, tp = 198.64 lines/s, ETA 00h41m52s\n",
      "Epoch 430 iteration 70: loss = 0.115, tp = 198.64 lines/s, ETA 00h41m20s\n",
      "Epoch 431 iteration 59: loss = 0.084, tp = 198.64 lines/s, ETA 00h40m48s\n",
      "Epoch 432 iteration 48: loss = 0.097, tp = 198.64 lines/s, ETA 00h40m16s\n",
      "Epoch 433 iteration 37: loss = 0.090, tp = 198.64 lines/s, ETA 00h39m43s\n",
      "Epoch 434 iteration 26: loss = 0.071, tp = 198.63 lines/s, ETA 00h39m11s\n",
      "Epoch 435 iteration 15: loss = 0.086, tp = 198.63 lines/s, ETA 00h38m39s\n",
      "Epoch 436 iteration 4: loss = 0.081, tp = 198.62 lines/s, ETA 00h38m07s\n",
      "Epoch 436 iteration 104: loss = 0.077, tp = 198.62 lines/s, ETA 00h37m35s\n",
      "Epoch 437 iteration 93: loss = 0.078, tp = 198.62 lines/s, ETA 00h37m03s\n",
      "Epoch 438 iteration 82: loss = 0.093, tp = 198.62 lines/s, ETA 00h36m30s\n",
      "Epoch 439 iteration 71: loss = 0.093, tp = 198.62 lines/s, ETA 00h35m58s\n",
      "Epoch 440 iteration 60: loss = 0.094, tp = 198.61 lines/s, ETA 00h35m26s\n",
      "Epoch 441 iteration 49: loss = 0.085, tp = 198.61 lines/s, ETA 00h34m54s\n",
      "Epoch 442 iteration 38: loss = 0.101, tp = 198.61 lines/s, ETA 00h34m21s\n",
      "Epoch 443 iteration 27: loss = 0.099, tp = 198.61 lines/s, ETA 00h33m49s\n",
      "Epoch 444 iteration 16: loss = 0.067, tp = 198.61 lines/s, ETA 00h33m17s\n",
      "Epoch 445 iteration 5: loss = 0.071, tp = 198.61 lines/s, ETA 00h32m45s\n",
      "Epoch 445 iteration 105: loss = 0.096, tp = 198.61 lines/s, ETA 00h32m13s\n",
      "Epoch 446 iteration 94: loss = 0.081, tp = 198.61 lines/s, ETA 00h31m40s\n",
      "Epoch 447 iteration 83: loss = 0.088, tp = 198.60 lines/s, ETA 00h31m08s\n",
      "Epoch 448 iteration 72: loss = 0.094, tp = 198.60 lines/s, ETA 00h30m36s\n",
      "Epoch 449 iteration 61: loss = 0.075, tp = 198.60 lines/s, ETA 00h30m04s\n",
      "Epoch 450 iteration 50: loss = 0.096, tp = 198.60 lines/s, ETA 00h29m32s\n",
      "Epoch 451 iteration 39: loss = 0.086, tp = 198.60 lines/s, ETA 00h28m59s\n",
      "Epoch 452 iteration 28: loss = 0.075, tp = 198.60 lines/s, ETA 00h28m27s\n",
      "Epoch 453 iteration 17: loss = 0.061, tp = 198.60 lines/s, ETA 00h27m55s\n",
      "Epoch 454 iteration 6: loss = 0.090, tp = 198.61 lines/s, ETA 00h27m23s\n",
      "Epoch 454 iteration 106: loss = 0.109, tp = 198.61 lines/s, ETA 00h26m50s\n",
      "Epoch 455 iteration 95: loss = 0.077, tp = 198.61 lines/s, ETA 00h26m18s\n",
      "Epoch 456 iteration 84: loss = 0.138, tp = 198.61 lines/s, ETA 00h25m46s\n",
      "Epoch 457 iteration 73: loss = 0.076, tp = 198.61 lines/s, ETA 00h25m14s\n",
      "Epoch 458 iteration 62: loss = 0.084, tp = 198.61 lines/s, ETA 00h24m41s\n",
      "Epoch 459 iteration 51: loss = 0.086, tp = 198.61 lines/s, ETA 00h24m09s\n",
      "Epoch 460 iteration 40: loss = 0.064, tp = 198.62 lines/s, ETA 00h23m37s\n",
      "Epoch 461 iteration 29: loss = 0.072, tp = 198.62 lines/s, ETA 00h23m05s\n",
      "Epoch 462 iteration 18: loss = 0.069, tp = 198.62 lines/s, ETA 00h22m33s\n",
      "Epoch 463 iteration 7: loss = 0.069, tp = 198.62 lines/s, ETA 00h22m00s\n",
      "Epoch 463 iteration 107: loss = 0.073, tp = 198.63 lines/s, ETA 00h21m28s\n",
      "Epoch 464 iteration 96: loss = 0.091, tp = 198.63 lines/s, ETA 00h20m56s\n",
      "Epoch 465 iteration 85: loss = 0.084, tp = 198.63 lines/s, ETA 00h20m24s\n",
      "Epoch 466 iteration 74: loss = 0.087, tp = 198.63 lines/s, ETA 00h19m51s\n",
      "Epoch 467 iteration 63: loss = 0.067, tp = 198.63 lines/s, ETA 00h19m19s\n",
      "Epoch 468 iteration 52: loss = 0.071, tp = 198.63 lines/s, ETA 00h18m47s\n",
      "Epoch 469 iteration 41: loss = 0.106, tp = 198.63 lines/s, ETA 00h18m15s\n",
      "Epoch 470 iteration 30: loss = 0.077, tp = 198.63 lines/s, ETA 00h17m42s\n",
      "Epoch 471 iteration 19: loss = 0.091, tp = 198.62 lines/s, ETA 00h17m10s\n",
      "Epoch 472 iteration 8: loss = 0.085, tp = 198.62 lines/s, ETA 00h16m38s\n",
      "Epoch 472 iteration 108: loss = 0.094, tp = 198.62 lines/s, ETA 00h16m06s\n",
      "Epoch 473 iteration 97: loss = 0.076, tp = 198.62 lines/s, ETA 00h15m34s\n",
      "Epoch 474 iteration 86: loss = 0.093, tp = 198.63 lines/s, ETA 00h15m01s\n",
      "Epoch 475 iteration 75: loss = 0.067, tp = 198.63 lines/s, ETA 00h14m29s\n",
      "Epoch 476 iteration 64: loss = 0.078, tp = 198.63 lines/s, ETA 00h13m57s\n",
      "Epoch 477 iteration 53: loss = 0.070, tp = 198.63 lines/s, ETA 00h13m25s\n",
      "Epoch 478 iteration 42: loss = 0.102, tp = 198.63 lines/s, ETA 00h12m52s\n",
      "Epoch 479 iteration 31: loss = 0.080, tp = 198.63 lines/s, ETA 00h12m20s\n",
      "Epoch 480 iteration 20: loss = 0.070, tp = 198.62 lines/s, ETA 00h11m48s\n",
      "Epoch 481 iteration 9: loss = 0.121, tp = 198.63 lines/s, ETA 00h11m16s\n",
      "Epoch 481 iteration 109: loss = 0.112, tp = 198.63 lines/s, ETA 00h10m44s\n",
      "Epoch 482 iteration 98: loss = 0.081, tp = 198.63 lines/s, ETA 00h10m11s\n",
      "Epoch 483 iteration 87: loss = 0.092, tp = 198.64 lines/s, ETA 00h09m39s\n",
      "Epoch 484 iteration 76: loss = 0.085, tp = 198.64 lines/s, ETA 00h09m07s\n",
      "Epoch 485 iteration 65: loss = 0.067, tp = 198.64 lines/s, ETA 00h08m35s\n",
      "Epoch 486 iteration 54: loss = 0.068, tp = 198.64 lines/s, ETA 00h08m02s\n",
      "Epoch 487 iteration 43: loss = 0.066, tp = 198.65 lines/s, ETA 00h07m30s\n",
      "Epoch 488 iteration 32: loss = 0.059, tp = 198.65 lines/s, ETA 00h06m58s\n",
      "Epoch 489 iteration 21: loss = 0.071, tp = 198.65 lines/s, ETA 00h06m26s\n",
      "Epoch 490 iteration 10: loss = 0.058, tp = 198.65 lines/s, ETA 00h05m54s\n",
      "Epoch 490 iteration 110: loss = 0.071, tp = 198.65 lines/s, ETA 00h05m21s\n",
      "Epoch 491 iteration 99: loss = 0.077, tp = 198.66 lines/s, ETA 00h04m49s\n",
      "Epoch 492 iteration 88: loss = 0.075, tp = 198.66 lines/s, ETA 00h04m17s\n",
      "Epoch 493 iteration 77: loss = 0.097, tp = 198.66 lines/s, ETA 00h03m45s\n",
      "Epoch 494 iteration 66: loss = 0.073, tp = 198.66 lines/s, ETA 00h03m12s\n",
      "Epoch 495 iteration 55: loss = 0.076, tp = 198.66 lines/s, ETA 00h02m40s\n",
      "Epoch 496 iteration 44: loss = 0.076, tp = 198.66 lines/s, ETA 00h02m08s\n",
      "Epoch 497 iteration 33: loss = 0.086, tp = 198.65 lines/s, ETA 00h01m36s\n",
      "Epoch 498 iteration 22: loss = 0.067, tp = 198.65 lines/s, ETA 00h01m04s\n",
      "Epoch 499 iteration 11: loss = 0.067, tp = 198.65 lines/s, ETA 00h00m31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7efe2c166d68>]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'learning_rate': 1e-4,\n",
    "    'verbose': True,\n",
    "    'batch_size': 64,\n",
    "    'init_scale': 0.01,\n",
    "    'epochs': 500,\n",
    "}\n",
    "\n",
    "train_loss_history_nocopy = train(encoder, decoder_nocopy, dataset, parameters, device)\n",
    "plt.plot(train_loss_history_nocopy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-5-9a2b5526c81f>(9)<module>()->None\n",
      "-> compressed = encoder.encode(s)\n",
      "(Pdb) s\n",
      "--Call--\n",
      "> /data/home/BingBong/cs224n-project/baseline.py(18)encode()\n",
      "-> def encode(self, s):\n",
      "(Pdb) dataset[SPLIT][0]\n",
      "*** NameError: name 'dataset' is not defined\n",
      "(Pdb) dataset.shape\n",
      "*** NameError: name 'dataset' is not defined\n",
      "--KeyboardInterrupt--\n",
      "--KeyboardInterrupt--\n"
     ]
    }
   ],
   "source": [
    "SPLIT = 'train'\n",
    "# load end-to-end models\n",
    "alphabet = torch.load('UniformEncoder(0.80)_0.005_pythonalphabet.model')\n",
    "encoder = UniformEncoder(.8)\n",
    "decoder = torch.load('UniformEncoder(0.80)_0.005_pythondecoder.model')\n",
    "for i in range(10):\n",
    "    s = random.choice(dataset[SPLIT])\n",
    "    import pdb; pdb.set_trace()\n",
    "    compressed = encoder.encode(s)\n",
    "\n",
    "    decompressed = decoder([compressed])\n",
    "\n",
    "    print('String:', repr(s))\n",
    "    print('Encoded:', repr(compressed))\n",
    "    print('Decoded:', repr(decompressed[0]))\n",
    "    print(decompressed[0] == s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with copy validation accuracy: 0.08808510638297873\n",
      "Model without copy validation accuracy: 0.3516312056737589\n"
     ]
    }
   ],
   "source": [
    "def top1accuracy(dataset, decoder):\n",
    "    return len(list(filter(lambda s: s == decoder([encoder.encode(s)])[0],\n",
    "                         dataset)))/len(dataset)\n",
    "\n",
    "print('Model with copy train accuracy:', top1accuracy(dataset['train'], decoder_copy))\n",
    "print('Model without copy train accuracy:', top1accuracy(dataset['train'], decoder_nocopy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save just the state dict\n",
    "torch.save(decoder_copy.state_dict(), 'decoder_python_copy_small.pts')\n",
    "torch.save(decoder_nocopy.state_dict(), 'decoder_python_nocopy_small.pts')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
